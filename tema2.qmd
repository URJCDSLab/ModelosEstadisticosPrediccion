---
editor: 
  markdown: 
    wrap: 72
---

# El modelo de regresión lineal múltiple {#sec-regresion-lineal-multiple}

El modelo de regresión lineal múltiple constituye la extensión natural y más potente del modelo simple que estudiamos en el capítulo anterior. Mientras que la regresión simple nos permitía examinar la relación entre una variable respuesta y un único predictor, la regresión múltiple nos capacita para **modelar simultáneamente el efecto de múltiples variables predictoras**, una situación mucho más realista en la mayoría de aplicaciones prácticas [@kutner2005applied; @james2021; @fox2018r].

En este capítulo profundizaremos en los aspectos únicos de la regresión múltiple que no están presentes en el caso simple: la **interpretación de coeficientes en presencia de otros predictores**, las **transformaciones de variables** para capturar relaciones no lineales, las **interacciones** entre variables, el **diagnóstico específico** del modelo múltiple, y el problema crucial de la **multicolinealidad**. Estos conceptos son fundamentales para desarrollar modelos predictivos robustos y interpretables [@harrell2015; @draper1998applied].

La comprensión sólida de estos principios es esencial, ya que constituyen la base conceptual sobre la que se construyen técnicas más avanzadas como la selección de variables (Tema 3), los modelos lineales generalizados (Tema 4), y las tendencias modernas en modelado estadístico (Tema 5).

::: {.callout-important title="Objetivos de aprendizaje"}
Al finalizar este capítulo, serás capaz de:

1.  **Formular y estimar** modelos de regresión lineal múltiple, comprendiendo las diferencias clave respecto al caso simple.
2.  **Interpretar coeficientes** en el contexto multivariante, entendiendo el concepto de "manteniendo las demás variables constantes".
3.  **Aplicar transformaciones de variables** para capturar relaciones no lineales y mejorar el ajuste del modelo.
4.  **Modelar e interpretar interacciones** entre variables predictoras y entender cuándo son necesarias.
5.  **Diagnosticar el modelo múltiple**, aplicando técnicas específicas para detectar problemas únicos del caso multivariante.
6.  **Identificar y tratar la multicolinealidad**, comprendiendo sus causas, consecuencias y soluciones.
7.  **Evaluar y comparar modelos múltiples** usando criterios apropiados para la selección del mejor modelo.
:::

## Formulación teórica del modelo

El paso de la regresión simple a la múltiple es más que una simple adición de términos; es un salto conceptual. Nos permite construir modelos que reflejan mejor la complejidad del mundo real, donde los resultados raramente dependen de una única causa. Al controlar simultáneamente por varios factores, podemos aislar con mayor precisión el efecto de una variable de interés, reduciendo el riesgo de llegar a conclusiones sesgadas por variables omitidas.

Para $n$ observaciones y $p$ variables predictoras, el modelo postula una relación lineal de la siguiente forma:

$$y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} + \varepsilon_i, \quad i = 1,\dots,n$$

Donde $y_i$ es la $i$-ésima observación de la variable respuesta, $x_{ij}$ es la $i$-ésima observación del $j$-ésimo predictor, y $\varepsilon_i$ es el término de error aleatorio. El coeficiente $\beta_j$ representa el cambio esperado en la media de $Y$ ante un cambio de una unidad en el predictor $x_j$, **manteniendo constantes todas las demás variables predictoras del modelo**. Este principio, conocido como *ceteris paribus* (del latín, "lo demás constante"), es la piedra angular de la interpretación en regresión múltiple.

La notación matricial es fundamental para el desarrollo teórico y computacional. Nos permite expresar el sistema de $n$ ecuaciones de forma compacta y elegante:

$$\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}$$

donde:

$$\mathbf{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}, \quad
\mathbf{X} = \begin{bmatrix}
1 & x_{11} & x_{12} & \cdots & x_{1p} \\
1 & x_{21} & x_{22} & \cdots & x_{2p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \cdots & x_{np}
\end{bmatrix}, \quad
\boldsymbol{\beta} = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{bmatrix}, \quad
\boldsymbol{\varepsilon} = \begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n \end{bmatrix}$$

La matriz $\mathbf{X}$, de dimensión $n \times (p+1)$, se denomina **matriz de diseño** y contiene toda la información de los predictores. La primera columna de unos corresponde al término del intercepto $\beta_0$.

### Supuestos del modelo lineal múltiple

Para que nuestros estimadores tengan propiedades deseables (como ser insesgados y eficientes), el modelo debe cumplir una serie de supuestos sobre el comportamiento del término de error, conocidos como las **condiciones de Gauss-Markov** [@kutner2005applied; @weisberg2005applied].

1.  **Linealidad en los parámetros**: El valor esperado de la respuesta es una función lineal de los parámetros $\boldsymbol{\beta}$. El modelo $E[\mathbf{y}|\mathbf{X}] = \mathbf{X}\boldsymbol{\beta}$ está bien especificado.
2.  **Exogeneidad (media del error nula)**: Los errores tienen una media de cero para cualquier valor de los predictores, $E[\boldsymbol{\varepsilon}|\mathbf{X}] = \mathbf{0}$. Esto implica que los predictores no contienen información sobre el término de error.
3.  **Homocedasticidad e independencia**: Los errores no están correlacionados entre sí y tienen una varianza constante $\sigma^2$ para cualquier valor de los predictores. En notación matricial: $\text{Var}(\boldsymbol{\varepsilon}|\mathbf{X}) = \sigma^2\mathbf{I}_n$.
4.  **Ausencia de multicolinealidad perfecta**: Ningún predictor es una combinación lineal exacta de los otros. Esto asegura que la matriz $\mathbf{X}$ tiene rango completo $(p+1)$, lo cual es necesario para poder estimar de forma única todos los coeficientes.
5.  **Normalidad de los errores (para inferencia)**: Para poder realizar contrastes de hipótesis e intervalos de confianza, se añade el supuesto de que los errores siguen una distribución Normal: $\boldsymbol{\varepsilon} \sim N(\mathbf{0}, \sigma^2\mathbf{I}_n)$.

## Estimación de los parámetros

Una vez definido el modelo y sus supuestos, el siguiente paso es estimar los parámetros desconocidos del vector $\boldsymbol{\beta}$. El método más extendido es el de Mínimos Cuadrados Ordinarios.

### El principio de mínimos cuadrados y la función objetivo

La idea de "mejor ajuste" se traduce matemáticamente en minimizar la discrepancia entre los valores observados $\mathbf{y}$ y los valores predichos por el modelo, $\hat{\mathbf{y}} = \mathbf{X}\boldsymbol{\beta}$. Esta discrepancia se captura a través de los residuos, $\mathbf{e} = \mathbf{y} - \hat{\mathbf{y}}$.

MCO no minimiza simplemente los residuos (ya que residuos positivos y negativos se cancelarían), sino la **Suma de los Cuadrados de los Residuos** (SCR o *SSR* en inglés). Al elevarlos al cuadrado, nos aseguramos de que todos los errores contribuyan positivamente y, además, penalizamos más fuertemente los errores grandes.

La función objetivo a minimizar, $S(\boldsymbol{\beta})$, es:

$$S(\boldsymbol{\beta}) = \sum_{i=1}^n e_i^2 = \mathbf{e}^T\mathbf{e} = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^T(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})$$

### Derivación de las ecuaciones normales

Para encontrar el vector $\hat{\boldsymbol{\beta}}$ que minimiza esta función, utilizamos cálculo diferencial. Primero, expandimos la expresión cuadrática de $S(\boldsymbol{\beta})$:

$$S(\boldsymbol{\beta}) = (\mathbf{y}^T - \boldsymbol{\beta}^T\mathbf{X}^T)(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})$$
$$S(\boldsymbol{\beta}) = \mathbf{y}^T\mathbf{y} - \mathbf{y}^T\mathbf{X}\boldsymbol{\beta} - \boldsymbol{\beta}^T\mathbf{X}^T\mathbf{y} + \boldsymbol{\beta}^T\mathbf{X}^T\mathbf{X}\boldsymbol{\beta}$$

Un punto clave aquí es notar que $\boldsymbol{\beta}^T\mathbf{X}^T\mathbf{y}$ es un escalar (una matriz $1 \times 1$). Por lo tanto, es igual a su propia transpuesta: $(\boldsymbol{\beta}^T\mathbf{X}^T\mathbf{y})^T = \mathbf{y}^T\mathbf{X}\boldsymbol{\beta}$. Esto nos permite simplificar la expresión:

$$S(\boldsymbol{\beta}) = \mathbf{y}^T\mathbf{y} - 2\boldsymbol{\beta}^T\mathbf{X}^T\mathbf{y} + \boldsymbol{\beta}^T(\mathbf{X}^T\mathbf{X})\boldsymbol{\beta}$$

Ahora, derivamos esta función con respecto al vector $\boldsymbol{\beta}$ e igualamos el resultado a un vector de ceros para encontrar el mínimo. Usando las reglas de la derivación matricial:

* La derivada de $\mathbf{y}^T\mathbf{y}$ respecto a $\boldsymbol{\beta}$ es $\mathbf{0}$.
* La derivada de $2\boldsymbol{\beta}^T\mathbf{X}^T\mathbf{y}$ respecto a $\boldsymbol{\beta}$ es $2\mathbf{X}^T\mathbf{y}$.
* La derivada de la forma cuadrática $\boldsymbol{\beta}^T(\mathbf{X}^T\mathbf{X})\boldsymbol{\beta}$ respecto a $\boldsymbol{\beta}$ es $2(\mathbf{X}^T\mathbf{X})\boldsymbol{\beta}$.

Aplicando estas reglas, obtenemos el gradiente de la función de pérdida:

$$\frac{\partial S(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} = -2\mathbf{X}^T\mathbf{y} + 2(\mathbf{X}^T\mathbf{X})\boldsymbol{\beta}$$

Igualando a cero y sustituyendo $\boldsymbol{\beta}$ por el estimador $\hat{\boldsymbol{\beta}}$ que cumple esta condición:

$$-2\mathbf{X}^T\mathbf{y} + 2(\mathbf{X}^T\mathbf{X})\hat{\boldsymbol{\beta}} = \mathbf{0}$$

Simplificando, llegamos al célebre sistema de $p+1$ ecuaciones conocido como las **Ecuaciones Normales**:

$$(\mathbf{X}^T\mathbf{X})\hat{\boldsymbol{\beta}} = \mathbf{X}^T\mathbf{y}$$

### La solución MCO y la condición de invertibilidad

Para resolver este sistema y despejar $\hat{\boldsymbol{\beta}}$, necesitamos multiplicar por la inversa de la matriz $(\mathbf{X}^T\mathbf{X})$. Esta inversa existe si y solo si la matriz es invertible, lo que está directamente garantizado por el **supuesto de ausencia de multicolinealidad perfecta**.

Si el rango de la matriz de diseño $\mathbf{X}$ es $p+1$ (sus columnas son linealmente independientes), entonces la matriz $\mathbf{X}^T\mathbf{X}$ (de dimensión $(p+1) \times (p+1)$) será de rango completo, simétrica y definida positiva, y por tanto, invertible.

La solución única para el vector de estimadores MCO es:

$$\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$$

Esta compacta y poderosa ecuación es la base de la estimación en regresión lineal y es implementada por todo el software estadístico.



Claro, aquí tienes el texto completo en formato Markdown y con las fórmulas clave sin los recuadros.

### Propiedades de los estimadores de MCO

Una vez que hemos obtenido la fórmula para calcular nuestros coeficientes, $\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$, la pregunta fundamental es: ¿qué tan buenos son estos estimadores? La teoría estadística nos proporciona una respuesta contundente a través de sus propiedades en el muestreo, que son la base para toda la inferencia estadística posterior.

El **Teorema de Gauss-Markov** es el resultado central. Afirma que, si se cumplen los supuestos del modelo lineal clásico (1-4), los estimadores de Mínimos Cuadrados Ordinarios son los **Mejores Estimadores Lineales Insesgados (MELI o BLUE)**. Desglosemos esto:

* **Lineal**: $\hat{\boldsymbol{\beta}}$ es una combinación lineal de la variable respuesta $\mathbf{y}$.
* **Insesgado (Unbiased)**: En promedio, a lo largo de infinitas muestras, nuestro estimador acertará al verdadero valor poblacional $\boldsymbol{\beta}$. No tiene un sesgo sistemático. La demostración formal es directa:
    \begin{align}
    E[\hat{\boldsymbol{\beta}}] &= E[(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}] \nonumber \\
    &= E[(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T(\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon})] \nonumber \\
    &= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}\boldsymbol{\beta} + (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T E[\boldsymbol{\varepsilon}] \nonumber \\
    &= \boldsymbol{I}\boldsymbol{\beta} + \mathbf{0} = \boldsymbol{\beta} \nonumber
    \end{align}
* **Mejor (Best)**: "Mejor" significa que tiene la mínima varianza posible dentro de la clase de todos los estimadores lineales e insesgados. No existe otro estimador de este tipo que sea más preciso. La precisión de nuestros estimadores se captura en su **matriz de varianzas-covarianzas**:
    \begin{align}
    \text{Var}(\hat{\boldsymbol{\beta}}) &= \text{Var}[(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}] \nonumber \\
    &= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T \, \text{Var}(\mathbf{y}) \, [(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T]^T \nonumber \\
    &= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T \, (\sigma^2 \mathbf{I}_n) \, \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1} \nonumber \\
    &= \sigma^2 (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1} \nonumber \\
    &= \sigma^2 (\mathbf{X}^T\mathbf{X})^{-1} \nonumber
    \end{align}
    Por tanto, la matriz que define la incertidumbre de nuestro estimador es:
    $$\text{Var}(\hat{\boldsymbol{\beta}}) = \sigma^2(\mathbf{X}^T\mathbf{X})^{-1}$$
    Los elementos de la diagonal de esta matriz nos dan la varianza de cada coeficiente individual, $Var(\hat{\beta}_j)$, mientras que los elementos fuera de la diagonal nos dan la covarianza entre pares de coeficientes, $Cov(\hat{\beta}_j, \hat{\beta}_k)$.

Finalmente, si añadimos el **supuesto de normalidad de los errores** ($\varepsilon_i \sim N(0, \sigma^2)$), las propiedades del estimador se completan. Dado que $\hat{\boldsymbol{\beta}}$ es una combinación lineal de $\mathbf{y}$ (que ahora es normal), el propio estimador seguirá una distribución normal:
$$\hat{\boldsymbol{\beta}} \sim N\left(\boldsymbol{\beta}, \sigma^2(\mathbf{X}^T\mathbf{X})^{-1}\right)$$Esto implica que cada coeficiente individual también se distribuye normalmente:$$\hat{\beta}_j \sim N\left(\beta_j, \sigma^2 [(\mathbf{X}^T\mathbf{X})^{-1}]_{jj}\right)$$
donde $[(\mathbf{X}^T\mathbf{X})^{-1}]_{jj}$ es el j-ésimo elemento de la diagonal de la matriz inversa. Este resultado es la puerta de entrada a la inferencia, permitiéndonos construir intervalos de confianza y realizar contrastes de hipótesis (como los test-t).

### Estimación de la varianza del error 

La matriz de varianzas-covarianzas de $\hat{\boldsymbol{\beta}}$ depende de $\sigma^2$, la varianza de los errores poblacionales, que es desconocida. Por lo tanto, el siguiente paso lógico es encontrar un buen estimador para ella a partir de nuestros datos.

El punto de partida natural son los residuos del modelo, $\mathbf{e} = \mathbf{y} - \hat{\mathbf{y}}$, que son la contraparte muestral de los errores teóricos $\boldsymbol{\varepsilon}$. La suma de los cuadrados de los residuos (SSE) es la base de nuestro estimador:

$$SSE = \mathbf{e}^T\mathbf{e} = \mathbf{y}^T(\mathbf{I}_n - \mathbf{H})^T(\mathbf{I}_n - \mathbf{H})\mathbf{y} = \mathbf{y}^T(\mathbf{I}_n - \mathbf{H})\mathbf{y}$$
(usando las propiedades de simetría e idempotencia de la matriz de proyección $\mathbf{H}$).

Para encontrar un estimador insesgado, calculamos el valor esperado de la SSE. Utilizando el lema $E[\mathbf{z}^T\mathbf{A}\mathbf{z}] = \text{traza}(\mathbf{A}\boldsymbol{\Sigma}) + \boldsymbol{\mu}^T\mathbf{A}\boldsymbol{\mu}$ con $\mathbf{z} = \mathbf{y}$, $\mathbf{A} = \mathbf{I}_n - \mathbf{H}$, $\boldsymbol{\mu} = \mathbf{X}\boldsymbol{\beta}$ y $\boldsymbol{\Sigma} = \sigma^2\mathbf{I}_n$:
\begin{align}
E[SSE] &= \text{traza}[(\mathbf{I}_n - \mathbf{H})\sigma^2\mathbf{I}_n] + \boldsymbol{\beta}^T\mathbf{X}^T(\mathbf{I}_n - \mathbf{H})\mathbf{X}\boldsymbol{\beta} \nonumber
\end{align}
El segundo término se anula porque $(\mathbf{I}_n - \mathbf{H})\mathbf{X} = \mathbf{X} - \mathbf{H}\mathbf{X} = \mathbf{X} - \mathbf{X} = \mathbf{0}$. Nos queda:
\begin{align}
E[SSE] &= \sigma^2 \text{traza}(\mathbf{I}_n - \mathbf{H}) \nonumber \\
&= \sigma^2 (\text{traza}(\mathbf{I}_n) - \text{traza}(\mathbf{H})) \nonumber \\
&= \sigma^2 (n - (p + 1)) \nonumber
\end{align}
El valor esperado de la SSE no es $\sigma^2$, sino un múltiplo de ella. Esto nos lleva directamente a un estimador insesgado para $\sigma^2$ dividiendo la SSE por sus **grados de libertad**, $n - p - 1$:
$$\hat{\sigma}^2 = s^2 = \frac{SSE}{n - p - 1} = \frac{\mathbf{e}^T\mathbf{e}}{n - p - 1}$$
Intuitivamente, perdemos un grado de libertad por cada parámetro que hemos estimado en el modelo (los $p$ coeficientes de las pendientes y el intercepto). La raíz cuadrada de este valor, $\hat{\sigma}$, se conoce como el **Error Estándar de la Regresión** y representa la magnitud de un error de predicción típico.

Con este estimador, podemos calcular el **error estándar de cada coeficiente**, que mide la incertidumbre de nuestra estimación para $\beta_j$:
$$\text{se}(\hat{\beta}_j) = \sqrt{\hat{\sigma}^2 [(\mathbf{X}^T\mathbf{X})^{-1}]_{jj}}$$
Bajo normalidad, se puede demostrar además que la cantidad $\frac{SSE}{\sigma^2}$ sigue una distribución **Chi-cuadrado** con $n-p-1$ grados de libertad, un resultado clave para la inferencia formal.

::: {.callout-tip title="Ejemplo: Estimación de un modelo múltiple" collapse=True}
Para ilustrar estos conceptos, usemos un ejemplo con datos de precios de viviendas. Supongamos que queremos predecir el precio de una vivienda basándonos en su superficie, número de habitaciones, antigüedad, distancia al centro y si tiene garaje.

```{r}
#| echo: false
#| warning: false
#| message: false
# Configuración inicial
suppressPackageStartupMessages({
  library(ggplot2)
  library(dplyr)
  library(broom)
})

# Crear datos simulados realistas
set.seed(123)
n <- 200

# Variables predictoras
superficie <- rnorm(n, 120, 30)  # m2
habitaciones <- round(2 + 0.02*superficie + rnorm(n, 0, 0.8))  # número
antiguedad <- sample(0:50, n, replace = TRUE)  # años
distancia_centro <- runif(n, 1, 25)  # km
garaje <- rbinom(n, 1, 0.6)  # 0/1

# Variable respuesta con relaciones realistas
precio <- 50000 + 
  1200*superficie + 
  15000*habitaciones - 
  800*antiguedad - 
  2000*distancia_centro + 
  25000*garaje + 
  rnorm(n, 0, 15000)

# Crear data frame
viviendas <- data.frame(
  precio = round(precio),
  superficie = round(superficie),
  habitaciones = pmax(1, habitaciones),
  antiguedad = antiguedad,
  distancia_centro = round(distancia_centro, 1),
  garaje = factor(garaje, labels = c("No", "Sí"))
)

# Ajustar el modelo múltiple
modelo <- lm(precio ~ superficie + habitaciones + antiguedad + 
             distancia_centro + garaje, data = viviendas)

# Mostrar resumen del modelo
summary(modelo)
```

Este output nos muestra:

- **Coeficientes estimados** ($\hat{\boldsymbol{\beta}}$) y sus errores estándar
- **Estadísticos t** y p-valores para cada coeficiente
- **Error estándar residual** ($\hat{\sigma} = `r round(summary(modelo)$sigma, 0)` euros)
- **R² múltiple** (`r round(summary(modelo)$r.squared, 4)`) - proporción de varianza explicada
- **Estadístico F global** para contrastar la significancia del modelo
:::

## La interpretación de los coeficientes

Estimar los coeficientes y sus errores estándar es solo la mitad del trabajo. La otra mitad, y a menudo la más importante, es interpretarlos correctamente.

El concepto fundamental en regresión múltiple es el de **ceteris paribus** ("lo demás constante"). Cada coeficiente $\beta_j$ representa el cambio esperado en $Y$ por un cambio de una unidad en $X_j$, **manteniendo todas las demás variables predictoras del modelo fijas**. Es el efecto "puro" o "aislado" de $X_j$ sobre $Y$, después de haber controlado por la influencia de las otras variables incluidas en el modelo. Matemáticamente, es la derivada parcial del valor esperado de $Y$ con respecto a $X_j$:
$$\beta_j = \frac{\partial E[Y|\mathbf{X}]}{\partial X_j}$$


Esta interpretación es crucialmente diferente de la que se obtiene en una regresión simple. El coeficiente de una regresión simple de $Y$ sobre $X_j$ captura no solo el efecto directo de $X_j$, sino también los efectos indirectos de cualquier otra variable omitida que esté correlacionada tanto con $Y$ como con $X_j$. Por ello, el valor de $\hat{\beta}_j$ en una regresión múltiple casi nunca es igual al de una regresión simple.

La forma más precisa de entender $\hat{\beta}_j$ es a través del concepto de **regresión parcial**. El coeficiente $\hat{\beta}_j$ de la regresión múltiple es idéntico al coeficiente de una regresión simple entre dos conjuntos de residuos:

1.  Los residuos de una regresión de $\mathbf{y}$ sobre todas las demás variables predictoras (excepto $X_j$).
2.  Los residuos de una regresión de $\mathbf{x_j}$ sobre todas las demás variables predictoras.

En otras palabras, $\hat{\beta}_j$ mide la relación entre la parte de $Y$ que no puede ser explicada por las otras variables y la parte de $X_j$ que tampoco puede ser explicada por las otras variables. Es la asociación entre $Y$ y $X_j$ después de haber "limpiado" o "netado" la influencia de todos los demás predictores de ambas. Este concepto se visualiza en los **gráficos de regresión parcial** (o *added-variable plots*), que son una herramienta de diagnóstico fundamental.

::: {.callout-tip title="Interpretación práctica de los coeficientes" collapse=True}
Volviendo a nuestro ejemplo de viviendas, interpretemos cada coeficiente aplicando el principio *ceteris paribus*:

```{r}
#| echo: false
# Extraer coeficientes para interpretación
coefs <- round(coef(modelo), 0)
```

- **Superficie** (`r coefs[2]` €/m²): Por cada metro cuadrado adicional, el precio aumenta en promedio `r coefs[2]` euros, **manteniendo constantes** el número de habitaciones, antigüedad, distancia al centro y presencia de garaje.

- **Habitaciones** (`r coefs[3]` €): Cada habitación adicional incrementa el precio en `r coefs[3]` euros en promedio, **controlando por** la superficie y demás variables.

- **Antigüedad** (`r coefs[4]` €/año): Por cada año de antigüedad, el precio disminuye en `r abs(coefs[4])` euros en promedio, **ceteris paribus**.

- **Distancia al centro** (`r coefs[5]` €/km): Cada kilómetro adicional de distancia reduce el precio en `r abs(coefs[5])` euros en promedio, **manteniendo todo lo demás constante**.

- **Garaje** (`r coefs[6]` €): Las viviendas con garaje cuestan `r coefs[6]` euros más que las que no tienen, **en promedio y controlando por las demás variables**.

**Punto clave**: Estos efectos son diferentes de los que obtendríamos con regresiones simples, ya que aquí hemos "limpiado" la influencia de las otras variables.
:::

::: {.callout-note title="La perspectiva geométrica de mínimos cuadrados"}
La estimación por mínimos cuadrados tiene una interpretación geométrica elegante y potente que nos ayuda a comprender qué está ocurriendo. 

Podemos pensar en el vector de observaciones $\mathbf{y}$ como un punto en un espacio de $n$ dimensiones. Las columnas de la matriz de diseño $\mathbf{X}$ generan un subespacio vectorial dentro de $\mathbb{R}^n$, conocido como el **espacio columna** de $\mathbf{X}$, denotado $C(\mathbf{X})$. Este subespacio contiene todas las posibles combinaciones lineales de nuestros predictores.

El método MCO encuentra el vector de valores ajustados $\hat{\mathbf{y}} = \mathbf{X}\hat{\boldsymbol{\beta}}$ que está "más cerca" de $\mathbf{y}$. Geométricamente, este punto no es otro que la **proyección ortogonal** del vector $\mathbf{y}$ sobre el subespacio $C(\mathbf{X})$.

Esta proyección se realiza a través de una matriz especial llamada **matriz de proyección** o **matriz sombrero** (*hat matrix*), denotada por $\mathbf{H}$:

$$\hat{\mathbf{y}} = \text{Proj}_{C(\mathbf{X})}\,\mathbf{y} = \mathbf{H}\,\mathbf{y}, \qquad \text{donde} \quad \mathbf{H} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T$$

Esta operación induce la **descomposición ortogonal fundamental** del vector de respuesta:

$$\mathbf{y} = \hat{\mathbf{y}} + \mathbf{e}$$

El hecho de que la proyección sea ortogonal implica que el vector de residuos $\mathbf{e}$ es ortogonal (perpendicular) al vector de valores ajustados $\hat{\mathbf{y}}$ y, de hecho, a todo el subespacio $C(\mathbf{X})$. Esta ortogonalidad, $\hat{\mathbf{y}}^T\mathbf{e}=0$, es la base del **Teorema de Pitágoras para la regresión**, que permite la descomposición de la variabilidad total en una parte explicada y una no explicada.
:::

## Evaluación del modelo y descomposición de la varianza

Una vez estimado el modelo, el siguiente paso es evaluar su desempeño. ¿Qué tan bien se ajustan nuestras predicciones a los datos reales? Aunque ya vimos la perspectiva geométrica y el Teorema de Pitágoras en regresión simple, es importante revisitar estos conceptos porque en regresión múltiple la interpretación y el cálculo de la descomposición de varianza presenta matices adicionales que debemos entender claramente.

En regresión múltiple, la **Descomposición de la Varianza** o **ANOVA** (*Analysis of Variance*) cobra especial relevancia porque ahora tenemos múltiples variables explicativas y necesitamos evaluar el aporte conjunto de todas ellas, así como su significancia global.

La idea fundamental es que la variabilidad total de la variable respuesta ($Y$) puede descomponerse en dos partes: una parte que es explicada por nuestro modelo de regresión (ahora con múltiples variables) y otra parte que queda sin explicar, atribuida al error aleatorio.

Partimos de la identidad: $(y_i - \bar{y}) = (\hat{y}_i - \bar{y}) + (y_i - \hat{y}_i) = (\hat{y}_i - \bar{y}) + e_i$.

Elevando al cuadrado y sumando para todas las observaciones (y gracias a la propiedad de ortogonalidad $\hat{\mathbf{y}}^T\mathbf{e}=0$, que hace que los productos cruzados se anulen), llegamos a la descomposición fundamental de las sumas de cuadrados:

$$\sum_{i=1}^n (y_i - \bar{y})^2 = \sum_{i=1}^n (\hat{y}_i - \bar{y})^2 + \sum_{i=1}^n e_i^2$$

Esto se conoce como la **ecuación de ANOVA**:

$$SST = SSR + SSE$$

Donde:

* **SST (Suma de Cuadrados Total)**: Es la variabilidad total de $Y$. Mide la dispersión de los datos observados alrededor de su media.
* **SSR (Suma de Cuadrados de la Regresión)**: Es la variabilidad **explicada** por el modelo. Mide la dispersión de los valores predichos alrededor de la media.
* **SSE (Suma de Cuadrados del Error)**: Es la variabilidad **no explicada** o residual. Mide la dispersión de los datos observados alrededor de la línea de regresión.

Esta tabla no es solo un resumen; es el motor de las principales herramientas de evaluación e inferencia del modelo.

### Coeficiente de determinación múltiple

El **coeficiente de determinación**, $R^2$, es la medida de ajuste más popular. Responde a la pregunta: *¿Qué proporción de la variabilidad total de Y es explicada por las variables predictoras del modelo?*

$$R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}$$

**Propiedades clave**:

* Su valor siempre está entre 0 (el modelo no explica nada) y 1 (el modelo explica toda la variabilidad).
* Puede interpretarse como el cuadrado de la correlación entre los valores observados y los valores predichos, $R^2 = \text{corr}^2(\mathbf{y}, \hat{\mathbf{y}})$.
* **Problema**: $R^2$ **nunca decrece** al añadir una nueva variable predictora al modelo, incluso si esta es completamente irrelevante. Esto lo convierte en una métrica engañosa para comparar modelos con distinto número de predictores.

### El coeficiente de determinación ajustado

Para solucionar el problema de $R^2$, utilizamos el **$R^2$ ajustado**, que introduce una penalización por cada variable añadida. Lo hace comparando las varianzas (sumas de cuadrados divididas por sus grados de libertad) en lugar de solo las sumas de cuadrados:

$$R^2_{adj} = 1 - \frac{SSE/(n-p-1)}{SST/(n-1)} = 1 - \frac{\hat{\sigma}^2}{s_Y^2}$$

Donde $s_Y^2$ es la varianza muestral de $Y$. El $R^2_{ajustado}$ solo aumentará si la nueva variable mejora el modelo más de lo que se esperaría por puro azar. Es, por tanto, la métrica preferida para comparar la calidad de ajuste de modelos anidados.

¡Excelente observación! Tienes toda la razón. En la sección de "Inferencia" me centré en los contrastes de hipótesis (el test t y el test F) pero omití una parte igualmente importante que sí estaba en tu Tema 1: la **construcción de intervalos de confianza para los parámetros**.

Un contraste de hipótesis te da una respuesta de "sí/no" sobre la significancia, pero un intervalo de confianza te ofrece un rango de valores plausibles para el efecto, lo cual es mucho más informativo.

Aquí tienes una versión revisada y ampliada de la sección **"Inferencia estadística en el modelo múltiple"** que incluye explícitamente los intervalos de confianza, manteniendo el flujo que hemos construido. Puedes reemplazar la sección anterior por esta.

## Inferencia Estadística en el Modelo Múltiple

La estimación nos da los valores de los coeficientes para nuestra muestra, pero la inferencia nos permite usar esos valores para sacar conclusiones sobre los parámetros de la población. ¿Son estos coeficientes "reales" o podrían ser fruto del azar muestral? Para responder, nos basamos en las propiedades distributivas de nuestros estimadores.

### Contraste de hipótesis sobre los coeficientes

El **test t** nos permite decidir si una variable predictora $X_j$ tiene una relación estadísticamente significativa con $Y$, después de controlar por el efecto de todas las demás variables en el modelo.

* **Hipótesis**: La hipótesis nula es que el coeficiente es cero en la población ($H_0: \beta_j = 0$), lo que implicaría que $X_j$ no tiene un efecto lineal sobre $Y$ una vez que se consideran los otros predictores. La alternativa es que el coeficiente es distinto de cero ($H_1: \beta_j \neq 0$).
* **Estadístico de contraste**: Construimos el estadístico t, que mide cuántos errores estándar separan nuestro coeficiente estimado del valor nulo (cero).
    $$\frac{\hat{\beta}_j - \beta_j}{\text{se}(\hat{\beta}_j)} \sim t_{n-p-1}$$
    Bajo la hipótesis nula, el estadístico que calculamos con nuestra muestra es $t_{obs} = \hat{\beta}_j / \text{se}(\hat{\beta}_j)$.
* **Decisión**: Comparamos el valor observado $t_{obs}$ con la distribución t de Student con $n-p-1$ grados de libertad. Si el **p-valor** asociado es suficientemente pequeño (normalmente < 0.05), rechazamos la hipótesis nula y concluimos que la variable es un predictor estadísticamente significativo.

### Intervalo de confianza para los coeficientes

Mientras que el test t nos da una decisión binaria, el **intervalo de confianza** nos proporciona un **rango de valores plausibles** para el verdadero parámetro poblacional $\beta_j$. Es una herramienta de estimación más informativa.

La estructura del intervalo se basa en la distribución t que acabamos de ver:

$$\text{Estimación puntual} \pm (\text{Valor crítico}) \times (\text{Error estándar})$$

Para un nivel de confianza del $100(1-\alpha)\%$, el intervalo para $\beta_j$ es:

$$\hat{\beta}_j \pm t_{\alpha/2, n-p-1} \cdot \text{se}(\hat{\beta}_j)$$

* **Interpretación**: Tenemos una confianza del $100(1-\alpha)\%$ de que el verdadero valor del parámetro poblacional $\beta_j$ se encuentra dentro de este rango.
* **Dualidad con el Contraste de Hipótesis**: Existe una relación directa entre el intervalo de confianza y el test t. Si el valor 0 **no está incluido** en el intervalo de confianza del 95% para $\hat{\beta}_j$, es matemáticamente equivalente a rechazar la hipótesis nula $H_0: \beta_j = 0$ con un nivel de significancia $\alpha=0.05$. Esto nos da dos formas de llegar a la misma conclusión sobre la significancia de un predictor.

### Inferencia sobre la significancia global del modelo 

El **test F** evalúa si el modelo en su conjunto tiene poder predictivo. Es decir, contrasta si **al menos uno** de los predictores tiene una relación significativa con $Y$.

* **Hipótesis**: La hipótesis nula es que todos los coeficientes de las pendientes son simultáneamente cero ($H_0: \beta_1 = \beta_2 = \dots = \beta_p = 0$), frente a la alternativa de que al menos uno es distinto de cero ($H_1: \text{Algún } \beta_j \neq 0$).
* **Estadístico de contraste**: El estadístico F se construye a partir de la tabla ANOVA, comparando la varianza explicada por el modelo con la varianza residual, ajustando por sus respectivos grados de libertad.
    $$F = \frac{\text{Varianza Explicada}}{\text{Varianza No Explicada}} = \frac{SSR / p}{SSE / (n-p-1)}$$
* **Decisión**: Comparamos el valor del estadístico F con una distribución F de Snedecor con $p$ y $n-p-1$ grados de libertad. Un p-valor pequeño indica que el modelo es globalmente significativo y que, como conjunto, nuestros predictores explican una parte de la variabilidad de $Y$ que no es atribuible al azar.

El test F es una herramienta fundamental, ya que representa el primer paso en la validación de cualquier modelo de regresión múltiple.

::: {.callout-warning title="Interpretación de las pruebas estadísticas"}
En nuestro ejemplo de viviendas:

```{r}
#| echo: false
# Obtener estadísticas de inferencia
modelo_summary <- summary(modelo)
f_stat <- modelo_summary$fstatistic
f_pvalue <- pf(f_stat[1], f_stat[2], f_stat[3], lower.tail = FALSE)

# Obtener coeficientes con significancia
coef_table <- tidy(modelo, conf.int = TRUE)
```

**Prueba F global**: 
- F(`r f_stat[2]`, `r f_stat[3]`) = `r round(f_stat[1], 2)`, p < 0.001
- **Conclusión**: El modelo es globalmente significativo. Al menos una variable predictora tiene una relación real con el precio.

**Pruebas t individuales** (ejemplos):
- **Superficie**: t = `r round(coef_table$statistic[2], 2)`, p < 0.001 → **Significativa**
- **Garaje**: t = `r round(coef_table$statistic[6], 2)`, p < 0.001 → **Significativa**

**Intervalos de confianza** (95%):
- **Superficie**: [`r round(coef_table$conf.low[2], 0)`, `r round(coef_table$conf.high[2], 0)`] euros/m²
- No incluye el 0, confirma la significancia estadística

**Interpretación práctica**: Estamos 95% confiados de que el verdadero efecto de la superficie está entre `r round(coef_table$conf.low[2], 0)` y `r round(coef_table$conf.high[2], 0)` euros por m², controlando por las demás variables.
:::

## Predicción con el modelo múltiple

Una vez que hemos ajustado y validado nuestro modelo, podemos utilizarlo para uno de sus propósitos más poderosos: hacer predicciones para nuevas observaciones. Es fundamental distinguir entre dos objetivos de predicción diferentes, ya que cada uno conlleva un nivel de incertidumbre distinto.

Supongamos que tenemos un nuevo conjunto de valores para las variables predictoras, representado por el vector $\mathbf{x}_0^T = [1, x_{01}, x_{02}, \dots, x_{0p}]$. La **predicción puntual** en ambos casos es la misma:

$$\hat{y}_0 = \mathbf{x}_0^T \hat{\boldsymbol{\beta}}$$

Sin embargo, esta estimación puntual está sujeta a error. Para cuantificar esta incertidumbre, construimos dos tipos de intervalos.

### Intervalo de confianza para la respuesta media

Este intervalo responde a la pregunta: *¿cuál es el valor **promedio** de Y para todas las observaciones con las características $\mathbf{x}_0$?* Su objetivo es acotar la posición de la verdadera (pero desconocida) superficie de regresión poblacional en el punto $\mathbf{x}_0$.

La incertidumbre aquí proviene únicamente de la estimación de los coeficientes $\hat{\boldsymbol{\beta}}$. La varianza de esta predicción media es:

$$\text{Var}(\hat{y}_0) = \text{Var}(\mathbf{x}_0^T \hat{\boldsymbol{\beta}}) = \mathbf{x}_0^T \text{Var}(\hat{\boldsymbol{\beta}}) \mathbf{x}_0 = \sigma^2 \mathbf{x}_0^T (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{x}_0$$

Reemplazando $\sigma^2$ por su estimador insesgado $\hat{\sigma}^2$, el **intervalo de confianza al $100(1-\alpha)\%$** para la respuesta media $E[Y|\mathbf{X}=\mathbf{x}_0]$ es:

$$\hat{y}_0 \pm t_{\alpha/2, n-p-1} \cdot \sqrt{\hat{\sigma}^2 \mathbf{x}_0^T (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{x}_0}$$


### Intervalo de predicción para una bbservación individual

Este intervalo responde a una pregunta más ambiciosa: *¿entre qué valores se encontrará la respuesta de una **única y nueva** observación con las características $\mathbf{x}_0$?*

Este intervalo debe considerar **dos fuentes de incertidumbre**:

1.  La incertidumbre sobre la localización de la verdadera superficie de regresión (la misma que en el intervalo de confianza).
2.  La variabilidad aleatoria inherente a una sola observación, que se desvía de la media poblacional (el error $\varepsilon_0$, cuya varianza es $\sigma^2$).

Por esta razón, un intervalo de predicción **siempre será más ancho** que un intervalo de confianza para el mismo nivel de significancia. La varianza del error de predicción es la suma de las dos fuentes de varianza:

$$\text{Var}(y_0 - \hat{y}_0) = \text{Var}(\varepsilon_0) + \text{Var}(\hat{y}_0) = \sigma^2 + \sigma^2 \mathbf{x}_0^T (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{x}_0$$

El **intervalo de predicción al $100(1-\alpha)\%$** para una observación individual $y_0$ es:

$$\hat{y}_0 \pm t_{\alpha/2, n-p-1} \cdot \sqrt{\hat{\sigma}^2 \left(1 + \mathbf{x}_0^T (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{x}_0\right)}$$

La diferencia clave es el **"+1"** dentro de la raíz cuadrada, que representa la varianza de la nueva observación. Visualmente, tanto la banda de confianza como la de predicción son más estrechas cerca del "centroide" de los datos (la media multivariante de los predictores) y se ensanchan a medida que nos alejamos hacia valores más extremos de los predictores.





## Diagnóstico del modelo múltiple {#sec-diagnostico-multiple}

El diagnóstico en regresión múltiple requiere técnicas específicas que
van más allá de las utilizadas en regresión simple. Nos enfocaremos en
los aspectos únicos del caso multivariante.

### Gráficos de residuos parciales

Los gráficos de residuos parciales (added-variable plots) son
específicos de regresión múltiple y ayudan a identificar la contribución
individual de cada variable:

```{r}
#| fig-cap: "Gráficos de residuos parciales (Added-Variable Plots)"
# Gráficos de residuos parciales
avPlots(modelo_completo, layout = c(2, 3))
```

### Gráficos de residuos vs predictores

```{r}
#| fig-cap: "Residuos vs variables predictoras individuales"
# Crear gráficos de residuos vs cada predictor
residuos <- residuals(modelo_completo)

p1 <- ggplot(viviendas, aes(x = superficie, y = residuos)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "loess", se = FALSE, color = "red") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Residuos vs Superficie") +
  theme_minimal()

p2 <- ggplot(viviendas, aes(x = habitaciones, y = residuos)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "loess", se = FALSE, color = "red") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Residuos vs Habitaciones") +
  theme_minimal()

p3 <- ggplot(viviendas, aes(x = antiguedad, y = residuos)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "loess", se = FALSE, color = "red") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Residuos vs Antigüedad") +
  theme_minimal()

p4 <- ggplot(viviendas, aes(x = distancia_centro, y = residuos)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "loess", se = FALSE, color = "red") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Residuos vs Distancia Centro") +
  theme_minimal()

grid.arrange(p1, p2, p3, p4, ncol = 2)
```

### Pruebas estadísticas específicas

Aplicamos las mismas pruebas que en regresión simple, pero interpretadas
en el contexto multivariante:

```{r}
#| echo: false
#| results: 'hide'
suppressPackageStartupMessages(library(lmtest))

# Pruebas de diagnóstico
bp_test <- bptest(modelo_completo)
sw_test <- shapiro.test(residuals(modelo_completo))
dw_test <- dwtest(modelo_completo)
reset_test <- resettest(modelo_completo, power = 2:3, type = "regressor")
```

```{r}
# Resultados de las pruebas
cat("=== DIAGNÓSTICO DEL MODELO MÚLTIPLE ===\n\n")

cat("1. ESPECIFICACIÓN - Prueba RESET:\n")
print(reset_test)
cat("Interpretación: p-valor", ifelse(reset_test$p.value > 0.05, ">", "<"), 
    "0.05 →", ifelse(reset_test$p.value > 0.05, "Modelo bien especificado", "Posible mala especificación"), "\n\n")

cat("2. HOMOCEDASTICIDAD - Prueba Breusch-Pagan:\n")
print(bp_test)
cat("Interpretación: p-valor", ifelse(bp_test$p.value > 0.05, ">", "<"), 
    "0.05 →", ifelse(bp_test$p.value > 0.05, "No hay heterocedasticidad", "Hay heterocedasticidad"), "\n\n")

cat("3. NORMALIDAD - Prueba Shapiro-Wilk:\n")
print(sw_test)
cat("Interpretación: p-valor", ifelse(sw_test$p.value > 0.05, ">", "<"), 
    "0.05 →", ifelse(sw_test$p.value > 0.05, "Residuos normales", "Residuos no normales"), "\n\n")

cat("4. INDEPENDENCIA - Prueba Durbin-Watson:\n")
print(dw_test)
cat("Interpretación: Estadístico ≈ 2 →", ifelse(abs(dw_test$statistic - 2) < 0.5, "Sin autocorrelación", "Posible autocorrelación"), "\n\n")
```

## Multicolinealidad {#sec-multicolinealidad}

La multicolinealidad es uno de los problemas más importantes y
específicos de la regresión múltiple. Ocurre cuando existe una relación
lineal fuerte entre dos o más variables predictoras.

### ¿Qué es la multicolinealidad?

La multicolinealidad **perfecta** existe cuando una variable predictora
es una combinación lineal exacta de otras variables predictoras. Más
comúnmente, encontramos multicolinealidad **imperfecta**, donde existe
una correlación alta pero no perfecta entre predictores.

### Consecuencias de la multicolinealidad

1.  **Estimación inestable**: pequeños cambios en los datos pueden
    producir grandes cambios en los coeficientes
2.  **Errores estándar inflados**: intervalos de confianza muy amplios
3.  **Problemas de interpretación**: dificulta la interpretación de
    coeficientes individuales
4.  **No afecta la predicción**: el $R^2$ y las predicciones no se ven
    afectados

### Detección: factor de inflación de varianza (VIF)

El VIF mide cuánto se infla la varianza de un coeficiente debido a la
colinealidad:

$$VIF_j = \frac{1}{1 - R_j^2}$$

donde $R_j^2$ es el $R^2$ de la regresión de $X_j$ sobre las demás
variables predictoras.

```{r}
# Cálculo de VIF
vif_valores <- vif(modelo_completo)
print(vif_valores)

cat("\nInterpretación de VIF:\n")
cat("- VIF = 1: Sin multicolinealidad\n")
cat("- 1 < VIF < 5: Multicolinealidad moderada\n")
cat("- VIF > 5: Multicolinealidad problemática\n")
cat("- VIF > 10: Multicolinealidad severa\n\n")

for(i in 1:length(vif_valores)) {
  nivel <- ifelse(vif_valores[i] < 1.5, "Baja", 
                  ifelse(vif_valores[i] < 5, "Moderada", 
                         ifelse(vif_valores[i] < 10, "Problemática", "Severa")))
  cat(names(vif_valores)[i], ": VIF =", round(vif_valores[i], 2), "(", nivel, ")\n")
}
```

### Ejemplo de multicolinealidad alta

Vamos a crear un ejemplo con multicolinealidad alta para ilustrar sus
efectos:

```{r}
# Crear variable altamente correlacionada con superficie
viviendas$superficie_alt <- viviendas$superficie * 1.2 + rnorm(n, 0, 5)

# Modelo con multicolinealidad
modelo_multicol <- lm(precio ~ superficie + superficie_alt + habitaciones + 
                      antiguedad + distancia_centro + garaje, data = viviendas)

# VIF del modelo con multicolinealidad
vif_multicol <- vif(modelo_multicol)
print(vif_multicol)

# Comparar errores estándar
se_original <- summary(modelo_completo)$coefficients[2, 2]
se_multicol <- summary(modelo_multicol)$coefficients[2, 2]

cat("\nEfecto en errores estándar:\n")
cat("- Error estándar superficie (modelo original):", round(se_original, 0), "\n")
cat("- Error estándar superficie (con multicolinealidad):", round(se_multicol, 0), "\n")
cat("- Incremento:", round((se_multicol/se_original - 1) * 100, 1), "%\n")
```

### Soluciones para la multicolinealidad

1.  **Eliminación de variables**: remover variables redundantes
2.  **Combinación de variables**: crear índices o variables compuestas
3.  **Regresión ridge o lasso**: técnicas de regularización (Tema 5)
4.  **Análisis de componentes principales**: reducción de
    dimensionalidad

```{r}
# Ejemplo: eliminar variable redundante
modelo_sin_multicol <- lm(precio ~ superficie + habitaciones + antiguedad + 
                          distancia_centro + garaje, data = viviendas)

# Comparar VIF
cat("VIF después de eliminar variable redundante:\n")
print(vif(modelo_sin_multicol))
```


## Análisis de casos especiales {#sec-casos-especiales}

### Variables categóricas con múltiples niveles

```{r}
# Crear variable categórica con múltiples niveles
viviendas$zona <- factor(sample(c("Centro", "Norte", "Sur", "Este", "Oeste"), 
                                nrow(viviendas), replace = TRUE))

# Modelo con variable categórica
modelo_zona <- lm(precio ~ superficie + habitaciones + antiguedad + 
                  distancia_centro + garaje + zona, data = viviendas)

# Resumen del modelo
summary(modelo_zona)$coefficients
```

### Análisis de outliers en regresión múltiple

```{r}
#| fig-cap: "Diagnóstico de observaciones influyentes en regresión múltiple"
# Identificar observaciones influyentes
influence_measures <- influence.measures(modelo_completo)

# Gráficos de diagnóstico específicos para regresión múltiple
par(mfrow = c(2, 2))
plot(modelo_completo, which = 1:4)
par(mfrow = c(1, 1))
```

## Ejemplo completo: análisis paso a paso {#sec-ejemplo-completo}

Vamos a realizar un análisis completo desde el inicio, aplicando todo lo
aprendido:

```{r}
# === PASO 1: EXPLORACIÓN INICIAL ===
cat("=== ANÁLISIS EXPLORATORIO ===\n")
cat("Dimensiones del dataset:", dim(viviendas)[1], "observaciones,", dim(viviendas)[2], "variables\n\n")

# Estadísticas descriptivas
print(summary(viviendas))
```

```{r}
# === PASO 2: MODELO INICIAL ===
modelo_inicial <- lm(precio ~ superficie + habitaciones + antiguedad + 
                     distancia_centro + garaje, data = viviendas)

cat("\n=== MODELO INICIAL ===\n")
print(summary(modelo_inicial))
```

```{r}
# === PASO 3: DIAGNÓSTICO ===
cat("\n=== DIAGNÓSTICO DEL MODELO ===\n")

# VIF
cat("Factor de Inflación de Varianza (VIF):\n")
print(round(vif(modelo_inicial), 2))

# Pruebas estadísticas silenciosas
bp_final <- bptest(modelo_inicial)
sw_final <- shapiro.test(residuals(modelo_inicial))
reset_final <- resettest(modelo_inicial, power = 2:3, type = "regressor")

cat("\nPruebas de diagnóstico:\n")
cat("- Breusch-Pagan (homocedasticidad): p =", round(bp_final$p.value, 4), 
    ifelse(bp_final$p.value > 0.05, " ✓", " ✗"), "\n")
cat("- Shapiro-Wilk (normalidad): p =", round(sw_final$p.value, 4), 
    ifelse(sw_final$p.value > 0.05, " ✓", " ✗"), "\n")
cat("- RESET (especificación): p =", round(reset_final$p.value, 4), 
    ifelse(reset_final$p.value > 0.05, " ✓", " ✗"), "\n")
```

```{r}
# === PASO 4: MODELO FINAL CON MEJORAS ===
# Basándonos en el diagnóstico, podríamos mejorar el modelo
modelo_final <- modelo_inicial  # En este caso, el modelo inicial es bueno

cat("\n=== RESUMEN FINAL ===\n")
cat("R² =", round(summary(modelo_final)$r.squared, 4), "\n")
cat("R² ajustado =", round(summary(modelo_final)$adj.r.squared, 4), "\n")
cat("Error estándar residual =", round(summary(modelo_final)$sigma, 0), "euros\n")
```

## Interacciones entre variables {#sec-interacciones}

Las interacciones permiten que el efecto de una variable predictora
dependa del nivel de otra variable. Son especialmente importantes cuando
las variables actúan de manera sinérgica.

### Concepto de interacción

Una interacción entre $X_1$ y $X_2$ se modela como:

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \varepsilon$$

El coeficiente $\beta_3$ representa el **cambio adicional** en el efecto
de $X_1$ por unidad de cambio en $X_2$.

### Interacción entre variables continuas

```{r}
# Modelo con interacción superficie × habitaciones
modelo_interaccion <- lm(precio ~ superficie * habitaciones + antiguedad + 
                         distancia_centro + garaje, data = viviendas)

summary(modelo_interaccion)$coefficients
```

### Interacción entre variable continua y categórica

```{r}
# Modelo con interacción superficie × garaje
modelo_int_garaje <- lm(precio ~ superficie * garaje + habitaciones + 
                        antiguedad + distancia_centro, data = viviendas)

summary(modelo_int_garaje)
```

```{r}
#| fig-cap: "Interacción entre superficie y presencia de garaje"
# Visualización de la interacción
ggplot(viviendas, aes(x = superficie, y = precio, color = garaje)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE, formula = y ~ x) +
  labs(title = "Interacción Superficie × Garaje",
       x = "Superficie (m²)", y = "Precio (euros)",
       color = "Garaje") +
  theme_minimal() +
  scale_color_manual(values = c("red", "blue"))
```

### Interpretación de interacciones

```{r}
# Coeficientes del modelo con interacción
coef_int <- coef(modelo_int_garaje)
print(coef_int)

cat("\nInterpretación de la interacción superficie × garaje:\n")
cat("- Para viviendas SIN garaje: efecto de superficie =", round(coef_int[2], 0), "euros/m²\n")
cat("- Para viviendas CON garaje: efecto de superficie =", round(coef_int[2] + coef_int[6], 0), "euros/m²\n")
cat("- Diferencia en el efecto de superficie =", round(coef_int[6], 0), "euros/m²\n")
```

::: {.callout-tip title="Mejores prácticas en regresión múltiple"}
## Flujo de trabajo recomendado

1. **Exploración inicial**: Examine correlaciones entre variables y distribuciones antes de ajustar el modelo
2. **Modelo base**: Comience con efectos principales antes de considerar interacciones
3. **Diagnóstico**: Verifique supuestos (linealidad, homocedasticidad, normalidad, independencia)
4. **Multicolinealidad**: Use VIF > 5 como señal de alarma
5. **Interpretación contextual**: Los coeficientes son efectos *ceteris paribus* (todo lo demás constante)
6. **Validación**: Use datos independientes cuando sea posible

## Puntos clave para recordar

- **Interpretación condicional**: Los coeficientes deben interpretarse manteniendo las demás variables constantes
- **Multicolinealidad**: Es el problema más específico de regresión múltiple
- **Transformaciones**: Son herramientas poderosas para capturar relaciones no lineales
- **Diagnóstico específico**: Use gráficos de residuos parciales y pruebas específicas para el caso multivariante
- **Significancia vs. importancia práctica**: Un efecto estadísticamente significativo no siempre es prácticamente relevante

## Preparación para temas avanzados

Los conceptos de este capítulo son fundamentales para:
- **Selección de variables** (Tema 3): decidir qué variables incluir
- **Modelos lineales generalizados** (Tema 4): extensión a variables respuesta no normales  
- **Técnicas modernas** (Tema 5): regularización y métodos no paramétricos
:::