---
editor: 
  markdown: 
    wrap: 72
---

# El modelo de regresión lineal múltiple {#sec-regresion-lineal-multiple}

El modelo de regresión lineal múltiple constituye la extensión natural y
más potente del modelo simple que estudiamos en el capítulo anterior.
Mientras que la regresión simple nos permitía examinar la relación entre
una variable respuesta y un único predictor, la regresión múltiple nos
capacita para **modelar simultáneamente el efecto de múltiples variables
predictoras**, una situación mucho más realista en la mayoría de
aplicaciones prácticas [@kutner2005applied; @james2021; @fox2018r].

En este capítulo profundizaremos en los aspectos únicos de la regresión
múltiple que no están presentes en el caso simple: la **interpretación
de coeficientes en presencia de otros predictores**, las
**transformaciones de variables** para capturar relaciones no lineales,
las **interacciones** entre variables, el **diagnóstico específico** del
modelo múltiple, y el problema crucial de la **multicolinealidad**.
Estos conceptos son fundamentales para desarrollar modelos predictivos
robustos y interpretables [@harrell2015; @draper1998applied].

La comprensión sólida de estos principios es esencial, ya que
constituyen la base conceptual sobre la que se construyen técnicas más
avanzadas como la selección de variables (Tema 3), los modelos lineales
generalizados (Tema 4), y las tendencias modernas en modelado
estadístico (Tema 5).

::: {.callout-important title="Objetivos de aprendizaje"}
Al finalizar este capítulo, serás capaz de:

1.  **Formular y estimar** modelos de regresión lineal múltiple,
    comprendiendo las diferencias clave respecto al caso simple.
2.  **Interpretar coeficientes** en el contexto multivariante,
    entendiendo el concepto de "manteniendo las demás variables
    constantes".
3.  **Aplicar transformaciones de variables** para capturar relaciones
    no lineales y mejorar el ajuste del modelo.
4.  **Modelar e interpretar interacciones** entre variables predictoras
    y entender cuándo son necesarias.
5.  **Diagnosticar el modelo múltiple**, aplicando técnicas específicas
    para detectar problemas únicos del caso multivariante.
6.  **Identificar y tratar la multicolinealidad**, comprendiendo sus
    causas, consecuencias y soluciones.
7.  **Evaluar y comparar modelos múltiples** usando criterios apropiados
    para la selección del mejor modelo.
:::

## Conceptos fundamentales {#sec-conceptos-fundamentales}

### Del modelo simple al múltiple: Una perspectiva teórica

El modelo de regresión lineal múltiple representa una extensión natural pero conceptualmente rica del modelo simple. Para $n$ observaciones y $p$ variables predictoras, el modelo se expresa como:

$$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + ... + \beta_p X_{ip} + \varepsilon_i, \quad i = 1, 2, ..., n$$

donde:
- $Y_i$ es la $i$-ésima observación de la variable respuesta
- $X_{ij}$ es la $i$-ésima observación de la $j$-ésima variable predictora
- $\beta_0, \beta_1, ..., \beta_p$ son los parámetros del modelo
- $\varepsilon_i$ es el término de error para la $i$-ésima observación

### Formulación matricial del modelo

La representación matricial es fundamental para el desarrollo teórico:

$$\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}$$

donde:

$$\mathbf{y} = \begin{bmatrix} Y_1 \\ Y_2 \\ \vdots \\ Y_n \end{bmatrix}, \quad
\mathbf{X} = \begin{bmatrix} 
1 & X_{11} & X_{12} & \cdots & X_{1p} \\
1 & X_{21} & X_{22} & \cdots & X_{2p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & X_{n1} & X_{n2} & \cdots & X_{np}
\end{bmatrix}$$

$$\boldsymbol{\beta} = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{bmatrix}, \quad
\boldsymbol{\varepsilon} = \begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n \end{bmatrix}$$

La matriz $\mathbf{X}$ es de dimensión $n \times (p+1)$ y se denomina **matriz de diseño**.

### Supuestos fundamentales del modelo

Los supuestos del modelo de regresión lineal múltiple, conocidos como **condiciones de Gauss-Markov**, son:

#### **Supuesto 1: Linealidad**
$$E[\mathbf{y}|\mathbf{X}] = \mathbf{X}\boldsymbol{\beta}$$

La esperanza condicional de $\mathbf{y}$ es una función lineal de los parámetros $\boldsymbol{\beta}$.

#### **Supuesto 2: Independencia**
$$E[\varepsilon_i \varepsilon_j] = 0, \quad \forall i \neq j$$

Los errores son no correlacionados entre observaciones.

#### **Supuesto 3: Homocedasticidad**
$$\text{Var}(\varepsilon_i) = \sigma^2, \quad \forall i = 1, 2, ..., n$$

La varianza del error es constante para todas las observaciones.

#### **Supuesto 4: Media nula**
$$E[\varepsilon_i] = 0, \quad \forall i = 1, 2, ..., n$$

Los errores tienen esperanza cero.

#### **Supuesto 5: Rango completo**
$$\text{rango}(\mathbf{X}) = p + 1 < n$$

La matriz $\mathbf{X}$ tiene rango completo (no hay multicolinealidad perfecta).

**Formulación matricial conjunta:**
$$E[\boldsymbol{\varepsilon}] = \mathbf{0}, \quad \text{Var}(\boldsymbol{\varepsilon}) = \sigma^2 \mathbf{I}_n$$

#### **Supuesto adicional para inferencia:**

**Normalidad**: 
$$\boldsymbol{\varepsilon} \sim N(\mathbf{0}, \sigma^2 \mathbf{I}_n)$$

Este supuesto no es necesario para las propiedades BLUE de los estimadores, pero sí para la inferencia estadística exacta.

### Geometría de la regresión múltiple

#### **Espacios vectoriales**

En regresión múltiple, trabajamos en el espacio $\mathbb{R}^n$:

- El vector de observaciones $\mathbf{y} \in \mathbb{R}^n$
- Cada columna de $\mathbf{X}$ (incluyendo el vector de unos) genera un subespacio
- El **espacio columna** de $\mathbf{X}$, denotado $C(\mathbf{X})$, es el subespacio generado por las columnas de $\mathbf{X}$

#### **Proyección ortogonal**

El vector de valores ajustados $\hat{\mathbf{y}} = \mathbf{X}\hat{\boldsymbol{\beta}}$ es la **proyección ortogonal** de $\mathbf{y}$ sobre $C(\mathbf{X})$:

$$\hat{\mathbf{y}} = \text{Proj}_{C(\mathbf{X})} \mathbf{y}$$

La matriz de proyección es:
$$\mathbf{H} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T$$

con las propiedades:
- $\mathbf{H}\mathbf{H} = \mathbf{H}$ (idempotente)
- $\mathbf{H}^T = \mathbf{H}$ (simétrica)
- $\text{traza}(\mathbf{H}) = p + 1$ (número de parámetros)

#### **Descomposición ortogonal**

$$\mathbf{y} = \hat{\mathbf{y}} + \mathbf{e} = \mathbf{H}\mathbf{y} + (\mathbf{I}_n - \mathbf{H})\mathbf{y}$$

donde $\mathbf{e} = \mathbf{y} - \hat{\mathbf{y}}$ es el vector de residuos, y se cumple:

$$\hat{\mathbf{y}}^T \mathbf{e} = 0$$

Esta ortogonalidad es fundamental para el Teorema de Pitágoras en regresión.

## Estimación de parámetros {#sec-estimacion-parametros}

### Derivación de los estimadores de mínimos cuadrados

#### **Función objetivo**

Buscamos minimizar la suma de cuadrados de errores:

$$SSE(\boldsymbol{\beta}) = \sum_{i=1}^n (Y_i - \beta_0 - \beta_1 X_{i1} - ... - \beta_p X_{ip})^2$$

En notación matricial:
$$SSE(\boldsymbol{\beta}) = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^T(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})$$

#### **Desarrollo de la función objetivo**

Expandiendo:
\begin{align}
SSE(\boldsymbol{\beta}) &= \mathbf{y}^T\mathbf{y} - \boldsymbol{\beta}^T\mathbf{X}^T\mathbf{y} - \mathbf{y}^T\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\beta}^T\mathbf{X}^T\mathbf{X}\boldsymbol{\beta}\\
&= \mathbf{y}^T\mathbf{y} - 2\boldsymbol{\beta}^T\mathbf{X}^T\mathbf{y} + \boldsymbol{\beta}^T\mathbf{X}^T\mathbf{X}\boldsymbol{\beta}
\end{align}

#### **Condiciones de primer orden**

Derivando respecto a $\boldsymbol{\beta}$ e igualando a cero:

$$\frac{\partial SSE}{\partial \boldsymbol{\beta}} = -2\mathbf{X}^T\mathbf{y} + 2\mathbf{X}^T\mathbf{X}\boldsymbol{\beta} = \mathbf{0}$$

#### **Ecuaciones normales**

$$\mathbf{X}^T\mathbf{X}\hat{\boldsymbol{\beta}} = \mathbf{X}^T\mathbf{y}$$

Si $\mathbf{X}^T\mathbf{X}$ es no singular (rango completo), la solución única es:

$$\boxed{\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}}$$

#### **Condiciones de segundo orden**

La matriz Hessiana es:
$$\frac{\partial^2 SSE}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^T} = 2\mathbf{X}^T\mathbf{X}$$

Como $\mathbf{X}^T\mathbf{X}$ es positiva definida (bajo el supuesto de rango completo), $\hat{\boldsymbol{\beta}}$ es efectivamente un mínimo global.

### Propiedades de los estimadores

#### **Teorema de Gauss-Markov**

**Enunciado**: Bajo los supuestos 1-5, los estimadores de mínimos cuadrados $\hat{\boldsymbol{\beta}}$ son BLUE (Best Linear Unbiased Estimators).

**Demostración de insesgadez:**

\begin{align}
E[\hat{\boldsymbol{\beta}}] &= E[(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}]\\
&= E[(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T(\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon})]\\
&= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}\boldsymbol{\beta} + (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T E[\boldsymbol{\varepsilon}]\\
&= \boldsymbol{\beta}
\end{align}

**Matriz de varianzas-covarianzas:**

\begin{align}
\text{Var}(\hat{\boldsymbol{\beta}}) &= \text{Var}[(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}]\\
&= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T \text{Var}(\mathbf{y}) \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\\
&= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T (\sigma^2 \mathbf{I}_n) \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\\
&= \sigma^2(\mathbf{X}^T\mathbf{X})^{-1}
\end{align}

Por tanto: $$\boxed{\text{Var}(\hat{\boldsymbol{\beta}}) = \sigma^2(\mathbf{X}^T\mathbf{X})^{-1}}$$

#### **Distribución de los estimadores**

Bajo el supuesto de normalidad:

$$\hat{\boldsymbol{\beta}} \sim N(\boldsymbol{\beta}, \sigma^2(\mathbf{X}^T\mathbf{X})^{-1})$$

**Corolario**: Para cada coeficiente individual:
$$\hat{\beta}_j \sim N\left(\beta_j, \sigma^2 [(\mathbf{X}^T\mathbf{X})^{-1}]_{jj}\right)$$

### Estimación de la varianza del error

#### **Suma de cuadrados de errores**

$$SSE = \mathbf{e}^T\mathbf{e} = (\mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}})^T(\mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}})$$

Usando $\hat{\mathbf{y}} = \mathbf{H}\mathbf{y}$ y $\mathbf{e} = (\mathbf{I}_n - \mathbf{H})\mathbf{y}$:

$$SSE = \mathbf{y}^T(\mathbf{I}_n - \mathbf{H})\mathbf{y}$$

#### **Esperanza de SSE**

**Lema**: $E[\mathbf{y}^T\mathbf{A}\mathbf{y}] = \text{traza}(\mathbf{A}\boldsymbol{\Sigma}) + \boldsymbol{\mu}^T\mathbf{A}\boldsymbol{\mu}$

Para nuestro caso, con $\mathbf{A} = \mathbf{I}_n - \mathbf{H}$, $\boldsymbol{\Sigma} = \sigma^2\mathbf{I}_n$, y $\boldsymbol{\mu} = \mathbf{X}\boldsymbol{\beta}$:

\begin{align}
E[SSE] &= \text{traza}[(\mathbf{I}_n - \mathbf{H})\sigma^2\mathbf{I}_n] + \boldsymbol{\beta}^T\mathbf{X}^T(\mathbf{I}_n - \mathbf{H})\mathbf{X}\boldsymbol{\beta}\\
&= \sigma^2 \text{traza}(\mathbf{I}_n - \mathbf{H}) + 0\\
&= \sigma^2 (n - p - 1)
\end{align}

#### **Estimador insesgado de σ²**

$$\boxed{\hat{\sigma}^2 = \frac{SSE}{n - p - 1} = \frac{\mathbf{e}^T\mathbf{e}}{n - p - 1}}$$

**Distribución**: Bajo normalidad,
$$\frac{SSE}{\sigma^2} \sim \chi^2_{n-p-1}$$

#### **Error estándar de los coeficientes**

$$\text{se}(\hat{\beta}_j) = \sqrt{\hat{\sigma}^2 [(\mathbf{X}^T\mathbf{X})^{-1}]_{jj}}$$

### Interpretación de coeficientes en regresión múltiple

#### **Concepto fundamental**

En regresión múltiple, cada coeficiente $\beta_j$ representa:

> **El cambio esperado en $Y$ por unidad de cambio en $X_j$, manteniendo todas las demás variables predictoras constantes.**

Matemáticamente:
$$\beta_j = \frac{\partial E[Y|\mathbf{X}]}{\partial X_j}$$

#### **Diferencia con correlaciones simples**

El coeficiente de regresión múltiple $\hat{\beta}_j$ es **diferente** del coeficiente de regresión simple de $Y$ sobre $X_j$:

- **Regresión simple**: $\hat{\beta}_j^{(simple)} = \frac{\text{Cov}(Y, X_j)}{\text{Var}(X_j)}$
- **Regresión múltiple**: $\hat{\beta}_j$ está "ajustado" por las demás variables

#### **Relación con correlaciones parciales**

El coeficiente de regresión múltiple está relacionado con la **correlación parcial**:

$$r_{Y,X_j|X_{-j}} = \text{corr}(Y, X_j | X_1, ..., X_{j-1}, X_{j+1}, ..., X_p)$$

Esta es la correlación entre $Y$ y $X_j$ después de eliminar el efecto de las demás variables.

### Propiedades de los residuos y valores ajustados

#### **Propiedades de los residuos**

Los residuos $\mathbf{e} = \mathbf{y} - \hat{\mathbf{y}}$ satisfacen:

1. **Suma cero**: $\sum_{i=1}^n e_i = \mathbf{1}^T\mathbf{e} = 0$
2. **Ortogonalidad con predictores**: $\mathbf{X}^T\mathbf{e} = \mathbf{0}$
3. **Ortogonalidad con valores ajustados**: $\hat{\mathbf{y}}^T\mathbf{e} = 0$

#### **Propiedades de los valores ajustados**

Los valores ajustados $\hat{\mathbf{y}} = \mathbf{X}\hat{\boldsymbol{\beta}}$ satisfacen:

1. **Media**: $E[\hat{Y}_i] = E[Y_i] = \mathbf{x}_i^T\boldsymbol{\beta}$
2. **Varianza**: $\text{Var}(\hat{Y}_i) = \sigma^2 \mathbf{h}_{ii}$ donde $\mathbf{h}_{ii} = \mathbf{x}_i^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i$
3. **Covarianza con residuos**: $\text{Cov}(\hat{Y}_i, e_i) = 0$

#### **Descomposición de varianza**

**Teorema de Pitágoras en regresión**:
$$\|\mathbf{y}\|^2 = \|\hat{\mathbf{y}}\|^2 + \|\mathbf{e}\|^2$$

En términos de sumas de cuadrados:
$$\sum_{i=1}^n (Y_i - \bar{Y})^2 = \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2 + \sum_{i=1}^n e_i^2$$

$$\boxed{SST = SSR + SSE}$$

donde:
- $SST = \sum_{i=1}^n (Y_i - \bar{Y})^2$ (suma total de cuadrados)
- $SSR = \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2$ (suma de cuadrados de la regresión)
- $SSE = \sum_{i=1}^n e_i^2$ (suma de cuadrados del error)

### Inferencia estadística

#### **Distribuciones para inferencia**

Bajo normalidad:

1. **Coeficientes individuales**:
   $$\frac{\hat{\beta}_j - \beta_j}{\text{se}(\hat{\beta}_j)} \sim t_{n-p-1}$$

2. **Combinaciones lineales**:
   $$\frac{\mathbf{c}^T\hat{\boldsymbol{\beta}} - \mathbf{c}^T\boldsymbol{\beta}}}{\sqrt{\hat{\sigma}^2 \mathbf{c}^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{c}}} \sim t_{n-p-1}$$

3. **Pruebas F**:
   $$\frac{(\mathbf{L}\hat{\boldsymbol{\beta}} - \mathbf{c})^T[\mathbf{L}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{L}^T]^{-1}(\mathbf{L}\hat{\boldsymbol{\beta}} - \mathbf{c})/q}{\hat{\sigma}^2} \sim F_{q, n-p-1}$$

#### **Coeficiente de determinación múltiple**

$$R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}$$

**Propiedades**:
- $0 \leq R^2 \leq 1$
- $R^2$ nunca decrece al agregar variables
- $R^2 = \text{corr}^2(\mathbf{y}, \hat{\mathbf{y}})$

**R² ajustado**:
$$R^2_{adj} = 1 - \frac{SSE/(n-p-1)}{SST/(n-1)} = 1 - \frac{\hat{\sigma}^2}{s_Y^2}$$

donde $s_Y^2 = SST/(n-1)$ es la varianza muestral de $Y$.

-   **Insesgadez**: $E[\hat{\boldsymbol{\beta}}] = \boldsymbol{\beta}$
-   **Varianza mínima**: Entre todos los estimadores lineales insesgados
-   **Matriz de varianzas-covarianzas**:
    $\text{Var}(\hat{\boldsymbol{\beta}}) = \sigma^2(\mathbf{X}^T\mathbf{X})^{-1}$

## Ejemplo práctico: Datos de viviendas {#sec-ejemplo-practico}

Vamos a desarrollar un ejemplo completo usando datos de precios de
viviendas, que nos permitirá ilustrar todos los conceptos del capítulo.

```{r}
#| warning: false
#| message: false
# Configuración inicial
suppressPackageStartupMessages({
  library(ggplot2)
  library(dplyr)
  library(broom)
  library(car)
  library(corrplot)
  library(gridExtra)
})

# Crear datos simulados realistas de precios de viviendas
set.seed(123)
n <- 200

# Variables predictoras
superficie <- rnorm(n, 120, 30)  # m2
habitaciones <- round(2 + 0.02*superficie + rnorm(n, 0, 0.8))  # número
antiguedad <- sample(0:50, n, replace = TRUE)  # años
distancia_centro <- runif(n, 1, 25)  # km
garaje <- rbinom(n, 1, 0.6)  # 0/1

# Variable respuesta con relaciones realistas
precio <- 50000 + 
  1200*superficie + 
  15000*habitaciones - 
  800*antiguedad - 
  2000*distancia_centro + 
  25000*garaje + 
  rnorm(n, 0, 15000)

# Crear data frame
viviendas <- data.frame(
  precio = round(precio),
  superficie = round(superficie),
  habitaciones = pmax(1, habitaciones),  # mínimo 1 habitación
  antiguedad = antiguedad,
  distancia_centro = round(distancia_centro, 1),
  garaje = factor(garaje, labels = c("No", "Sí"))
)

# Mostrar las primeras observaciones
head(viviendas)
```

```{r}
#| fig-cap: "Matriz de correlaciones entre variables numéricas del dataset de viviendas"
# Exploración inicial: matriz de correlaciones
viviendas_num <- viviendas %>% 
  select(-garaje) %>%
  mutate(garaje_num = as.numeric(viviendas$garaje) - 1)

cor_matrix <- cor(viviendas_num)
corrplot(cor_matrix, method = "color", type = "upper", 
         addCoef.col = "black", tl.cex = 0.8, number.cex = 0.8)
```

### Modelo múltiple inicial

Comenzamos ajustando un modelo que incluye todas las variables
disponibles:

```{r}
# Modelo múltiple completo
modelo_completo <- lm(precio ~ superficie + habitaciones + antiguedad + 
                      distancia_centro + garaje, data = viviendas)

summary(modelo_completo)
```

### Interpretación de coeficientes

Analicemos la interpretación de cada coeficiente:

```{r}
# Extraer coeficientes con intervalos de confianza
coef_tabla <- tidy(modelo_completo, conf.int = TRUE) %>%
  mutate(
    estimate = round(estimate, 0),
    std.error = round(std.error, 0),
    conf.low = round(conf.low, 0),
    conf.high = round(conf.high, 0)
  )

print(coef_tabla)
```

**Interpretación:**

-   **Superficie**: Por cada m² adicional, el precio aumenta en promedio
    `r round(coef(modelo_completo)[2], 0)` euros, manteniendo constantes
    las demás variables.
-   **Habitaciones**: Cada habitación adicional incrementa el precio en
    `r round(coef(modelo_completo)[3], 0)` euros en promedio, ceteris
    paribus.
-   **Antigüedad**: Por cada año de antigüedad, el precio disminuye en
    `r abs(round(coef(modelo_completo)[4], 0))` euros en promedio.
-   **Distancia al centro**: Cada km adicional de distancia reduce el
    precio en `r abs(round(coef(modelo_completo)[5], 0))` euros en
    promedio.
-   **Garaje**: Las viviendas con garaje cuestan
    `r round(coef(modelo_completo)[6], 0)` euros más que las que no
    tienen, en promedio.

## Transformaciones de variables {#sec-transformaciones-variables}

Las transformaciones nos permiten capturar relaciones no lineales entre
las variables manteniendo la estructura del modelo lineal. Las
transformaciones más comunes incluyen logarítmicas, polinomiales, y de
raíz cuadrada.

### Transformación logarítmica

La transformación logarítmica es útil cuando esperamos efectos
multiplicativos o cuando la variabilidad de la respuesta aumenta con su
nivel:

```{r}
#| fig-cap: "Comparación entre modelo lineal y logarítmico para la relación precio-superficie"
# Modelo con transformación logarítmica del precio
modelo_log <- lm(log(precio) ~ superficie + habitaciones + antiguedad + 
                 distancia_centro + garaje, data = viviendas)

# Comparación gráfica
p1 <- ggplot(viviendas, aes(x = superficie, y = precio)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Modelo Lineal", y = "Precio (euros)") +
  theme_minimal()

p2 <- ggplot(viviendas, aes(x = superficie, y = log(precio))) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(title = "Modelo Logarítmico", y = "log(Precio)") +
  theme_minimal()

grid.arrange(p1, p2, ncol = 2)
```

### Transformación polinomial

Los términos polinomiales capturan relaciones curvilíneas:

```{r}
# Modelo con término cuadrático para superficie
modelo_poly <- lm(precio ~ superficie + I(superficie^2) + habitaciones + 
                  antiguedad + distancia_centro + garaje, data = viviendas)

summary(modelo_poly)$coefficients
```

```{r}
#| fig-cap: "Efecto cuadrático de la superficie sobre el precio"
# Visualización del efecto polinomial
superficie_seq <- seq(min(viviendas$superficie), max(viviendas$superficie), length.out = 100)
pred_data <- data.frame(
  superficie = superficie_seq,
  habitaciones = mean(viviendas$habitaciones),
  antiguedad = mean(viviendas$antiguedad),
  distancia_centro = mean(viviendas$distancia_centro),
  garaje = "No"
)

pred_data$precio_pred <- predict(modelo_poly, pred_data)

ggplot() +
  geom_point(data = viviendas, aes(x = superficie, y = precio), alpha = 0.6) +
  geom_line(data = pred_data, aes(x = superficie, y = precio_pred), 
            color = "red", size = 1.2) +
  labs(title = "Modelo con término cuadrático",
       x = "Superficie (m²)", y = "Precio (euros)") +
  theme_minimal()
```

### Transformación de variables predictoras

También podemos transformar las variables predictoras:

```{r}
# Modelo con transformación logarítmica de distancia
modelo_log_dist <- lm(precio ~ superficie + habitaciones + antiguedad + 
                      log(distancia_centro + 1) + garaje, data = viviendas)

# Comparar modelos
AIC(modelo_completo, modelo_log, modelo_poly, modelo_log_dist)
```

## Interacciones entre variables {#sec-interacciones}

Las interacciones permiten que el efecto de una variable predictora
dependa del nivel de otra variable. Son especialmente importantes cuando
las variables actúan de manera sinérgica.

### Concepto de interacción

Una interacción entre $X_1$ y $X_2$ se modela como:

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \varepsilon$$

El coeficiente $\beta_3$ representa el **cambio adicional** en el efecto
de $X_1$ por unidad de cambio en $X_2$.

### Interacción entre variables continuas

```{r}
# Modelo con interacción superficie × habitaciones
modelo_interaccion <- lm(precio ~ superficie * habitaciones + antiguedad + 
                         distancia_centro + garaje, data = viviendas)

summary(modelo_interaccion)$coefficients
```

### Interacción entre variable continua y categórica

```{r}
# Modelo con interacción superficie × garaje
modelo_int_garaje <- lm(precio ~ superficie * garaje + habitaciones + 
                        antiguedad + distancia_centro, data = viviendas)

summary(modelo_int_garaje)
```

```{r}
#| fig-cap: "Interacción entre superficie y presencia de garaje"
# Visualización de la interacción
ggplot(viviendas, aes(x = superficie, y = precio, color = garaje)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE, formula = y ~ x) +
  labs(title = "Interacción Superficie × Garaje",
       x = "Superficie (m²)", y = "Precio (euros)",
       color = "Garaje") +
  theme_minimal() +
  scale_color_manual(values = c("red", "blue"))
```

### Interpretación de interacciones

```{r}
# Coeficientes del modelo con interacción
coef_int <- coef(modelo_int_garaje)
print(coef_int)

cat("\nInterpretación de la interacción superficie × garaje:\n")
cat("- Para viviendas SIN garaje: efecto de superficie =", round(coef_int[2], 0), "euros/m²\n")
cat("- Para viviendas CON garaje: efecto de superficie =", round(coef_int[2] + coef_int[6], 0), "euros/m²\n")
cat("- Diferencia en el efecto de superficie =", round(coef_int[6], 0), "euros/m²\n")
```

## Diagnóstico del modelo múltiple {#sec-diagnostico-multiple}

El diagnóstico en regresión múltiple requiere técnicas específicas que
van más allá de las utilizadas en regresión simple. Nos enfocaremos en
los aspectos únicos del caso multivariante.

### Gráficos de residuos parciales

Los gráficos de residuos parciales (added-variable plots) son
específicos de regresión múltiple y ayudan a identificar la contribución
individual de cada variable:

```{r}
#| fig-cap: "Gráficos de residuos parciales (Added-Variable Plots)"
# Gráficos de residuos parciales
avPlots(modelo_completo, layout = c(2, 3))
```

### Gráficos de residuos vs predictores

```{r}
#| fig-cap: "Residuos vs variables predictoras individuales"
# Crear gráficos de residuos vs cada predictor
residuos <- residuals(modelo_completo)

p1 <- ggplot(viviendas, aes(x = superficie, y = residuos)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "loess", se = FALSE, color = "red") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Residuos vs Superficie") +
  theme_minimal()

p2 <- ggplot(viviendas, aes(x = habitaciones, y = residuos)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "loess", se = FALSE, color = "red") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Residuos vs Habitaciones") +
  theme_minimal()

p3 <- ggplot(viviendas, aes(x = antiguedad, y = residuos)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "loess", se = FALSE, color = "red") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Residuos vs Antigüedad") +
  theme_minimal()

p4 <- ggplot(viviendas, aes(x = distancia_centro, y = residuos)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "loess", se = FALSE, color = "red") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Residuos vs Distancia Centro") +
  theme_minimal()

grid.arrange(p1, p2, p3, p4, ncol = 2)
```

### Pruebas estadísticas específicas

Aplicamos las mismas pruebas que en regresión simple, pero interpretadas
en el contexto multivariante:

```{r}
#| echo: false
#| results: 'hide'
suppressPackageStartupMessages(library(lmtest))

# Pruebas de diagnóstico
bp_test <- bptest(modelo_completo)
sw_test <- shapiro.test(residuals(modelo_completo))
dw_test <- dwtest(modelo_completo)
reset_test <- resettest(modelo_completo, power = 2:3, type = "regressor")
```

```{r}
# Resultados de las pruebas
cat("=== DIAGNÓSTICO DEL MODELO MÚLTIPLE ===\n\n")

cat("1. ESPECIFICACIÓN - Prueba RESET:\n")
print(reset_test)
cat("Interpretación: p-valor", ifelse(reset_test$p.value > 0.05, ">", "<"), 
    "0.05 →", ifelse(reset_test$p.value > 0.05, "Modelo bien especificado", "Posible mala especificación"), "\n\n")

cat("2. HOMOCEDASTICIDAD - Prueba Breusch-Pagan:\n")
print(bp_test)
cat("Interpretación: p-valor", ifelse(bp_test$p.value > 0.05, ">", "<"), 
    "0.05 →", ifelse(bp_test$p.value > 0.05, "No hay heterocedasticidad", "Hay heterocedasticidad"), "\n\n")

cat("3. NORMALIDAD - Prueba Shapiro-Wilk:\n")
print(sw_test)
cat("Interpretación: p-valor", ifelse(sw_test$p.value > 0.05, ">", "<"), 
    "0.05 →", ifelse(sw_test$p.value > 0.05, "Residuos normales", "Residuos no normales"), "\n\n")

cat("4. INDEPENDENCIA - Prueba Durbin-Watson:\n")
print(dw_test)
cat("Interpretación: Estadístico ≈ 2 →", ifelse(abs(dw_test$statistic - 2) < 0.5, "Sin autocorrelación", "Posible autocorrelación"), "\n\n")
```

## Multicolinealidad {#sec-multicolinealidad}

La multicolinealidad es uno de los problemas más importantes y
específicos de la regresión múltiple. Ocurre cuando existe una relación
lineal fuerte entre dos o más variables predictoras.

### ¿Qué es la multicolinealidad?

La multicolinealidad **perfecta** existe cuando una variable predictora
es una combinación lineal exacta de otras variables predictoras. Más
comúnmente, encontramos multicolinealidad **imperfecta**, donde existe
una correlación alta pero no perfecta entre predictores.

### Consecuencias de la multicolinealidad

1.  **Estimación inestable**: Pequeños cambios en los datos pueden
    producir grandes cambios en los coeficientes
2.  **Errores estándar inflados**: Intervalos de confianza muy amplios
3.  **Problemas de interpretación**: Dificulta la interpretación de
    coeficientes individuales
4.  **No afecta la predicción**: El $R^2$ y las predicciones no se ven
    afectados

### Detección: Factor de Inflación de Varianza (VIF)

El VIF mide cuánto se infla la varianza de un coeficiente debido a la
colinealidad:

$$VIF_j = \frac{1}{1 - R_j^2}$$

donde $R_j^2$ es el $R^2$ de la regresión de $X_j$ sobre las demás
variables predictoras.

```{r}
# Cálculo de VIF
vif_valores <- vif(modelo_completo)
print(vif_valores)

cat("\nInterpretación de VIF:\n")
cat("- VIF = 1: Sin multicolinealidad\n")
cat("- 1 < VIF < 5: Multicolinealidad moderada\n")
cat("- VIF > 5: Multicolinealidad problemática\n")
cat("- VIF > 10: Multicolinealidad severa\n\n")

for(i in 1:length(vif_valores)) {
  nivel <- ifelse(vif_valores[i] < 1.5, "Baja", 
                  ifelse(vif_valores[i] < 5, "Moderada", 
                         ifelse(vif_valores[i] < 10, "Problemática", "Severa")))
  cat(names(vif_valores)[i], ": VIF =", round(vif_valores[i], 2), "(", nivel, ")\n")
}
```

### Ejemplo de multicolinealidad alta

Vamos a crear un ejemplo con multicolinealidad alta para ilustrar sus
efectos:

```{r}
# Crear variable altamente correlacionada con superficie
viviendas$superficie_alt <- viviendas$superficie * 1.2 + rnorm(n, 0, 5)

# Modelo con multicolinealidad
modelo_multicol <- lm(precio ~ superficie + superficie_alt + habitaciones + 
                      antiguedad + distancia_centro + garaje, data = viviendas)

# VIF del modelo con multicolinealidad
vif_multicol <- vif(modelo_multicol)
print(vif_multicol)

# Comparar errores estándar
se_original <- summary(modelo_completo)$coefficients[2, 2]
se_multicol <- summary(modelo_multicol)$coefficients[2, 2]

cat("\nEfecto en errores estándar:\n")
cat("- Error estándar superficie (modelo original):", round(se_original, 0), "\n")
cat("- Error estándar superficie (con multicolinealidad):", round(se_multicol, 0), "\n")
cat("- Incremento:", round((se_multicol/se_original - 1) * 100, 1), "%\n")
```

### Soluciones para la multicolinealidad

1.  **Eliminación de variables**: Remover variables redundantes
2.  **Combinación de variables**: Crear índices o variables compuestas
3.  **Regresión ridge o lasso**: Técnicas de regularización (Tema 5)
4.  **Análisis de componentes principales**: Reducción de
    dimensionalidad

```{r}
# Ejemplo: eliminar variable redundante
modelo_sin_multicol <- lm(precio ~ superficie + habitaciones + antiguedad + 
                          distancia_centro + garaje, data = viviendas)

# Comparar VIF
cat("VIF después de eliminar variable redundante:\n")
print(vif(modelo_sin_multicol))
```

## Evaluación y comparación de modelos {#sec-evaluacion-modelos}

### Criterios de información

Los criterios de información nos ayudan a comparar modelos con
diferentes números de parámetros:

```{r}
# Comparar diferentes modelos
modelos <- list(
  "Completo" = modelo_completo,
  "Logarítmico" = modelo_log,
  "Polinomial" = modelo_poly,
  "Con interacción" = modelo_int_garaje
)

# Calcular criterios
criterios <- data.frame(
  Modelo = names(modelos),
  R2 = sapply(modelos, function(m) summary(m)$r.squared),
  R2_adj = sapply(modelos, function(m) summary(m)$adj.r.squared),
  AIC = sapply(modelos, AIC),
  BIC = sapply(modelos, BIC)
)

# Redondear solo las columnas numéricas
criterios[, 2:5] <- round(criterios[, 2:5], 4)
print(criterios)
```

### Validación cruzada

```{r}
# Validación cruzada simple (división 80-20)
set.seed(456)
train_indices <- sample(1:nrow(viviendas), 0.8 * nrow(viviendas))

# Datos de entrenamiento y prueba
train_data <- viviendas[train_indices, ]
test_data <- viviendas[-train_indices, ]

# Ajustar modelo en datos de entrenamiento
modelo_train <- lm(precio ~ superficie + habitaciones + antiguedad + 
                   distancia_centro + garaje, data = train_data)

# Predicciones en datos de prueba
pred_test <- predict(modelo_train, test_data)

# Error cuadrático medio de predicción
rmse_test <- sqrt(mean((test_data$precio - pred_test)^2))

cat("Error de predicción (RMSE):", round(rmse_test, 0), "euros\n")
cat("RMSE como % del precio promedio:", round(rmse_test / mean(test_data$precio) * 100, 1), "%\n")
```

## Análisis de casos especiales {#sec-casos-especiales}

### Variables categóricas con múltiples niveles

```{r}
# Crear variable categórica con múltiples niveles
viviendas$zona <- factor(sample(c("Centro", "Norte", "Sur", "Este", "Oeste"), 
                                nrow(viviendas), replace = TRUE))

# Modelo con variable categórica
modelo_zona <- lm(precio ~ superficie + habitaciones + antiguedad + 
                  distancia_centro + garaje + zona, data = viviendas)

# Resumen del modelo
summary(modelo_zona)$coefficients
```

### Análisis de outliers en regresión múltiple

```{r}
#| fig-cap: "Diagnóstico de observaciones influyentes en regresión múltiple"
# Identificar observaciones influyentes
influence_measures <- influence.measures(modelo_completo)

# Gráficos de diagnóstico específicos para regresión múltiple
par(mfrow = c(2, 2))
plot(modelo_completo, which = 1:4)
par(mfrow = c(1, 1))
```

## Ejemplo completo: Análisis paso a paso {#sec-ejemplo-completo}

Vamos a realizar un análisis completo desde el inicio, aplicando todo lo
aprendido:

```{r}
# === PASO 1: EXPLORACIÓN INICIAL ===
cat("=== ANÁLISIS EXPLORATORIO ===\n")
cat("Dimensiones del dataset:", dim(viviendas)[1], "observaciones,", dim(viviendas)[2], "variables\n\n")

# Estadísticas descriptivas
print(summary(viviendas))
```

```{r}
# === PASO 2: MODELO INICIAL ===
modelo_inicial <- lm(precio ~ superficie + habitaciones + antiguedad + 
                     distancia_centro + garaje, data = viviendas)

cat("\n=== MODELO INICIAL ===\n")
print(summary(modelo_inicial))
```

```{r}
# === PASO 3: DIAGNÓSTICO ===
cat("\n=== DIAGNÓSTICO DEL MODELO ===\n")

# VIF
cat("Factor de Inflación de Varianza (VIF):\n")
print(round(vif(modelo_inicial), 2))

# Pruebas estadísticas silenciosas
bp_final <- bptest(modelo_inicial)
sw_final <- shapiro.test(residuals(modelo_inicial))
reset_final <- resettest(modelo_inicial, power = 2:3, type = "regressor")

cat("\nPruebas de diagnóstico:\n")
cat("- Breusch-Pagan (homocedasticidad): p =", round(bp_final$p.value, 4), 
    ifelse(bp_final$p.value > 0.05, " ✓", " ✗"), "\n")
cat("- Shapiro-Wilk (normalidad): p =", round(sw_final$p.value, 4), 
    ifelse(sw_final$p.value > 0.05, " ✓", " ✗"), "\n")
cat("- RESET (especificación): p =", round(reset_final$p.value, 4), 
    ifelse(reset_final$p.value > 0.05, " ✓", " ✗"), "\n")
```

```{r}
# === PASO 4: MODELO FINAL CON MEJORAS ===
# Basándonos en el diagnóstico, podríamos mejorar el modelo
modelo_final <- modelo_inicial  # En este caso, el modelo inicial es bueno

cat("\n=== RESUMEN FINAL ===\n")
cat("R² =", round(summary(modelo_final)$r.squared, 4), "\n")
cat("R² ajustado =", round(summary(modelo_final)$adj.r.squared, 4), "\n")
cat("Error estándar residual =", round(summary(modelo_final)$sigma, 0), "euros\n")
```

## Conclusiones y recomendaciones prácticas {#sec-conclusiones}

### Puntos clave de la regresión múltiple

1.  **Interpretación condicional**: Los coeficientes deben interpretarse
    manteniendo las demás variables constantes.

2.  **Multicolinealidad**: Es el problema más específico de regresión
    múltiple. Use VIF \> 5 como señal de alarma.

3.  **Transformaciones**: Son herramientas poderosas para capturar
    relaciones no lineales manteniendo la interpretabilidad.

4.  **Interacciones**: Considere interacciones cuando tenga razones
    teóricas o empíricas para esperarlas.

5.  **Diagnóstico específico**: Use gráficos de residuos parciales y
    pruebas específicas para el caso multivariante.

### Flujo de trabajo recomendado

1.  **Exploración**: Analice correlaciones y distribuciones
2.  **Modelo base**: Comience con efectos principales
3.  **Diagnóstico**: Verifique supuestos y multicolinealidad
4.  **Refinamiento**: Considere transformaciones e interacciones
5.  **Validación**: Use validación cruzada para evaluar poder predictivo
6.  **Interpretación**: Comunique resultados de manera clara y
    contextualizada

### Preparación para temas avanzados

Los conceptos de este capítulo son fundamentales para: - **Selección de
variables** (Tema 3): Decidir qué variables incluir - **GLM** (Tema 4):
Extensión a variables respuesta no normales - **Técnicas modernas**
(Tema 5): Regularización y métodos no paramétricos

::: {.callout-tip title="Buenas prácticas"}
-   Siempre verifique VIF antes de interpretar coeficientes
-   Use gráficos de residuos parciales para evaluar la contribución
    individual de cada variable
-   Considere la interpretabilidad vs. capacidad predictiva según sus
    objetivos
-   Documente todas las decisiones de modelado para reproducibilidad
-   Valide siempre en datos independientes cuando sea posible
:::