[
  {
    "objectID": "anova.html",
    "href": "anova.html",
    "title": "5  Análisis de varianza",
    "section": "",
    "text": "5.1 Modelo con un factor\nEl modelo de ANOVA de un factor se utiliza cuando se estudia el efecto de un solo factor (variable independiente) en una variable dependiente continua. Este modelo permite comparar las medias de varios grupos para determinar si existen diferencias significativas entre ellos.\nTenemos una variable aleatoria \\(Y\\) que toma valores reales y una variable cualitativa o factor \\(X\\) con \\(k\\) niveles \\(1,2,\\ldots,i,\\ldots,k\\). La variable \\(Y\\) toma valores \\(Y_{ij}, j=1,\\ldots,n_i\\) en el nivel \\(i\\) del factor \\(X\\), siendo \\(n_i\\) el número de observaciones en el nivel \\(i\\) del factor \\(X\\).\nTenemos los siguientes supuestos:\nEscribimos el modelo como sigue: \\[\n  Y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\n\\] Donde:\nLa suma de las diferencias al cuadrado de cada dato respecto a la media general se calcula como sigue: \\[\n    \\text{SST} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y}_{..})^2        \n\\] donde \\(\\bar{Y}_{..}\\) es la media general de todas las observaciones.\nTeniendo en cuenta que: \\[\nY_{ij} - \\bar{Y}_{..}= Y_i  + \\epsilon_{ij} - \\bar{Y}_{..}\n\\] Podemos descomponer la suma de cuadrados, como sigue:\n\\[\n    \\text{SST} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y}_{..})^2=  \\sum_{i=1}^{k} n_i (\\bar{Y}_i - \\bar{Y}_{..})^2+\\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y}_i)^2      \n\\]\nDonde: La varianza entre grupos se calcula como la suma de las diferencias al cuadrado de las medias de los grupos respecto a la media general, ponderada por el tamaño de los grupos: \\[\n\\text{SSB} = \\sum_{i=1}^{k} n_i (\\bar{Y}_i - \\bar{Y}_{..})^2\n\\] donde \\(\\bar{Y}_i\\) es la media del grupo \\(i\\).\nAdemás, la varianza dentro de los grupos, se calcula como la suma de las diferencias al cuadrado de cada dato respecto a la media de su grupo se obtiene como: \\[\n\\text{SSW} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y}_i)^2\n\\] Esto es, se descompone la variabilidad total de los datos en dos componentes, SSB que refleja la diferencia de cada grupo respecto a la media global y SSW que refleja la variabilidad intrínseca dentro de cada grupo: \\[\n\\text{SST} = \\text{SSB} + \\text{SSW}\n\\]\nCálculo del Estadístico F: \\[\nF = \\frac{\\text{Varianza Entre Grupos (MSB)}}{\\text{Varianza Dentro de los Grupos (MSW)}}\n\\] Donde:\nEsto es: \\[\nF=\\frac{SSB/df_B}{SSW/df_W}\n\\] donde:\nUna vez se dispone de toda esta información, es común representarla en forma de tabla, en la llamada Tabla ANOVA:\nEl estadístico de prueba \\(F \\sim F_{df_B,df_W}\\) bajo la hipótesis nula de igualdad de medias.\nEl \\(p-valor\\) se obtiene a partir de la distribución \\(F\\), considerando los grados de libertad de numerador y denominador. Esto es: \\[\np-valor=P(F_{df_b,df_W}&gt;F_{muestral})\n\\]\nComo en otros contrastes, si el \\(p-valor\\) p es menor que el nivel de significancia (\\(\\alpha\\)), se rechaza la hipótesis nula, concluyendo que al menos una de las medias de los grupos es diferente.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Análisis de varianza</span>"
    ]
  },
  {
    "objectID": "anova.html#modelo-con-un-factor",
    "href": "anova.html#modelo-con-un-factor",
    "title": "5  Análisis de varianza",
    "section": "",
    "text": "Hipótesis:\n\nHipótesis Nula (\\(H_0\\)): Todas las medias de los grupos son iguales (\\(\\mu_1 = \\mu_2 = \\cdots = \\mu_k\\)).\nHipótesis Alternativa (\\(H_a\\)): Al menos una de las medias de los grupos es diferente.\n\n\n\n\n\nNormalidad: Las distribuciones de las poblaciones de las que provienen las muestras son normales.\nHomogeneidad de varianzas: Las varianzas de las poblaciones son iguales.\nIndependencia: Las observaciones son independientes entre sí.\n\n\n\n\\(Y_{ij}\\) es la observación \\(j\\)-ésima del grupo \\(i\\)-ésimo.\n\\(\\mu\\) es la media general.\n\\(\\epsilon_{ij}\\) es el término de error aleatorio.\n\\(\\tau_i\\) es el efecto del grupo i-ésimo en la media de la variable respuesta \\(Y\\). Esto es, cuánto aumenta o disminuye la media de \\(Y\\) por pertenecer la observación a la categoría \\(i\\). De modo que podemos llamar \\[\nY_i=\\mu+\\tau_i\n\\] al efecto medio del grupo \\(i\\)-esimo.\n\n\n\n\n\n\n\n\nMSB (Mean Square Between): Media cuadrática entre grupos.\nMSW (Mean Square Within): Media cuadrática dentro de los grupos.\n\n\n\n\\(df_B=k-1\\) son los grados de libertad entre los grupos\n\\(df_W=N-k\\) son los grado sd elibertado dentro de los grupos, siendo \\(N\\) el número total de observaciones.\n\n\n\n\n\n\n\n\n\n\n\nFuente de variación\nSuma de cuadrados\nGrados de libertad\nCuadrado Medio\n\n\n\n\nDisferencias entre grupos\nSSB\nk-1\nMSB\n\n\nDiferencias dentro de los grupos, Residual o Error\nSSW\nN-k\nMSW\n\n\nTotal\nSST\nN-1\n\n\n\n\n\n\n\n\n\n\nPara el futuro\n\n\n\nLa proporción de variabilidad explicada por los grupos se calcula como: \\[\nR^2=1-SSW/SST\n\\] Este valor, será muy importante en la asignatura de Regresión del Grado en Ciencia e Ingeniería de Datos.\n\n\n\n\n\n\n\n\n\n\n\nEjemplo. ANOVA de un factor\n\n\n\n\n\nVamos a realizar un ejemplo completo de ANOVA de un factor, donde calcularemos todos los pasos del contraste, incluidos el valor de F y el p-valor.\nSupongamos que tenemos tres tratamientos (A, B y C) y sus correspondientes muestras de datos son:\n\nGrupo A: \\([5, 7, 6, 9, 6]\\)\nGrupo B: \\([8, 12, 9, 11, 10]\\)\nGrupo C: \\([14, 10, 13, 15, 12]\\)\n\nNuestro objetivo es determinar si existe una diferencia significativa entre las medias de estos tres grupos.\nEn primer lugar calculamos la media de cada grupo y la media general:\n\nMedia de Grupo A (\\(\\bar{Y}_A\\)): \\[\n\\bar{Y}_A = \\frac{5 + 7 + 6 + 9 + 6}{5} = \\frac{33}{5} = 6.6\n\\]\nMedia de Grupo B (\\(\\bar{Y}_B\\)): \\[\n\\bar{Y}_B = \\frac{8 + 12 + 9 + 11 + 10}{5} = \\frac{50}{5} = 10.0\n\\]\nMedia de Grupo C (\\(\\bar{Y}_C\\)): \\[\n\\bar{Y}_C = \\frac{14 + 10 + 13 + 15 + 12}{5} = \\frac{64}{5} = 12.8\n\\]\nMedia General (\\(\\bar{Y}\\)): \\[\n\\bar{Y} = \\frac{6.6 + 10.0 + 12.8}{3} = \\frac{29.4}{3} \\approx 9.8\n\\]\n\nCalculamos la Suma de Cuadrados entre Grupos (SSB)\n\\[\n\\text{SSB} = n_A (\\bar{Y}_A - \\bar{Y})^2 + n_B (\\bar{Y}_B - \\bar{Y})^2 + n_C (\\bar{Y}_C - \\bar{Y})^2\n\\] donde \\(n_A = n_B = n_C = 5\\) (número de observaciones en cada grupo). Fíjate que el número de observaciones en cada grupo podría ser diferente. En este ejemplo, son iguales.\n\\[\n\\text{SSB} = 5 (6.6 - 9.8)^2 + 5 (10.0 - 9.8)^2 + 5 (12.8 - 9.8)^2 =96.4\n\\]\nA continuación calculamos la Suma de Cuadrados Dentro de los Grupos (SSW)\n\\[\n\\text{SSW} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y}_i)^2\n\\]\nPara cada grupo, calculamos la suma de las diferencias al cuadrado entre cada dato y la media del grupo:\n\nGrupo A: \\[\n(5 - 6.6)^2 + (7 - 6.6)^2 + (6 - 6.6)^2 + (9 - 6.6)^2 + (6 - 6.6)^2 = 9.2\n\\]\nGrupo B: \\[\n(8 - 10.0)^2 + (12 - 10.0)^2 + (9 - 10.0)^2 + (11 - 10.0)^2 + (10 - 10.0)^2 = 10.0\n\\]\nGrupo C: \\[\n(14 - 12.8)^2 + (10 - 12.8)^2 + (13 - 12.8)^2 + (15 - 12.8)^2 + (12 - 12.8)^2 = 14.8\n\\]\n\nEntonces, la Suma de Cuadrados Dentro de los Grupos (SSW) es: \\[\n\\text{SSW} = 9.2 + 10.0 + 14.8 = 34.0\n\\]\nTenemos, por tanto que la Suma Total de Cuadrados (SST) es: \\[\n\\text{SST} = \\text{SSB} + \\text{SSW} = 96.4 + 34.0 = 130.4\n\\]\nPara realizar el contraste, necesitamos calcular los Grados de Libertad del estadístico:\n\nGrados de libertad entre los grupos (\\(\\text{df}_B = k-1=3-1=2\\)):\nGrados de libertad dentro de los grupos (\\(\\text{df}_W=N-k=15-3=12\\)):\n\nCalcularemos las varianzas como sigue:\n\nVarianza entre los grupos (MSB): \\[\n\\text{MSB} = \\frac{\\text{SSB}}{\\text{df}_B} = \\frac{96.4}{2} = 48.2\n\\]\nVarianza dentro de los grupos (MSW): \\[\n\\text{MSW} = \\frac{\\text{SSW}}{\\text{df}_W} = \\frac{34.0}{12} \\approx 2.8333\n\\]\n\nYa estamos en disposición de calcular el estadístico de contraste : \\[\nF = \\frac{\\text{MSB}}{\\text{MSW}} = \\frac{48.2}{2.8333} \\approx 17.01\n\\]\nEl p-valor se obtiene utilizando la distribución \\(F\\) con \\(\\text{df}_B = 2\\) y \\(\\text{df}_W = 12\\). Para este ejemplo, podemos utilizar software estadístico o tablas de distribución F.\nUsando R:\n\npf(17.01, df1 = 2, df2 = 12, lower.tail = FALSE)\n\n[1] 0.0003143459\n\n\nEl p-valor es muy pequeño, mucho menor que el grado de significancia \\(\\alpha=0.05\\), indicando una diferencia significativa entre los grupos.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Análisis de varianza</span>"
    ]
  },
  {
    "objectID": "anova.html#modelo-con-dos-factores-con-y-sin-interacción",
    "href": "anova.html#modelo-con-dos-factores-con-y-sin-interacción",
    "title": "5  Análisis de varianza",
    "section": "5.2 Modelo con dos factores con y sin interacción",
    "text": "5.2 Modelo con dos factores con y sin interacción\nEl modelo de ANOVA de dos factores se utiliza cuando se estudian dos factores simultáneamente para evaluar su efecto individual y conjunto en una variable dependiente. Podemos verlo como una generalización del caso de ANOVA con un único factor. Este modelo es más complejo y permite entender no solo los efectos principales de cada factor, sino también si existe una interacción entre ellos.\nSean A y B dos factores que se desean estudiar, con \\(m_A\\) y \\(m_B\\) niveles. Trabajaremos con las siguientes hipótesis nulas:\n\nHipótesis:\n\nHipótesis Nula para los efectos principales (\\(H_0\\)):\n\nNo hay efecto del primer factor.\nNo hay efecto del segundo factor.\n\nHipótesis Nula para la interacción (\\(H_0\\)): No hay interacción entre los dos factores.\n\n\nModelo sin Interacción:\n\\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ijk} \\]\nDonde:\n\n\\(Y_{ijk}\\) es la observación \\(k\\)-ésima del nivel \\(j\\)-ésimo del factor B y nivel \\(i\\)-ésimo del factor A. - \\(\\mu\\) es la media general.\n\\(\\alpha_i\\) es el efecto del nivel \\(i\\)-ésimo del factor A.\n\\(\\beta_j\\) es el efecto del nivel \\(j\\)-ésimo del factor B. - \\(\\epsilon_{ijk}\\) es el término de error aleatorio.\n\nEn este caso, la tabla ANOVA queda como sigue:\n\n\n\n\n\n\n\n\n\nFuente de variación\nSuma de cuadrados\nGrados de libertad\nCuadrado Medio\n\n\n\n\nDiferencias entre niveles del factor A\n\\(SSB_A\\)\n\\(m_A-1\\)\n\\(MSB_A\\)\n\n\nDiferencias entre niveles del factor B\n\\(SSB_B\\)\n\\(m_B-1\\)\n\\(MSB_B\\)\n\n\nError\n\\(SSW\\)\n\\(N-m_A-m_B+1\\)\n\\(MSW\\)\n\n\nTotal\n\\(SST\\)\n\\(N-1\\)\n\n\n\n\nPara estudiar la importancia de cada factor se calcula el estadístico \\(F\\) particular para cada uno de ellos como sigue: \\[\nF_A=\\frac{MSB_A}{MSW} \\sim F_{m_A-1,N-m_A-m_B+1}\n\\] y \\[\nF_B=\\frac{MSB_B}{MSW}\\sim F_{m_B-1,N-m_A-m_B+1}\n\\] A partir de estos estadísticos de prueba podemos contrastar las hipótesis nulas de no existencia de efectos asociados a los factores \\(A\\) y \\(B\\) respectivamente.\nModelo con Interacción: \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\epsilon_{ijk} \\]\nDonde: \\((\\alpha\\beta)_{ij}\\) representa el efecto de interacción entre el nivel \\(i\\)-ésimo del factor A y el nivel \\(j\\)-ésimo del factor B.\nEn este caso, la tabla ANOVA añade el factor de interacción:\n\n\n\n\n\n\n\n\n\nFuente de variación\nSuma de cuadrados\nGrados de libertad\nCuadrado Medio\n\n\n\n\nDiferencias entre niveles del factor A\n\\(SSB_A\\)\n\\(m_A-1\\)\n\\(MSB_A\\)\n\n\nDiferencias entre niveles del factor B\n\\(SSB_B\\)\n\\(m_B-1\\)\n\\(MSB_B\\)\n\n\nDiferencias debidas la interacción\n\\(SSB_{AB}\\)\n\\((m_A-1)*(m_B-1)\\)\n\\(MSB_{AB}\\)\n\n\nError\n\\(SSW\\)\n\\(N-m_A*m_B\\)\n\\(MSW\\)\n\n\nTotal\n\\(SST\\)\n\\(N-1\\)\n\n\n\n\nPara estudiar la importancia de la interacción se calcula el estadístico \\(F\\) correspondiente:\n\\[\nF_{AB}=\\frac{MSB_{AB}}{MSW}\\sim F_{(m_A-1)*(m_B-1),N-m_A*m_B}\n\\]\nA partir de este estadísticos de prueba podemos contrastar la hipótesis nula de no existencia de interacción entre los dos factores \\(A\\), \\(B\\). Si podemos rechazar esa hipótesis, es decir, si existe interacción entre los factores, entonces hemos terminado. Es decir, no podemos eliminar ningún factor del modelo. En cambio, si no rechazamos la hipótesis nula, es decir, si no existe interacción entre los factores, podemos eliminar dicho efecto (la interacción) del modelo y pasar a un modelos sin interacción como el anteriormente descrito.\n\n\n\n\n\n\nEjemplo. ANOVA de dos factores con interacción\n\n\n\n\n\nSupongamos que estamos estudiando el efecto de dos factores sobre el rendimiento de los estudiantes: el método de enseñanza (con dos niveles: Tradicional y Experimental) y el tipo de material de estudio (con tres niveles: Libro, Video, y Online). Queremos saber si estos factores, y su posible interacción, tienen un efecto significativo en el rendimiento.\n\nrendimiento &lt;- c(85, 88, 90, 83, 87, 85, 86, 89, 91, 84, 88, 87,\n                 78, 79, 80, 76, 77, 75, 78, 79, 81, 76, 77, 76,\n                 90, 92, 91, 93, 90, 89, 91, 92, 91, 93, 92, 91)\nmetodo &lt;- factor(rep(c(\"Tradicional\", \"Experimental\"), each=18))\nmaterial &lt;- factor(rep(c(\"Libro\", \"Video\", \"Online\"), each=6, times=2))\n\n# Crear un data frame\ndatos &lt;- data.frame(rendimiento, metodo, material)\n\n# Visualizar los datos\nlibrary(\"ggpubr\")\nggline(datos, x = \"material\", y = \"rendimiento\", color = \"metodo\",\n       add = c(\"mean_se\", \"dotplot\"),\n       palette = c(\"#00AFBB\", \"#E7B800\"))\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\nAhora que tenemos nuestros datos, vamos a realizar el ANOVA de dos factores con interacción.\n\n# Realizar el ANOVA de dos factores con interacción\nanova_result &lt;- aov(rendimiento ~ metodo * material, data=datos)\n\n# Mostrar los resultados del ANOVA\nsummary(anova_result)\n\n                Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nmetodo           1   81.0    81.0   21.83 5.88e-05 ***\nmaterial         2  309.7   154.9   41.73 2.16e-09 ***\nmetodo:material  2  771.2   385.6  103.90 3.26e-14 ***\nResiduals       30  111.3     3.7                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLos resultados del ANOVA se interpretan observando los \\(p-valores\\) para cada uno de los componentes del modelo:\n\nmetodo: Efecto principal del método de enseñanza.\nmaterial: Efecto principal del tipo de material de estudio.\nmetodo:material: Interacción entre el método de enseñanza y el tipo de material de estudio.\n\nEl \\(p-valor\\) asociado a la interacción es menor que \\(0.05\\), lo que indica que la interacción entre el método de enseñanza y el tipo de material de estudio es significativa y por lo tanto no debe ser eliminada del modelo.\nEfectivamente, viendo la figura anterior concluimos que el rendimiento depende de la interacción entre los dos factores. Así, por ejemplo, cuando el matería es proporcionado de modo “online” o en “vídeo” el rendimiento es más elevado en el método experimental que en el método tradicional. La diferencia entre los dos métodos es especialmente notable cuando el material es “online”. Sin embargo, cuando el material se ofrece en modo “libro” el método tradicional ofrece mejores resultados que el método experimental.\n\n\n\n\n\n\n\n\n\nEjemplo. ANOVA de dos factores no significativos\n\n\n\n\n\nSupongamos que estamos estudiando el efecto del tipo de fertilizante (con dos niveles: \\(A\\) y \\(B\\)) y el tipo de riego (con tres niveles: Goteo, Aspersión, Manual) sobre el crecimiento de las plantas.\nEn primer lugar observamos los datos:\n\n# Crear datos simulados\ncrecimiento &lt;- c(20, 21, 23, 22, 24, 25, 26, 27, 28, 29, 30, 31,\n                 22, 23, 25, 24, 26, 27, 28, 29, 30, 31, 32, 33,\n                 21, 23, 22, 24, 23, 25, 26, 28, 27, 29, 28, 30)\nfertilizante &lt;- factor(rep(c(\"A\", \"B\"), each=18))\nriego &lt;- factor(rep(c(\"Goteo\", \"Aspersión\", \"Manual\"), each=6, times=2))\n\n# Crear un data frame\ndatos &lt;- data.frame(crecimiento, fertilizante, riego)\n\n# Visualizar los datos\nlibrary(\"ggpubr\")\nggline(datos, x = \"riego\", y = \"crecimiento\", color = \"fertilizante\",\n       add = c(\"mean_se\", \"dotplot\"),\n       palette = c(\"#00AFBB\", \"#E7B800\"))\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\nRealizamos el ANOVA de dos factores con interacción.\n\n# Realizar el ANOVA de dos factores con interacción\nanova_result &lt;- aov(crecimiento ~ fertilizante * riego, data=datos)\n\n# Mostrar los resultados del ANOVA\nsummary(anova_result)\n\n                   Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfertilizante        1   36.0   36.00  12.000  0.00162 ** \nriego               2    3.5    1.75   0.583  0.56424    \nfertilizante:riego  2  283.5  141.75  47.250 5.36e-10 ***\nResiduals          30   90.0    3.00                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nEl \\(p-valor\\) asociado a la interaccióni es mayor que \\(0.05\\) lo, lo que indica que la interacción entre el tipo de fertilizante y el tipo de riego no es significativa y debe de ser eliminada del modelo.\nPor tanto, obtenemos la tabla ANOVA para los dos factores sin interacción:\n\n# Realizar el ANOVA de dos factores sin interacción\nanova_result &lt;- aov(crecimiento ~ fertilizante + riego, data=datos)\n\n# Mostrar los resultados del ANOVA\nsummary(anova_result)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)  \nfertilizante  1   36.0   36.00   3.084 0.0886 .\nriego         2    3.5    1.75   0.150 0.8614  \nResiduals    32  373.5   11.67                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nUna vez eliminado el efecto de la interacción del modelo, podemos observar como ninguno de los dos factores para estadísticamente significativo, puesto que los \\(p-valores\\) asociados son mayores que \\(0.05\\). Sin embargo, hemos de actuar con cautela. Veamos el modelo cuando se elimina el factor menos significativo riego:\n\n# Realizar el ANOVA de un factor\nanova_result &lt;- aov(crecimiento ~ fertilizante, data=datos)\n\n# Mostrar los resultados del ANOVA\nsummary(anova_result)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)  \nfertilizante  1     36   36.00   3.247 0.0804 .\nResiduals    34    377   11.09                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAl nivel de significancia estadística de \\(0.05\\) podríamos decir que el factor fertilizante no es estadísticamente significativo y que ninguno de los dos factores influye en el crecimiento de las planteas.\nAhora bien, si tuvieras que elegir un método de riego y un fertilizando, ¿cuál recomendarías al cliente? ¿por qué?\n\n\n\n\n\n\n\n\n\nEjemplo. ANOVA de dos factores sin interacción\n\n\n\n\n\nSupongamos que estamos estudiando del tiempo de estudio y del tipo de dieta en el rendimiento académico de un grupo de estudiantes. El tiempo de estudio tiene \\(3\\) niveles (Alto, Medio y Bajo). El tipo de dieta tiene \\(3\\) niveles (Normal, Vegana y Vegetariana).\nEn primer lugar observamos los datos:\n\n# Datos\ndatos &lt;- data.frame(\n  Tiempo_estudio = factor(rep(c(\"1. Alto\", \"2. Medio\", \"3. Bajo\"), each = 9)),\n  Tipo_dieta = factor(rep(c(\"Vegetariana\", \"Normal\", \"Vegana\"), times = 9)),\n  Rendimiento = c(81, 71, 90, 75, 84, 81, 91, 100, 98, # Datos para Nivel1 de Factor1\n                60, 83, 70, 54, 65, 73, 82, 92, 73, # Datos para Nivel2 de Factor1\n                47, 63, 44, 61, 55, 52, 73, 63, 53) # Datos para Nivel3 de Factor1\n)\n\n# Visualizar\nlibrary(\"ggpubr\")\nggline(datos, x = \"Tiempo_estudio\", y = \"Rendimiento\", color = \"Tipo_dieta\",\n       add = c(\"mean_se\", \"dotplot\"),\n       palette = c(\"#00AFBB\", \"#E7B800\",\"#c94545\"))\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\nRealizamos el ANOVA de dos factores con interacción.\n\n# Realizar el ANOVA de dos factores con interacción\nanova_result &lt;- aov(Rendimiento ~ Tiempo_estudio * Tipo_dieta, data = datos)\n\n# Mostrar los resultados del ANOVA\nsummary(anova_result)\n\n                          Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nTiempo_estudio             2   3765  1882.3  17.410 6.2e-05 ***\nTipo_dieta                 2    169    84.6   0.782   0.472    \nTiempo_estudio:Tipo_dieta  4    465   116.1   1.074   0.398    \nResiduals                 18   1946   108.1                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLa interacción entre los dos factores no es estadísticamente significativa a nivel \\(\\alpha=0.05\\) y por lo tanto eliminamos dicha fuente de variabilidad del modelo.\n\n# Realizar el ANOVA de dos factores sin interacción\nanova_result &lt;- aov(Rendimiento ~ Tiempo_estudio + Tipo_dieta, data = datos)\n\n# Mostrar los resultados del ANOVA\nsummary(anova_result)\n\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTiempo_estudio  2   3765  1882.3  17.178 3.21e-05 ***\nTipo_dieta      2    169    84.6   0.772    0.474    \nResiduals      22   2411   109.6                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPodemos observar que el tipo de dieta no es un factor significativo en el rendimiento académico. Por tanto, eliminamos dicho factor del modelo.\n\n# Realizar el ANOVA de un factor\nanova_result &lt;- aov(Rendimiento ~ Tiempo_estudio , data = datos)\n\n# Mostrar los resultados del ANOVA\nsummary(anova_result)\n\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTiempo_estudio  2   3765  1882.3   17.51 2.04e-05 ***\nResiduals      24   2580   107.5                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nEn cambio, el tiempo de estudio sí es un factor determinante en el rendimiento académico. Su \\(p-valor\\) asociado es claramente inferior al nivel de significancia \\(0.05\\).\nPodemos visualizar el resultado:\n\n# Visualizar\nlibrary(\"ggpubr\")\nggline(datos, x = \"Tiempo_estudio\", y = \"Rendimiento\",\n       add = c(\"mean_se\", \"dotplot\"))\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Análisis de varianza</span>"
    ]
  },
  {
    "objectID": "anova.html#comparaciones-múltiples",
    "href": "anova.html#comparaciones-múltiples",
    "title": "5  Análisis de varianza",
    "section": "5.3 Comparaciones múltiples",
    "text": "5.3 Comparaciones múltiples\nEn el último ejemplo del apartado anterior hemos determinado que un factor con \\(3\\) niveles era estadísticamente significativo. Es decir, podemos rechazar la siguiente hipótesis nula:\n\\[H_O: \\mu_1=\\mu_2\\ldots = \\mu_k\\] (con \\(k\\) igual al número de niveles en el factor). Podemos plantearnos la pregunta siguiente: ¿En qué niveles del factor se encuentran las principales diferencias? Es decir, qué hipotesis (una o varias) de las siguientes son rechazadas: \\[H_O: \\mu_1=\\mu_2\\] \\[H_O: \\mu_1=\\mu_3\\] \\[\\ldots\\] \\[H_O: \\mu_{k-1} = \\mu_k\\]\nEn un ejemplo con \\(k=3\\) niveles en el factor, es posible plantear \\(3\\) contrastes de igualdad de medias, como los estudiados en capítulos anteriores. En un ejemplo con \\(k\\) niveles en el factor, es posible plantear \\(k*(k-1)/2\\) posibles contrastes de igualda de medias. Sin embargo, si realizamos todos estos contrastes, aumenta la probabilidad de cometer errores de tipo I (rechazar incorrectamente la hipótesis nula). Este fenómeno se conoce como el problema de las pruebas múltiples.\nComo hemos venido viendo, cuando realizamos una sola prueba de hipótesis (por ejemplo, una prueba \\(t\\) de Student o un ANOVA), generalmente establecemos un nivel de significancia predeterminado, como \\(\\alpha = 0.05\\). Esto significa que estamos dispuestos a aceptar una probabilidad de error de tipo I del \\(5\\%\\), es decir, hay un \\(5\\%\\) de probabilidad de rechazar incorrectamente la hipótesis nula cuando es verdadera.\nSin embargo, cuando realizamos múltiples pruebas de hipótesis, la probabilidad acumulada de cometer al menos un error de tipo I aumenta significativamente con cada prueba adicional. Por ejemplo, si realizamos \\(10\\) pruebas de hipótesis independientes, cada una con un nivel de significancia de \\(\\alpha = 0.05\\), la probabilidad de cometer al menos un error de tipo I aumenta a más del \\(40\\%\\) (\\(1-(1-0.05)^{10}\\approx 0.401\\).\nExiste una solución, las comparaciones múltiples necesarias para controlar este aumento en el riesgo de error. Existen varios métodos para controlar el problema de las pruebas múltiples, como los ajustes de Bonferroni, Holm-Bonferroni, Holm, Hochberg, Benjamini-Hochberg (FDR), entre otros. Estos métodos controlan la tasa global de error de tipo I para todas las comparaciones realizadas, manteniendo un nivel de significancia general específico.\n\n5.3.1 Método de Bonferroni\nEl método de Bonferroni es una técnica comúnmente utilizada para corregir el problema de las comparaciones múltiples y controlar el riesgo de error de tipo I. Este método es relativamente simple y conservador, lo que lo hace popular en muchas aplicaciones estadísticas.\nLa idea principal detrás del método de Bonferroni es ajustar el nivel de significancia individual para cada prueba de hipótesis realizada. En lugar de utilizar un nivel de significancia estándar (por ejemplo, \\(\\alpha = 0.05\\)), dividimos el nivel de significancia global deseado (generalmente \\(0.05\\)) por el número total de pruebas realizadas (\\(m\\)): \\[\n\\alpha' = \\frac{\\alpha}{m}\n\\] Esta división produce un nivel de significancia más estricto (\\(\\alpha'\\)) para cada prueba individual, lo que ayuda a controlar el riesgo global de error de tipo I.\nUtilizamos el nivel de significancia individual ajustado para cada prueba de hipótesis. Si el \\(p-valor\\) de una prueba es menor que el nivel de significancia ajustado, rechazamos la hipótesis nula de la prueba.\nEl método de Bonferroni es fácil de entender e implementar, y proporciona un control conservador sobre el error de tipo I en comparaciones múltiples. No obstante puede ser un método demasiado conservador en situaciones donde se realizan muchas comparaciones, lo que puede resultar en una pérdida de potencia estadística. Además, no tiene en cuenta la correlación entre las pruebas realizadas.\n\n\n\n\n\n\nEjemplo. Comparaciones múltiples\n\n\n\n\n\nContinuamos con el ejemplo anterior anterior sobre la influencia del tiempo de estudio en el rendimiento académico. Hemos visto que existe una relación entre ambos factores. En otras palabras, hemos rechazado la siguiente hipótesis nula:\n\\[\nH_0: \\mu_{Alto}=\\mu_{Medio}=\\mu_{Bajo}\n\\] Pero, ¿dónde se encuentran las diferencias relevantes?. Calculamos las tres medias muestrales\n\nmean(datos[datos$Tiempo_estudio==\"1. Alto\",]$Rendimiento)\n\n[1] 85.66667\n\nmean(datos[datos$Tiempo_estudio==\"2. Medio\",]$Rendimiento)\n\n[1] 72.44444\n\nmean(datos[datos$Tiempo_estudio==\"3. Bajo\",]$Rendimiento)\n\n[1] 56.77778\n\n\nAplicamos el método de Bonferroni, y obtenemos:\n\npairwise.t.test(datos$Rendimiento, g=datos$Tiempo_estudio,p.adjust.method = \"bonferroni\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  datos$Rendimiento and datos$Tiempo_estudio \n\n         1. Alto 2. Medio\n2. Medio 0.037   -       \n3. Bajo  1.3e-05 0.011   \n\nP value adjustment method: bonferroni \n\n\nDado que todos los \\(p-valores\\) ajustados son menores que \\(0.05\\), podemos rechazar las \\(3\\) hipótesis nulas. Es decir, rechazamos que el rendimiento sea el mismo para los diferentes niveles de tiempos de estudio.\nY en R podemos pintarlo como sigue (aquí empleamos otro método de corrección diferente al de Bonferroni, revísalo)\n\ncomparaciones_mult &lt;- TukeyHSD(anova_result)\nplot(comparaciones_mult)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Análisis de varianza</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introducción",
    "section": "",
    "text": "1.1 Ciencia de datos\nLa última revolución asociada a la IA ha estado enmarcada por el crecimiento en el uso del Aprenzaje Automático dentro del contexto de la ciencia de datos (Kelleher, Mac Namee, y D’arcy 2020).\nPero, ¿qué áreas, métodos y técnicas están implicados en la ciencia de datos?. En primer lugar, presentemos los aspectos teórico y prácticos que sustentan un proyecto real de ciencia de datos. Para ellos recurrimos a la Figura Figura 1.1 (a) que representa el clásico diagrama de la ciencia de datos, como una disciplina en la intersección de tres aspectos fundamentales. Para saber más sobre estos aspectos, desplegad los paneles siguientes:\nLa Figura Figura 1.1 (b) basada en CRISP-DM: “Cross Industry Standard Process for Data Mining” (Wirth y Hipp 2000) presenta el ciclo de vida que todo proyecto de ciencia de datos debería seguir. El inicio del proyecto viene dado por la definición de los objetivos de la organización. A continuación, se recogen y gestionan los datos. Como siguiente paso, se desarrollan y evalúan algoritmos matemáticos sobre los datos. Los resultados de estos modelos se presentan a los expertos en el dominio de aplicación para su posterior integración dentro de la organización. Nótese que el proyecto puede tener varias iteraciones, volviendo a alguna de las etapas anteriores siempre que una etapa posterior así lo requiera. Para saber más detalles sobre estas etapas investigad los paneles siguientes:\nEn este libro vamos a tratar en profundidad la Inferencia Estadística. Pero, ¿qué relación tiene con la Estadística. Vamos a verlo.\nLa estadística es la ciencia que se encarga de recolectar, organizar, analizar e interpretar datos para tomar decisiones informadas. Su objetivo principal es comprender y describir la variabilidad inherente en los datos y utilizar esta comprensión para hacer predicciones y tomar decisiones bajo condiciones de incertidumbre. La estadística se divide en dos ramas principales:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#ciencia-de-datos",
    "href": "intro.html#ciencia-de-datos",
    "title": "1  Introducción",
    "section": "",
    "text": "Ciencia de datos\n\n\n\nLa ciencia de datos es un área interdisciplinar que abarca un conjunto de principios, problemas, definiciones, algoritmos y procesos cuyo objetivo es extraer conocimiento no obvio y útil a partir de un conjunto de datos.\n\n\n\n\n\n\n\n\n\nMatemáticas y Estadística\n\n\n\n\n\nConocimientos de Matemáticas, y más concretamente de Estadística, son necesarios para analizar correctamente los datos disponibles. Conceptos como intervalo de confianza, histograma de frecuencias, contraste de hipótesis, espacio de características, métrica, hiperplano separador, error de clasificación, p_valor, etc. han de formar parte del conocimiento de todo científico de datos. Un equipo de ciencia de datos ha de contar con uno o varios expertos en Matemáticas y Estadística. Un buen libro de referencia para dominar los conceptos fundamentales en el ámbito matemático y estadístico que son necesarios en ciencia de datos es (Hastie et al. 2009). Además disponéis de esta versión online (James et al. 2013), similar pero más enfocada al análisis de datos. ¿Lo conocías ya?\n\n\n\n\n\n\n\n\n\nCiencias de la Computación\n\n\n\n\n\nEstudio del diseño y la arquitectura de los ordenadores y su aplicación en el campo de la ciencia y la tecnología, incluyendo el hardware, el software y las redes de comunicación. Un experto en ciencias de la computación ha de dominar lenguajes de programación como Python, JavaScript, C++, así como los elementos fundamentales que hacen que estos lenguajes funcionen. Algunas referencias útiles para estudiar estos lenguajes son (Hao y Ho 2019), (Osmani 2012) y (Oualline 2003). De igual modo, el científico de datos, ha de conocer ámbitos como los diferentes sistemas operativos, redes, seguridad, algoritmos y arquitectura de ordenadores. Un equipo de ciencia de datos ha de contar con uno o varios expertos en Ciencias de la Computación.\n\n\n\n\n\n\n\n\n\nConocimiento del Dominio\n\n\n\n\n\nRepresenta el problema que deseamos estudiar, la organización que lo proporciona y su dominio de aplicación. Existen casos de éxito de la ciencia de datos en prácticamente todos los dominios de interés que podamos mencionar: medicina, ciudades inteligentes, energía, telecomunicaciones, finanzas, seguros, ganadería, agricultura, ciencias sociales, ciberseguridad, etc. Un equipo de ciencia de datos ha de contar con uno o varios expertos en el dominio de aplicación. Estos expertos han de implicarse, fuertemente, en el problema que se quiere resolver. En (Kelleher, Mac Namee, y D’arcy 2020) podéis encontrar ejemplos muy interesantes de cómo la ciencia de datos es aplicada en diferentes dominios.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Fundamentos\n\n\n\n\n\n\n\n\n\n\n\n(b) Aplicaciones\n\n\n\n\n\n\n\nFigura 1.1: Ciencia de datos\n\n\n\n\n\n\n\n\n\n\nDefinir objetivos\n\n\n\n\n\nEntender el negocio es el primer paso en el proceso. Con ayuda de expertos en el dominio, se definen las preguntas a responder con el proyecto (definición de objetivos de la entidad responsable del proyecto). Una vez comprendido el negocio se designa una solución analítica para abordar el problema. En esta etapa las reuniones entre los matemáticos, informáticos y los expertos del dominio (habitualmente trabajadores con grandes conocimiento del problema que se trata de abordar) son frecuentes, necesarias y (casi) nunca, suficientes.\n\n\n\n\n\n\n\n\n\nObtener, preparar y gestionar los datos\n\n\n\n\n\nMediante técnicas informáticas, se recopilan y se preparan los datos para su posterior análisis. Podremos hablar de Big Data si los datos se caracterizan por su volumen, variedad o velocidad de procesamiento. Todo proceso de Ciencia de Datos es un proceso de aprendizaje en torno al dato. Las preguntas no surgen de los datos, pero se necesitan datos para responderlas. Es esta la etapa en la que trataremos una parte fundamental de todo el proceso: el análisis exploratorio de datos. Podrás estudiar más sobre cómo preparar los datos para etapas posteriores en el tema 3 de este libro.\n\n\n\n\n\n\n\n\n\nConstruir un modelo\n\n\n\n\n\nA través de métodos matemáticos y estadísticos se estudian y analizan los datos, se construyen algoritmos y se aplican modelos. Es la etapa asociada a los modelos de ML que trataremos en los temas 5,7 y 8.\n\n\n\n\n\n\n\n\n\nEvaluar y criticar el modelo\n\n\n\n\n\nSe definen medidas de rendimiento de los modelos que permitan su evaluación tanto por parte del desarrollador como por parte del cliente. Trataremos estas medidas en el tema 6 de nuestro curso.\n\n\n\n\n\n\n\n\n\nVisualización y presentación\n\n\n\n\n\nSe presentan los resultados buscando la comprensión por parte del cliente. No se trata únicamente de aplicar modelos complejos que nadie, más allá del desarrollador o experto matemático, pueda comprender. Muy al contrario, existe en la actualidad una corriente de investigación orientada a construir métodos capaces de ser explicables, junto con otras técnicas para convertir en entendibles los resultados obtenidos por los más complejos algoritmos matemáticos. Hablaremos de explicabilidad de modelos en el último tema del curso.\n\n\n\n\n\n\n\n\n\nDespliegue en producción\n\n\n\n\n\nLa solución final se convierte en un producto que podrá ser comercializado. Los modelos de ML se construyen para un propósito dentro de una organización. La integración de este modelo dentro del proceso de la organización debería de ser el último paso del proyecto de ciencia de datos.\n\n\n\n\n\n\n\n\n\n\n\nEstadística Descriptiva\n\n\n\n\n\nSe ocupa de resumir y describir las características de un conjunto de datos mediante herramientas gráficas y numéricas, como tablas, gráficos, medias, medianas, varianzas, etc. Su objetivo es proporcionar una visión clara y comprensible de la estructura y características de los datos.\n\n\n\n\n\n\n\n\n\nEstadística Inferencial\n\n\n\n\n\nUtiliza muestras de datos para hacer generalizaciones o inferencias sobre una población más amplia. Involucra el uso de métodos como la estimación de parámetros, pruebas de hipótesis y la construcción de intervalos de confianza. La inferencia estadística permite tomar decisiones y hacer predicciones basadas en datos muestreados.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-inferencia",
    "href": "intro.html#sec-inferencia",
    "title": "1  Introducción",
    "section": "1.2 Inferencia Estadística",
    "text": "1.2 Inferencia Estadística\nLa inferencia estadística se refiere a los métodos y procesos utilizados para extraer conclusiones acerca de una población a partir de una muestra de datos. A diferencia de la mera descripción de datos, la inferencia permite ir más allá de lo observado y hacer generalizaciones, estimaciones y decisiones en presencia de incertidumbre. Esto es fundamental para cualquier análisis de datos que aspire a ser predictivo o que busque comprender fenómenos más amplios que los capturados por los datos disponibles.\nAlgunos ejemplos del uso de la inteferencia estadística son:\n\nProporción de votantes a un determinado partido\nProporción de elementos defectuosos en una partida de productos\nProporción de paquetes que llegan tarde\nSalario medio\nAltura media\nConcentración media de un componente\nDuración media de un viaje en tren\nEspera media entre dos trenes consecutivos\n\nEn el contexto de la Ciencia de Datos, la inferencia estadística permite abordar preguntas críticas como:\n\n¿Cuál es la efectividad de un nuevo medicamento?\n¿Qué factores influyen en la satisfacción del cliente?\n¿Cómo se puede predecir el comportamiento futuro de los mercados financieros?\n\nEstas preguntas no solo requieren una recopilación cuidadosa de datos, sino también un análisis riguroso que tenga en cuenta la variabilidad inherente y las posibles fuentes de error.\nLa relevancia de la inferencia estadística en la Ciencia de Datos se manifiesta en varias áreas clave. Despliega el panel para averiguarlas.\n\n\n\n\n\n\nÁreas clave\n\n\n\n\n\nEstas son algunas de las áreas clave donde la inferencia estadística cobra valor.\n\nModelado predictivo: La inferencia estadística es fundamental para construir modelos que pueden predecir resultados futuros basándose en datos históricos. Estos modelos se utilizan en una variedad de campos, desde el marketing hasta la medicina y las finanzas.\nAnálisis experimental: En muchos dominios, como la biomedicina y la psicología, los experimentos controlados son esenciales para determinar causalidad y no solo correlación. La inferencia estadística proporciona el marco para diseñar estos experimentos y analizar los resultados de manera adecuada.\nDecisiones basadas en datos: En el ámbito empresarial y gubernamental, las decisiones basadas en datos permiten optimizar procesos, asignar recursos de manera eficiente y mejorar los servicios. La inferencia estadística permite que estas decisiones sean informadas y respaldadas por evidencia cuantitativa.\nManejo de la incertidumbre: En cualquier análisis de datos, es crucial manejar y comunicar la incertidumbre. La inferencia estadística ofrece métodos para cuantificar esta incertidumbre y tomar decisiones informadas pese a la presencia de variabilidad y error.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#datos-y-variables",
    "href": "intro.html#datos-y-variables",
    "title": "1  Introducción",
    "section": "1.3 Datos y variables",
    "text": "1.3 Datos y variables\nLos datos son las observaciones o medidas que recopilamos del mundo que nos rodea. Estos pueden ser números, categorías o cualquier tipo de información cuantificable. Llamaremos elementos a los individuos, los sujetos, las observaciones sobre las que se recojen un conjunto de variables.\n\n1.3.1 Datos en R\nR incluye en sus librerías numerosos conjuntos de datos. Para acceder a ellos, basta con cargar la librería correspondiente.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\nEs posible leer datos desde una cuenta de git. Y podemos tener una primera visión de los datos con sentencias comohead que nos enseña las primeras observaciones y sus variables.\n\nlibrary (readr)\n\nurlfile=\"https://raw.githubusercontent.com/IsaacMartindeDiego/IA/master/datasets/california_housing.csv\"\n\nmydata&lt;-read_csv(url(urlfile))\n\nhead(mydata)\n\n# A tibble: 6 × 10\n  longitude latitude housing_median_age total_rooms total_bedrooms population\n      &lt;dbl&gt;    &lt;dbl&gt;              &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n1     -122.     37.9                 41         880            129        322\n2     -122.     37.9                 21        7099           1106       2401\n3     -122.     37.8                 52        1467            190        496\n4     -122.     37.8                 52        1274            235        558\n5     -122.     37.8                 52        1627            280        565\n6     -122.     37.8                 52         919            213        413\n# ℹ 4 more variables: households &lt;dbl&gt;, median_income &lt;dbl&gt;,\n#   median_house_value &lt;dbl&gt;, ocean_proximity &lt;chr&gt;\n\ndim(mydata)\n\n[1] 20640    10\n\nsummary(mydata)\n\n   longitude         latitude     housing_median_age  total_rooms   \n Min.   :-124.3   Min.   :32.54   Min.   : 1.00      Min.   :    2  \n 1st Qu.:-121.8   1st Qu.:33.93   1st Qu.:18.00      1st Qu.: 1448  \n Median :-118.5   Median :34.26   Median :29.00      Median : 2127  \n Mean   :-119.6   Mean   :35.63   Mean   :28.64      Mean   : 2636  \n 3rd Qu.:-118.0   3rd Qu.:37.71   3rd Qu.:37.00      3rd Qu.: 3148  \n Max.   :-114.3   Max.   :41.95   Max.   :52.00      Max.   :39320  \n                                                                    \n total_bedrooms     population      households     median_income    \n Min.   :   1.0   Min.   :    3   Min.   :   1.0   Min.   : 0.4999  \n 1st Qu.: 296.0   1st Qu.:  787   1st Qu.: 280.0   1st Qu.: 2.5634  \n Median : 435.0   Median : 1166   Median : 409.0   Median : 3.5348  \n Mean   : 537.9   Mean   : 1425   Mean   : 499.5   Mean   : 3.8707  \n 3rd Qu.: 647.0   3rd Qu.: 1725   3rd Qu.: 605.0   3rd Qu.: 4.7432  \n Max.   :6445.0   Max.   :35682   Max.   :6082.0   Max.   :15.0001  \n NA's   :207                                                        \n median_house_value ocean_proximity   \n Min.   : 14999     Length:20640      \n 1st Qu.:119600     Class :character  \n Median :179700     Mode  :character  \n Mean   :206856                       \n 3rd Qu.:264725                       \n Max.   :500001                       \n                                      \n\n\nLa estructura de datos más habitual para realizar análisis de datos es el data frame. ¿Has estudiado este concepto en cursos anteriores?. Los data frame son estructuras de datos de dos dimensiones (como una matriz) que pueden contener datos de diferentes tipos. Normalmente nos referimos a las filas de un data frame como observaciones o registros, mientras que las columnas reciben el nombre de campos, variables, o características.\n\nL3 &lt;- LETTERS[1:3]\nchar &lt;- sample(L3, 10, replace = TRUE)\ndatos &lt;- data.frame(x = 1, y = 1:10, char = char)\ndatos\n\n   x  y char\n1  1  1    A\n2  1  2    A\n3  1  3    C\n4  1  4    A\n5  1  5    B\n6  1  6    A\n7  1  7    B\n8  1  8    B\n9  1  9    A\n10 1 10    A\n\nis.data.frame(datos)\n\n[1] TRUE\n\n\nUn tibble, o tbl_df, es una actualización del concepto del data frame. Los tibbles son data.frames perezosos. Esto le obliga a enfrentarse a los problemas antes, lo que normalmente conduce a un código más limpio y expresivo. Los tibbles también tienen un método print() mejorado que facilita su uso con grandes conjuntos de datos que contienen objetos complejos.\n\nlibrary(tidyverse)\nas.tibble(iris)\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ℹ 140 more rows\n\n\n\n\n\n\n\n\nPráctica\n\n\n\nEs importante que realices algún ejercicio en R, leyendo datos de diferentes fuentes y familiarizándote con las diferentes estructuras de datos que R proporciona.\n\n\n\n\n1.3.2 Tipo de variables\nUna variable es una característica o atributo que se pueden observar en los elementos y que puede tomar diferentes valores. Llamaremos valores a los resultados que se pueden observar de la característica en el elemento. Las variables siguen una distribución de probabilidad. Las variables pueden ser de varios tipos:\n\nCualitativas: También conocidas como categóricas, estas variables describen atributos o cualidades y se dividen en nominales (sin orden específico, como colores) y ordinales (con un orden, como niveles de satisfacción).\nCuantitativas: Estas variables son numéricas y pueden ser discretas (valores contables, como el número de hijos) o continuas (valores dentro de un rango, como la altura o el peso).\nMarcas de tiempo o identificadores: Como por ejemplo la fecha y hora de una transacción o el código de un producto o el número de identidad.\n\nEn este tema vamos a trabajar con los datos de Bank Marketing del repositorio UCI. En primer lugar debemos comprender el problema. ¿Qué sabes del marketing bancario? En el caso que nos ocupa, los datos están relacionados con campañas de marketing directo (llamadas telefónicas) de una entidad bancaria portuguesa. El objetivo del problema en cuestión es predecir si el cliente suscribirá un depósito a plazo (variable objetivo). Abordaremos este problema en próximos cursos. En este curso nos conformamos con entender los datos y proponer ciertas hipótesis en base a ellos.\nLas variables que debemos estudiar son:\nVariables de entrada:\n# datos del cliente bancario:\n\nedad (variable numérica)\nempleo : tipo de empleo (variable categórica con las siguientes categorías: “admin.”, “desconocido”, “desempleado”, “directivo”, “empleada del hogar”, “empresario”, “estudiante”, “obrero”, “autónomo”, “jubilado”, “técnico”, “servicios”)\nestado civil : estado civil (variable categórica con categorías: “casado”, “divorciado”, “soltero”; nota: “divorciado” significa divorciado o viudo)\neducación (variable categórica con categorías: “desconocida”, “secundaria”, “primaria”, “terciaria”)\nimpago: ¿tiene un crédito impagado? (variable binaria con dos posibles valores: “sí”, “no”)\nsaldo: saldo medio anual, en euros (variable numérica)\nvivienda: ¿tiene préstamo para vivienda? (variable binaria: “sí”, “no”)\npréstamo: ¿tiene préstamo personal? (variable binaria: “sí”, “no”)\n# relacionado con el último contacto de la campaña actual:\ncontacto: tipo de comunicación del contacto (variable categórica: “desconocido”, “teléfono”, “móvil”)\ndía: día del mes del último contacto (variable numérica)\nmes: mes del año del último contacto (variable categórica: “ene”, “feb”, “mar”, …, “nov”, “dic”)\nduración: duración del último contacto, en segundos (variable numérica)\n\n# otros atributos\n\ncampaña: número de contactos realizados durante esta campaña y para este cliente (variable numérica, incluye el último contacto)\npdays: número de días transcurridos desde que el cliente fue contactado por última vez en una campaña anterior (variable numérica, -1 significa que el cliente no fue contactado previamente)\nprevious: número de contactos realizados antes de esta campaña y para este cliente (variable numérica)\npoutcome: resultado de la campaña de marketing anterior (variable categórica: “desconocido”, “otro”, “fracaso”, “éxito”)\n\n# Variable de salida (objetivo deseado):\n17 - y: ¿ha suscrito el cliente un depósito a plazo? (variable binaria: “sí”, “no”)\n\n\n\n\n\n\nPara recordar\n\n\n\nA veces (muchas veces) la descripción que encontramos en una primera etapa no coincide al completo con los datos que luego nos entrega el cliente.\nEn otras ocasiones no se dispone de la descripción de las variables. En ese caso, ¡hay que hacer lo imposible por conseguirla!\n\n\nLeemos los datos con R.\n\nlibrary(tidyverse)\nbank = read.csv('https://raw.githubusercontent.com/rafiag/DTI2020/main/data/bank.csv')\ndim(bank)\n\n[1] 11162    17\n\nbank=as.tibble(bank)\nbank\n\n# A tibble: 11,162 × 17\n     age job       marital education default balance housing loan  contact   day\n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;int&gt;\n 1    59 admin.    married secondary no         2343 yes     no    unknown     5\n 2    56 admin.    married secondary no           45 no      no    unknown     5\n 3    41 technici… married secondary no         1270 yes     no    unknown     5\n 4    55 services  married secondary no         2476 yes     no    unknown     5\n 5    54 admin.    married tertiary  no          184 no      no    unknown     5\n 6    42 manageme… single  tertiary  no            0 yes     yes   unknown     5\n 7    56 manageme… married tertiary  no          830 yes     yes   unknown     6\n 8    60 retired   divorc… secondary no          545 yes     no    unknown     6\n 9    37 technici… married secondary no            1 yes     no    unknown     6\n10    28 services  single  secondary no         5090 yes     no    unknown     6\n# ℹ 11,152 more rows\n# ℹ 7 more variables: month &lt;chr&gt;, duration &lt;int&gt;, campaign &lt;int&gt;, pdays &lt;int&gt;,\n#   previous &lt;int&gt;, poutcome &lt;chr&gt;, deposit &lt;chr&gt;\n\n\nDisponemos de más de 10000 observaciones y un total de 17 variables.\nPara averiguar qué tipo de variables manejamos, ejecutar:\n\nstr(bank)\n\ntibble [11,162 × 17] (S3: tbl_df/tbl/data.frame)\n $ age      : int [1:11162] 59 56 41 55 54 42 56 60 37 28 ...\n $ job      : chr [1:11162] \"admin.\" \"admin.\" \"technician\" \"services\" ...\n $ marital  : chr [1:11162] \"married\" \"married\" \"married\" \"married\" ...\n $ education: chr [1:11162] \"secondary\" \"secondary\" \"secondary\" \"secondary\" ...\n $ default  : chr [1:11162] \"no\" \"no\" \"no\" \"no\" ...\n $ balance  : int [1:11162] 2343 45 1270 2476 184 0 830 545 1 5090 ...\n $ housing  : chr [1:11162] \"yes\" \"no\" \"yes\" \"yes\" ...\n $ loan     : chr [1:11162] \"no\" \"no\" \"no\" \"no\" ...\n $ contact  : chr [1:11162] \"unknown\" \"unknown\" \"unknown\" \"unknown\" ...\n $ day      : int [1:11162] 5 5 5 5 5 5 6 6 6 6 ...\n $ month    : chr [1:11162] \"may\" \"may\" \"may\" \"may\" ...\n $ duration : int [1:11162] 1042 1467 1389 579 673 562 1201 1030 608 1297 ...\n $ campaign : int [1:11162] 1 1 1 1 2 2 1 1 1 3 ...\n $ pdays    : int [1:11162] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...\n $ previous : int [1:11162] 0 0 0 0 0 0 0 0 0 0 ...\n $ poutcome : chr [1:11162] \"unknown\" \"unknown\" \"unknown\" \"unknown\" ...\n $ deposit  : chr [1:11162] \"yes\" \"yes\" \"yes\" \"yes\" ...\n\n\n\n\n\n\n\n\nEjercicio\n\n\n\nIdentifica el tipo de cada variable y las unidades de medida.\n\n\n\n\n1.3.3 Escalas de medición\nUna escala de medición define cómo se cuantifican o categorizan las variables recogidas sobre un conjunto de datos, influyendo en el análisis estadístico aplicable.\n\nNominal: categorización sin orden inherente. Por ejemplo, el género, la nacionalidad o el tipo de sangre.\nOrdinal: categorización con un orden lógico. Por ejemplo, el nivel educativo, o una clasificación de hoteles.\nMétrica: medición de diferentes escalas:\n\nIntervalo: sin cero verdadero, por ejemplo la temperatura en Celsius.\nRazón: con cero verdadero, por ejemplo los ingresos o la distancia.\n\n\nEn estadística y análisis de datos, es muy común recurrir a conversión entre diferentes escalas de medición. Las más comunes son:\n\nFechas a categóricas: convertir fechas exactas en mes, día de la semana, etc.\nCuantitativas a cualitativas: crear clases o rangos a partir de datos numéricos. Por ejemplo convertir el nivel de ingresos en “bajo”, “medio” y “alto”.\nOrdinales como numéricas: pasar de un orden a unos valores numéricos puede ser peligroso. Esta transformación ha de ser empleada con precaución, especialmente cuando se trabaja con pocos datos (\\(&lt;100\\)). Ten en cuenta que combinar en índices puede ser más informativo.\nVariables calculadas: creación de nuevas variables a partir de las existentes es una práctica muy habitual en análisis de datos. Hay una gran cantidad de situaciones donde esta tarea será muy beneficiosa. Por ejemplo, se crea el Índice de Masa Corporal (IMC) a partir de peso y altura.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#población-y-muestra",
    "href": "intro.html#población-y-muestra",
    "title": "1  Introducción",
    "section": "1.4 Población y muestra",
    "text": "1.4 Población y muestra\nLa población es el conjunto completo de todos los elementos o individuos que se desean estudiar. Puede ser una colección de personas, objetos, eventos o cualquier unidad de observación que sea de interés en un estudio. Por ejemplo, la población podría ser todos los estudiantes de una universidad, todos los árboles en un bosque, o todos los productos fabricados en una planta.\nDado que es a menudo impráctico o imposible estudiar toda la población, se selecciona una muestra, que es un subconjunto de la población. El estudio de las observaciones de una población podría implicar destruir dichas observaciones (vida útil del componente). El coste del estudio de las características de las observaciones podría ser muy elevado (experimentos biológicos). A veces el tamaño de la población es tan elevado (very big data!) que es obligatorio utilizar métodos de muestreo. Eso sí, es importante que la muestra sea representativa para que las conclusiones que se extraen de ella sean válidas para la población completa.\n\n\n\n\n\n\nEjemplo. Sondeo Electoral de 1936\n\n\n\n\n\nEl caso del sondeo electoral de 1936, realizado por la revista “Literary Digest” en Estados Unidos, es un ejemplo clásico que ilustra la importancia de la representatividad de una muestra en la investigación estadística y en particular en la realización de encuestas.\nEn 1936, la revista “Literary Digest” llevó a cabo un sondeo para predecir el resultado de la elección presidencial entre el candidato demócrata Franklin D. Roosevelt y el candidato republicano Alf Landon. La revista enviaba millones de encuestas a sus lectores y a otras listas de individuos, como propietarios de automóviles y personas que aparecían en directorios telefónicos. En ese año, “Literary Digest” envió unos 10 millones de encuestas y recibió alrededor de 2.4 millones de respuestas, lo que constituía una muestra enorme para los estándares de la época.\nLa predicción del sondeo fue que Alf Landon ganaría la elección con un margen significativo. Sin embargo, el resultado real fue una victoria aplastante para Franklin D. Roosevelt, quien ganó con más del 60% del voto popular y el 98.5% del voto electoral. Este error de predicción se debió principalmente a problemas relacionados con la representatividad de la muestra.\nProblemas de Representatividad\n\nSesgo de selección: La muestra del sondeo no era representativa de la población general. La lista de encuestados estaba basada en suscriptores de la revista, propietarios de automóviles y personas que aparecían en directorios telefónicos. En la década de 1930, estos grupos eran, en su mayoría, personas de ingresos más altos y de tendencia republicana, que no representaban adecuadamente a la población estadounidense en general, que incluía una proporción significativa de votantes de ingresos más bajos y de tendencia demócrata.\nTasa de respuesta: Aunque el tamaño de la muestra era grande (2.4 millones de respuestas), la tasa de respuesta fue baja en relación al número de encuestas enviadas (10 millones). Esto puede introducir un sesgo adicional, ya que las personas que respondieron pueden no ser representativas de la población objetivo. Es posible que aquellos más inclinados a responder fueran también más inclinados a votar por Landon.\nMuestreo no aleatorio: El método de selección de la muestra no era aleatorio. Las listas utilizadas para enviar las encuestas no cubrían todas las demografías de manera equitativa. Un muestreo aleatorio hubiera asegurado que cada individuo de la población tuviera una probabilidad conocida y no nula de ser seleccionado, lo cual es crucial para la representatividad.\n\n\n\n\nLa probabilidad juega un papel crucial en el diseño del muestreo y en la inferencia estadística. En el contexto del muestreo, la probabilidad asegura que cada miembro de la población tiene una oportunidad conocida y, generalmente, igual de ser incluido en la muestra. Esto permite que las muestras sean representativas y que los resultados sean generalizables. La teoría de la probabilidad también se utiliza para modelar la incertidumbre y el comportamiento aleatorio inherente en los datos.\n\n\n\n\n\n\nAdvertencia\n\n\n\nProbablemente has estudiado estos conceptos en la asignatura de Probabilidad y Simulación.\n\n\nUna vez que se ha recolectado una muestra, la estadística descriptiva se utiliza para organizar, resumir y presentar los datos de manera comprensible. Las herramientas de estadística descriptiva incluyen:\n\nMedidas de tendencia central: como la media, mediana y moda, que resumen el centro de los datos.\nMedidas de dispersión: como el rango, la varianza y la desviación estándar, que describen la variabilidad de los datos.\nGráficos: como histogramas, gráficos de caja y gráficos de dispersión, que visualizan los datos.\n\nLa estadística descriptiva se centra en describir lo que los datos muestran, sin hacer inferencias o generalizaciones sobre la población.\nLa inferencia estadística utiliza los datos de la muestra para hacer estimaciones, predicciones y generalizaciones sobre la población completa. Esto incluye dos aspectos principales:\n\nEstimación: Utilizar los datos de la muestra para estimar parámetros de la población, como la media o la proporción. Las estimaciones pueden ser puntuales (un solo valor) o por intervalo (un rango de valores con un nivel de confianza asociado).\nContraste de hipótesis: Probar afirmaciones sobre la población utilizando los datos de la muestra. Esto implica formular una hipótesis nula y una hipótesis alternativa, y usar pruebas estadísticas para decidir cuál es más consistente con los datos observados.\n\nLa inferencia estadística se basa en la teoría de la probabilidad para evaluar la incertidumbre y la variabilidad en las estimaciones y pruebas.\nLa Figura 1.2 muestra la relación entre los conceptos de Población, Muestra, Inferencia Estadística, Probabilida y Estadística Descriptiva. Estos conceptos permiten a los estadísticos recolectar datos de manera eficiente, describir los datos recolectados y hacer inferencias significativas y precisas sobre la población a partir de la muestra.\n\n\n\n\n\n\nFigura 1.2: La esencia de la Estadística\n\n\n\nExiste un principio fundamental en el análisis de datos que podríamos simplificar así:\n\\[DATOS = MODELO + ERROR\\]\n\nLos datos representan la realidad (procesos de negocios, clientes, productos, actividades, fenómenos físicos, etc.) que se quiere comprender, predecir o mejorar.\nEl modelo es una representación simplificada de la realidad que proponemos para describirla e interpretarla más fácilmente.\nEl error refleja la diferencia entre nuestra representación simplificada de la realidad (el modelo) y los datos que relamente describen esa realidad de forma precisa.\n\nBuscaremos, especialmente cuando estudiemos Apendizaje Automático, encontrar el modelo que explique los datos minimizando al máximo el error.\n\n1.4.1 Muestreo\nEl muestreo estadístico es una técnica fundamental en la estadística que permite extraer conclusiones sobre una población basándose en el análisis de una parte más pequeña de dicha población, conocida como muestra. A continuación, presentamos algunas de las principales técnicas de muestreo estadístico. Cada una de estas técnicas tiene sus propias ventajas y limitaciones, y la elección de la técnica adecuada depende del objetivo del estudio, las características de la población y los recursos disponibles.\n\n\n\n\n\n\nMuestreo aleatorio simple\n\n\n\n\n\nEn el muestreo aleatorio simple, cada miembro de la población tiene la misma probabilidad de ser seleccionado. Diremos que la muestra es aletoria simple (m.a.s.) cuando empleamos este tipo de muestreo. Se suele realizar utilizando métodos aleatorios, como sorteo, tablas de números aleatorios o generadores de números aleatorios. Este método es simple y fácil de entender, pero puede no ser siempre el más eficiente, especialmente si la población es muy grande. Podemos realizar el muestro con o sin reemplazamiento. Diremos que un muestreo es con reemplazamiento cuando una observación poblacional puede ser elegida varias veces para formar parte de la muestra. La misma observación puede aparecer repetida. Habitualmente se recurre al muestro sin reemplazamiento, donde una observación de la población, una vez elegida para formar parte de la muestra, es eliminada de la población y no puede volver a ser elegida.\nLa siguiente sentencia de R elige \\(5\\) observaciones de la base de datos bank mediante un muestreo aleatorio simple.\n\nn=dim(bank)[1] # número de observaciones en la base de datos\nindices=sample(1:n,size=5,replace=FALSE)\nindices # indices de las observaciones elegidas\n\n[1]  9871  2846 10505  1818   200\n\nmuestra=bank[indices,] # muestra de observaciones \n\nEsta técnica será empleada en la asignatura de Aprendizaje Automático para crear las particiones de entrenamiento, test y validación.\n\n\n\n\n\n\n\n\n\nMuestreo sistemático\n\n\n\n\n\nEn el muestreo sistemático, se selecciona un punto de inicio al azar y luego se elige a cada n-ésimo individuo de la lista de la población. Por ejemplo, si se quiere una muestra del 10% de una población de 1000 individuos, se seleccionaría un punto de inicio al azar entre los primeros 10 individuos y luego se seleccionaría cada décimo individuo a partir de ese punto. Este método es más fácil de ejecutar que el muestreo aleatorio simple, pero puede introducir sesgos si hay un patrón en la lista de la población.\n\n\n\n\n\n\n\n\n\nMuestreo estratificado\n\n\n\n\n\nEl muestreo estratificado implica dividir la población en subgrupos o estratos homogéneos y luego tomar una muestra aleatoria de cada estrato. Los estratos se forman en base a una característica específica como la edad, el género, el nivel socioeconómico, etc. Este método asegura que todas las subpoblaciones importantes estén representadas en la muestra y puede proporcionar estimaciones más precisas que el muestreo aleatorio simple. En el Aprendizaje Automático la variable elegida será la variable respuesta, o variable objetivo. De este modo, cuando los datos están desequilibrados, es decir, cuando tenemos más observaciones de una clase que de otra en los valores de la variable respuesta, aseguramos que todas los grupos están igualmente representados en la muestra.\n\n\n\n\n\n\n\n\n\nMuestreo por conglomerados\n\n\n\n\n\nEn el muestreo por conglomerados, la población se divide en grupos o conglomerados, y se seleccionan algunos de estos conglomerados al azar. Luego, todos (o una muestra de) los individuos dentro de los conglomerados seleccionados se incluyen en la muestra. Este método es útil cuando es difícil o costoso crear una lista completa de la población, pero puede ser menos preciso si los conglomerados no son homogéneos.\n\n\n\n\n\n\n\n\n\nMuestreo por cuotas\n\n\n\n\n\nEl muestreo por cuotas es un tipo de muestreo no probabilístico en el que se selecciona una muestra que cumple con ciertas cuotas preestablecidas basadas en características específicas. Por ejemplo, se puede querer que la muestra tenga un cierto número de individuos de diferentes edades o géneros. Aunque este método es práctico y rápido, no permite estimaciones estadísticas precisas de la población porque no todos los individuos tienen la misma probabilidad de ser seleccionados.\n\n\n\n\n\n\n\n\n\nMuestreo de bola de nieve\n\n\n\n\n\nEl muestreo de bola de nieve es otra técnica no probabilística, utilizada principalmente cuando es difícil acceder a los miembros de la población. Se empieza con unos pocos individuos conocidos de la población, quienes a su vez refieren a otros individuos, y así sucesivamente. Este método es útil para estudios de poblaciones ocultas o difíciles de alcanzar, como personas sin hogar o usuarios de drogas, pero introduce un alto riesgo de sesgo. También es muy común utilizar este tipo de muestreo cuando se realizan encuestas en redes sociales puesto que, habitualmente, la población objetivo está oculta o es complejo acceder a ella.\n\n\n\n\n\n\n\n\n\nMuestreo intencional o dirigido\n\n\n\n\n\nEn el muestreo intencional o dirigido, se seleccionan individuos que cumplen con ciertos criterios específicos de la investigación. Es un método no probabilístico donde la selección se basa en el juicio del investigador. Es útil cuando se busca estudiar casos específicos, pero no permite hacer generalizaciones precisas sobre la población completa.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#parámetros-y-estadísticos",
    "href": "intro.html#parámetros-y-estadísticos",
    "title": "1  Introducción",
    "section": "1.5 Parámetros y estadísticos",
    "text": "1.5 Parámetros y estadísticos\nLos parámetros de una población son (casi) siempre desconocidos. Son valores teóricos que se definen sobre la población y que son de interés para el investigador. Son sobre los que haremos inferencia. Normalmente se representan con letras griegas.\nDiremos que un estadístico (además de una persona que hace estadística) es una función definida sobre los datos de una muestra (valores de una o más variables). En cada muestra serán distintos, debido a la variabilidad inherente a la extracción de una muestra representativa de una población. Los estadísticos siguen una distribución en el muestro y se representa con letras latinas.\n\n\n\n\n\n\nFigura 1.3: Parámetros vs. Estadísticos\n\n\n\nEs decir, una vez obtenida una muestra, es necesario extraer información útil de la misma. Esto se hace a través de los estadísticos.\nDiremos que un estadístico \\(T= T(x_1,\\ldots,x_n)\\) es una función real de la muestra aleatoria \\((x_1,\\ldots,x_n)\\).\nUn estadístico es una variable aleatoria, y por lo tanto, tiene asociada una distribución. La ditribución de probabilidad correspondiente a un estadístico se denomina distribución muestral.\nPor ejemplo, sea \\((X_1,\\ldots,X_n)\\) una muestra aletoria de una población \\(X\\) con esperanza \\(\\mu\\) y vaianza \\(\\sigma^2\\). Consideremos los siguientes ejemplos de estadísticos:\n\nMedia Muestral (\\(\\bar{X}\\)): Utilizada para estimar la media poblacional. \\[\n\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\n\\]\nVarianza Muestral (\\(V\\)): Utilizada para estimar la varianza poblacional. \\[\nV=\\frac{1}{n}\\sum_{i=1}^n(X_i-\\bar{X})^2\n\\]\nCuasivarianza muestral (\\(S^2\\)) \\[\nS^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X})^2=\\frac{1}{n-1}\\left [ \\sum_{i=1}^nX_i^2-n\\bar{X}^2\\right]\n\\] Entonces se tienen las siguientes propiedades:\n\n\\[\nE(\\bar{X})=\\mu\n\\] \\[\nVar(\\bar{X})=\\frac{\\sigma^2}{n}\n\\]\n\\[\nE(V)=\\frac{n-1}{n}\\sigma^2\n\\] \\[\nE(S^2)=\\sigma^2\n\\]\n\n1.5.1 Uso de la muestra\nSupongamos que queremos estudiar una característica de cierta población. Esta característica se representa mediante una variable aleatoria \\(X\\) y el estudio se centra en su valor medio \\(E[X]\\). Para ello, se decide tomar una muestra y se obtiene un estadístico, la media muestral, que se utiliza para estimar el valor de \\(E[X]\\): \\[\n\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\n\\] Fíjate que este estadístico \\(\\bar{X}\\) es una variable aleatoria. Una vez tomada una muestra particular \\((x_1,\\ldots,x_n)\\) de la variable aleatoria \\(X\\) se obtiene un valor numérico particular para la variable aleatoria \\(\\bar{X}\\): \\[\n\\bar{x}=\\frac{1}{n}\\sum_{i=1}^nx_i\n\\]\n\n\n\n\n\n\nPara recordar\n\n\n\n\\[\\bar{X}\\neq\\bar{x}\\]\n\n\n\n\n\n\n\n\nEjemplo. Diferencia entre parámetro y estadístico\n\n\n\n\n\nSe quiere estudiar la temperatura de una solución líquida en un laboratorio y estimar el valor medio \\(\\mu\\) (parámetro de la población), que es desconocido. Se supone que la temperatura se puede aproximar mediante una variable aleatoria de la que se desconoce su distribución. Una opción razonable para estimarla sería escoger una muestra de la solución, medir su temperatura, y estimar \\(\\mu\\) mediante el promedio de esa muestra (estadístico muestral): \\[ \\hat{\\mu}=\\bar{X}\\]. Es importante señalar que podríamos emplear otros estadísticos diferentes.\nSupongamos ahora que se toma una muestra de 10 mediciones de temperatura de dicha solución, obteniendo estos valores:\n\n\n [1] 45.76 42.75 45.54 49.61 53.00 44.15 51.48 48.69 51.33 49.21\n\n\nSiendo su valor promedio \\(\\bar{x}=\\) 48.15. Este valor estima el verdadero valor desconocido del parámetro \\(\\mu\\).\nAhora, supongamos que se toma otra muestra de 5 mediciones de temperatura de dicha solución, obteniendo estos valores:\n\n\n[1] 50.18 43.89 46.18 47.74 46.59\n\n\nLa media de esta otra muestra es \\(\\bar{x}=\\) 46.92 otra estimación del desconocido valor del parámetro \\(\\mu\\).\n¿Cuál de las dos estimaciones te parece más fiable? ¿Por qué? ¿Cómo podríamos “asegurar” que nuestro estimador es altamente fiable?\nRepetimos el primer experimento \\(100\\) días consecutivos. Es decir, tomamos \\(10\\) muestras de tamaño 10 y estimamos, para cada muestra el valor del parámetro \\(\\mu\\) mediante la media muestral. El siguiente gráfico muestra los valores de esas \\(100\\) muestras:\n\n\n\n\n\n\n\n\n\n¿Cuál es, a tu juicio, un buen valor del estimador del parámetro? Supongamos que tu profesor ha simulado estos datos. Es decir, tu profesor conoce realmente el verdadero valor del parámetro. En ese caso, te propone el siguiente reto: Dame dos valores (uno bajo y otro alto) entre los cuales crees que está el verdadero valor que solo yo (el profesor) conozco. ¿Qué valores darías? Te propone un reto mayor: ¿Qué valores darías si quisieras estar seguro al \\(100\\%\\) de acertar? Es decir, que la probabilidad de fallo sea \\(0\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#estadística-paramétrica-y-no-paramétrica",
    "href": "intro.html#estadística-paramétrica-y-no-paramétrica",
    "title": "1  Introducción",
    "section": "1.6 Estadística paramétrica y no paramétrica",
    "text": "1.6 Estadística paramétrica y no paramétrica\nTal y como hemos visto hasta ahora, el objetivo general de la inferencia es obtener información acerca de la distribución de una variable aleatoria \\(X\\) mediante la observación de una muestra \\((X_1,\\ldots,X_n)\\). En este curso vamos a tratar con dos tipos de herramientas para realizar inferencia estadística: la estadística paramétrica y la no paramétrica. Veamos sus semejanzas y diferencias:\nEstadística Paramétrica\nLa estadística paramétrica se basa en la suposición de que los datos siguen una distribución conocida, como la distribución normal, binomial, Poisson, etc. Estas suposiciones (o hipótesis) deben ser comprobadas para dar validez a este tipo de pruebas. Los parámetros de estas distribuciones, como la media y la varianza, se utilizan para resumir la información de los datos y realizar inferencias.\n\n\n\n\n\n\nFamilias paramétricas\n\n\n\n\n\nSe tiene una variable aleatoria \\(X\\) cuya distribución se supone perteneciente a una cierta familia paramétrica {\\(f_\\theta\\)} donde \\(\\theta \\in \\Theta\\).\nLa distribución de \\(X\\) es conocida excepto por el valor del parámetro \\(\\theta\\), del cual lo único que se conoce es su rango de posibles valores, \\(\\Theta\\), denominado espacio paramétrico.\nEjemplos de familias paramétricas\n\n\\(X \\sim N(\\mu,\\sigma^2) \\rightarrow \\theta=(\\mu,\\sigma^2)\\)\n\\(X \\sim Bernoulli(p) \\rightarrow \\theta=p\\)\n\\(X \\sim Exp(\\lambda) \\rightarrow \\theta=\\lambda\\)\n\n\n\n\nPor tanto, el objeto de los métodos paramétricos es obtener información sobre el parámetro de interés mediante la obtención de muestras de la variable aleatoria.\nLos métodos paramétricos son más potentes que los no paramétricos (es decir, tienen una mayor probabilidad de detectar un efecto verdadero) si las suposiciones son correctas. Ejemplos de pruebas paramétricas incluyen la prueba t de Student, el análisis de varianza (ANOVA), que estudiaremos en el Capítulo 5 y la regresión lineal que veréis en el segundo cuatrimestre.\nEstadística no paramétrica\nLa estadística no paramétrica no hace suposiciones fuertes sobre la distribución de los datos. Estos métodos son más flexibles y robustos a las violaciones de las suposiciones, pero pueden ser menos potentes si las suposiciones de los métodos paramétricos son verdaderas. Los métodos no paramétricos a menudo se basan en el orden de los datos, en lugar de sus valores exactos. Ejemplos de pruebas no paramétricas incluyen la prueba de Mann-Whitney U, la prueba de Kruskal-Wallis y la prueba de Chi-cuadrado. Tendremos un capítulo (Capítulo 4), dedicado a este tipo de herramientas.\nLa elección entre métodos paramétricos y no paramétricos depende de la naturaleza de tus datos y de las suposiciones que estés dispuesto a hacer. Si tus datos cumplen con las suposiciones de una prueba paramétrica, esa prueba puede ser la opción más potente. Si no, una prueba no paramétrica puede ser más apropiada.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#frecuentistas-vs-bayesianos",
    "href": "intro.html#frecuentistas-vs-bayesianos",
    "title": "1  Introducción",
    "section": "1.7 Frecuentistas vs Bayesianos",
    "text": "1.7 Frecuentistas vs Bayesianos\nEn el campo de la estadística existen dos enfoques diferentes que han de ser comentados, la inferencia clásica (o frecuentista) y la inferencia Bayesiana.\nEnfoque Frecuentista\nLos frecuentistas interpretan la probabilidad como la frecuencia relativa de un evento en un número infinito de repeticiones del experimento. Se obtienen datos a través de una muestra y con técnicas estadísticas se extrae información de los mismos mediante, los llamados, estimadores. En base a esas estimaciones se toman decisiones en el dominio de aplicación.\nLos métodos frecuentistas son ampliamente utilizados y son la base de muchas técnicas estadísticas clásicas. Los parámetros son considerados como valores fijos y desconocidos que se estiman a partir de los datos. Los intervalos de confianza, que veremos más adelante en el Capítulo 3, se interpretan en términos de repetibilidad: si se repite el experimento muchas veces, el intervalo de confianza capturará el verdadero parámetro en un porcentaje dado de las repeticiones. Esta interpretación es un poco extraña para el no iniciado y trataremos de explicarlo en detalle en capítulos posteriores.\nEnfoque Bayesiano\nLa inferencia Bayesiana tiene su fundamento en el teorema de Bayes. El teorema de Bayes, también conocido como regla de Bayes, es un principio fundamental en la teoría de la probabilidad que describe la forma de actualizar las probabilidades de una hipótesis basándose en nueva evidencia o información. Fue formulado por el matemático británico Thomas Bayes en el siglo XVIII. Es posible que hayas estudiado este teorema en asignaturas anteriores del grado. En cualquier caso, es sencillo y dice así:\n\\[\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\]\ndonde:\n\n\\(P(A|B)\\) es la probabilidad de que ocurra el evento \\(A\\) dado que ha ocurrido el evento \\(B\\). Esta es conocida como la probabilidad a posteriori.\n\\(P(B|A)\\) es la probabilidad de que ocurra el evento \\(B\\) dado que ha ocurrido el evento \\(A\\). Esta es conocida como la probabilidad verosímil o likelihood.\n\\(P(A)\\) es la probabilidad de que ocurra el evento \\(A\\) sin ninguna información adicional sobre \\(B\\). Esta es conocida como la probabilidad a priori o simplemente la probabilidad previa.\n\\(P(B)\\) es la probabilidad de que ocurra el evento \\(B\\) bajo todas las posibles hipótesis. Esta es conocida como la probabilidad marginal de \\(B\\).\n\nVemos que el teorema de Bayes permite actualizar la probabilidad de una hipótesis \\(A\\) a la luz de nueva evidencia \\(B\\). Básicamente, proporciona una forma de ajustar nuestras creencias iniciales (probabilidad a priori) en base a la nueva información disponible (evidencia).\n\n\n\n\n\n\nEjemplo. Teorema de Bayes.\n\n\n\n\n\nImaginemos que estamos tratando de diagnosticar una enfermedad rara que afecta al \\(1\\%\\) de la población (es decir, \\(P(\\text{Enfermedad}) = 0.01\\)). Ademas, se sabe que existe una prueba para esta enfermedad que es \\(99\\%\\) precisa:\n\nSi una persona tiene la enfermedad, la prueba es positiva el \\(99\\%\\) de las veces, es decir \\(P(\\text{Positivo}|\\text{Enfermedad}) = 0.99\\).\nSi una persona no tiene la enfermedad, la prueba es negativa el \\(99\\%\\) de las veces \\(P(\\text{Negativo}|\\text{No Enfermedad}) = 0.99\\), lo que significa que tiene un \\(1\\%\\) de falsos positivos \\(P(\\text{Positivo}|\\text{No Enfermedad}) = 0.01\\).\n\nEn este caso, deseamos saber cuál es la probabilidad de que una persona tenga la enfermedad si la prueba ha sido positiva \\(P(\\text{Enfermedad}|\\text{Positivo})\\).\nAplicamos el teorema de Bayes:\n\\[\nP(\\text{Enfermedad}|\\text{Positivo}) = \\frac{P(\\text{Positivo}|\\text{Enfermedad}) \\cdot P(\\text{Enfermedad})}{P(\\text{Positivo})}\n\\]\nPrimero, calculamos \\(P(\\text{Positivo})\\), la probabilidad total de que la prueba sea positiva:\n\\[\nP(\\text{Positivo}) = P(\\text{Positivo}|\\text{Enfermedad}) \\cdot P(\\text{Enfermedad}) +\n\\] \\[P(\\text{Positivo}|\\text{No Enfermedad}) \\cdot P(\\text{No Enfermedad})\n\\]\n\\[\nP(\\text{Positivo}) = (0.99 \\times 0.01) + (0.01 \\times 0.99) = 0.0099 + 0.0099 = 0.0198\n\\]\nAhora, aplicamos el teorema de Bayes:\n\\[\nP(\\text{Enfermedad}|\\text{Positivo}) = \\frac{0.99 \\times 0.01}{0.0198} = \\frac{0.0099}{0.0198} \\approx 0.50\n\\]\nEsto significa que, a pesar de que la prueba es bastante precisa, si una persona da positivo en la prueba, la probabilidad de que realmente tenga la enfermedad es aproximadamente \\(50\\%\\), debido a la baja prevalencia de la enfermedad en la población.\n\n\n\nEl teorema de Bayes es una herramienta poderosa para la toma de decisiones y la inferencia estadística, ya que proporciona un marco formal para actualizar nuestras creencias en base a la evidencia disponible.\nLos bayesianos interpretan la probabilidad como una medida de la creencia o confianza en un evento. Esta creencia puede ser actualizada a medida que se obtiene más información. Los parámetros son considerados como variables aleatorias y se describe su incertidumbre a través de distribuciones de probabilidad. Los intervalos de credibilidad bayesianos proporcionan una medida directa de la incertidumbre del parámetro: hay una probabilidad dada de que el verdadero parámetro esté dentro del intervalo de credibilidad. Esto parece tener más lógica que el enfoque frecuentista, pero es menos habitual. Los métodos bayesianos permiten la incorporación directa de conocimientos previos en el análisis a través de la distribución a priori.\nPor tanto, la principal diferencia entre los enfoques frecuentista y bayesiano radica en cómo interpretan el concepto de probabilidad. El enfoque frecuentista se basa en frecuencias de eventos, mientras que el enfoque bayesiano se basa en la incertidumbre y la actualización de las creencias.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#notación",
    "href": "intro.html#notación",
    "title": "1  Introducción",
    "section": "1.8 Notación",
    "text": "1.8 Notación\nA lo largo de este libro vamos a usar la siguiente notación:\n\n\\(X, Y\\): Variables\n\\(i\\): Identificador o índice para cada observación o clase\n\\(x_i\\): Valor que toma la variable \\(X\\) en la observación \\(i\\)\n\\(c_i\\): Marca de clase en datos agrupados\n\\(n\\): Número total de observaciones\n\\(k\\): Número de clases\n\\(n_i\\): Número de observaciones en la clase \\(i\\)\n\\(\\bar{x}\\): Media muestral de la variable \\(X\\)\n\\(s^2\\): Varianza muestral de la variable \\(X\\)\n\\(s\\): Desviación típica muestral de la variable \\(X\\)\n\\(\\mu\\): Media poblacional\n\\(\\sigma^2\\): Varianza poblacional\n\\(\\hat{[\\cdot]}\\): Simboliza un estimador de \\(\\cdot\\). Por ejemplo, \\(s = \\hat{\\sigma}\\) quiere decir que la desviación típica muestral \\(s\\) es un estimador de la desviación típica poblacional \\(\\sigma\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#resúmenes-gráficos-y-numéricos-útiles-en-la-inferencia-estadística",
    "href": "intro.html#resúmenes-gráficos-y-numéricos-útiles-en-la-inferencia-estadística",
    "title": "1  Introducción",
    "section": "1.9 Resúmenes gráficos y numéricos útiles en la inferencia estadística",
    "text": "1.9 Resúmenes gráficos y numéricos útiles en la inferencia estadística\n\n1.9.1 Métodos numéricos\nTal y como hemos indicado anteriormente, los métodos numéricos de inferencia estadística son técnicas y procedimientos utilizados para analizar datos y hacer inferencias sobre una población a partir de una muestra. Estos métodos se apoyan en herramientas matemáticas y computacionales para estimar parámetros, evaluar hipótesis y tomar decisiones informadas. A continuación, se describen los principales métodos numéricos en la inferencia estadística. No tienes que aprenderlos ahora puesto que vamos a trabajar con estos métodos a lo largo de todo el curso. Los veremos con mayor detalle en los próximos capítulos.\n\n\n\n\n\n\nEstimación puntual\n\n\n\n\n\nLa estimación puntual implica el uso de un solo valor estadístico de la muestra para estimar un parámetro de la población.\n\nMedia Muestral (\\(\\bar{x}\\)): Utilizada para estimar la media poblacional (\\(\\mu\\)).\nProporción Muestral (\\(\\hat{p}\\)): Utilizada para estimar la proporción poblacional (\\(p\\)).\nVarianza Muestral (\\(s^2\\)): Utilizada para estimar la varianza poblacional (\\(\\sigma^2\\)).\n\n\n\n\n\n\n\n\n\n\nEstimación por intervalo\n\n\n\n\n\nLa estimación por intervalo proporciona un rango de valores dentro del cual se espera que se encuentre el parámetro poblacional con un cierto nivel de confianza.\n\nIntervalo de confianza para la media:\n\nPara muestras grandes (\\(n \\ge 30\\)): \\(\\bar{x} \\pm Z_{\\alpha/2} \\left(\\frac{\\sigma}{\\sqrt{n}}\\right )\\)\n\nPara muestras pequeñas (\\(n &lt; 30\\)) y cuando la distribución es normal: \\(\\bar{x} \\pm t_{\\alpha/2, n-1} \\left(\\frac{s}{\\sqrt{n}}\\right)\\)\n\nIntervalo de confianza para la proporción: \\(\\hat{p} \\pm Z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\)\nIntervalo de confianza para la varianza: \\(\\left( \\frac{(n-1)s^2}{\\chi^2_{\\alpha/2, n-1}}, \\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2, n-1}} \\right)\\)\n\n\n\n\n\n\n\n\n\n\nContraste de hipótesis\n\n\n\n\n\nEl contraste de hipótesis es un procedimiento para tomar decisiones sobre los parámetros poblacionales basándose en la evidencia proporcionada por los datos muestrales.\n\nFormulación de hipótesis:\n\nHipótesis nula (\\(H_0\\)): Es la afirmación que se desea probar o refutar.\nHipótesis alternativa (\\(H_1\\)): Es la afirmación que se acepta si se rechaza (\\(H_0\\)).\n\nPruebas para la media:\n\nPrueba Z: Utilizada para muestras grandes (\\(n \\ge 30\\)) o cuando se conoce la desviación estándar poblacional (\\(\\sigma\\)).\nPrueba t: Utilizada para muestras pequeñas (\\(n &lt; 30\\)) y cuando no se conoce (\\(\\sigma\\)).\n\nPruebas para la proporción:\n\nPrueba Z para proporciones: Utilizada para evaluar hipótesis sobre una proporción poblacional.\n\nPruebas para la Varianza:\n\nPrueba Chi-cuadrado: Utilizada para evaluar hipótesis sobre la varianza poblacional.\n\n\n\n\n\n\n\n\n\n\n\nMétodos de resampling\n\n\n\n\n\nLos métodos de resampling, como el bootstrap y el jackknife, son técnicas computacionales utilizadas para estimar la precisión de los estadísticos de la muestra.\n\nBootstrap: Consiste en tomar múltiples muestras con reemplazo de los datos originales y calcular el estadístico de interés para cada muestra. Esto permite construir una distribución empírica del estadístico y estimar intervalos de confianza.\nJackknife: Involucra excluir sistemáticamente cada observación de la muestra y recalcular el estadístico de interés. Esto proporciona una manera de estimar el sesgo y la varianza del estimador.\n\n\n\n\n\n\n\n\n\n\nMétodos bayesianos\n\n\n\n\n\nLa inferencia bayesiana utiliza la probabilidad subjetiva para actualizar la creencia sobre los parámetros poblacionales basándose en la evidencia muestral.\n\nTeorema de Bayes: \\(P( \\theta |x) = \\frac{P(x | \\theta)P( \\theta)}{P(x)}\\), donde \\(P(\\theta|x)\\) es la distribución a posteriori, \\(P(x|\\theta)\\) es la verosimilitud, \\(P(\\theta)\\) es la distribución a priori y \\(P(x)\\) es la probabilidad marginal de los datos.\nSimulación Monte Carlo Markov Chain (MCMC): Es un método numérico para aproximar distribuciones posteriores complejas.\n\n\n\n\n\n\n\n\n\n\nPruebas no paramétricas\n\n\n\n\n\nCuando no se cumplen los supuestos de normalidad, se utilizan pruebas no paramétricas que no requieren asumir una distribución específica.\n\nPrueba de Wilcoxon: Para comparar medianas entre dos muestras emparejadas.\nPrueba de Mann-Whitney: Para comparar medianas entre dos muestras independientes.\nPrueba de Kruskal-Wallis: Extensión de la prueba de Mann-Whitney para más de dos grupos.\nPrueba de Chi-cuadrado: Para pruebas de independencia y homogeneidad en tablas de contingencia.\n\n\n\n\n\n\n1.9.2 Métodos gráficos\nA lo largo del curso usaremos numerosos métodos gráficos para explorar los datos dentro del contexto de la inferencia estadística. Algunas de los métodos básicos son:\n\n\n\n\n\n\nHistogramas\n\n\n\n\n\nGráficos de barras que muestran la distribución de frecuencias de datos cuantitativos.\n\n\n\n\n\n\n\n\n\nGráficos de caja (boxplots)\n\n\n\n\n\nMuestran la mediana, los cuartiles y los posibles valores atípicos.\n\n\n\n\n\n\n\n\n\nGráficos de dispersión (scatterplots)\n\n\n\n\n\nUtilizados para observar la relación entre dos variables cuantitativas.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#teorema-central-del-límite",
    "href": "intro.html#teorema-central-del-límite",
    "title": "1  Introducción",
    "section": "1.10 Teorema Central del Límite",
    "text": "1.10 Teorema Central del Límite\nEl Teorema Central del Límite (TCL) es uno de los principios más fundamentales en la estadística y la probabilidad. Establece que, bajo ciertas condiciones, la distribución de la suma (o el promedio) de un gran número de variables aleatorias independientes e identicamente distribuidas tiende a seguir una distribución Normal, independientemente de la distribución original de las variables.\nFormalmente, el TCL establece que si, \\(X_1,X_2,\\ldots,X_n\\) son variables aleatorias independientes e idénticamente distribuidas, con media \\(\\mu\\) y varianza \\(\\sigma^2&lt;\\infty\\), entonces, para \\(n\\) suficientemente grande se verifica:\n\\[\\bar{X} \\approx N \\left ( \\mu,\\frac{\\sigma^2}{n}\\right )\\] Este resultado es válido tanto para variables discretas como continuas, sean simétricas o asimétricas, unimodales o multimodales.\n\n\n\n\n\n\nPara recordar\n\n\n\nEl Teorema Central del Límite asegura que con muestas suficientemente grandes se pueden utilizar estimaciones basadas en la distribución Normal independientemente del tipo de distribución que siga la variable que nos interesa.\n\n\n\n\n\n\n\n\nEjemplo. Teorema Central del Límite\n\n\n\n\n\nAcabamos de ver que el TCL establece que, para una gran cantidad de muestras aleatorias tomadas de una población con una distribución cualquiera (con una media y una varianza finitas), la distribución de las medias muestrales tiende a ser Normal, independientemente de la forma de la distribución original.\nAquí tienes un ejemplo en R que ilustra el Teorema Central del Límite. Vamos a tomar muestras de una distribución no normal (por ejemplo, una distribución uniforme). Calcularemos las medias de estas muestras y observaremos cómo las medias muestrales se aproximan a una distribución normal.\n\n# Configuraciones iniciales\nset.seed(123)          # Fijamos la semilla para reproducibilidad\nn_samples &lt;- 1000      # Número de muestras\nsample_size &lt;- 30      # Tamaño de cada muestra\n\n# Generamos las muestras de una distribución uniforme\nsample_means &lt;- numeric(n_samples)\nfor (i in 1:n_samples) {\n  sample &lt;- runif(sample_size, min = 0, max = 10)\n  sample_means[i] &lt;- mean(sample)\n}\n\n# Crear un data frame con las medias muestrales\ndata &lt;- data.frame(sample_means)\n\n# Graficamos el histograma de las medias muestrales utilizando ggplot2\nggplot(data, aes(x = sample_means)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = mean(sample_means), sd = sd(sample_means)),\n                color = \"red\", size = 1) +\n  labs(title = \"Distribución de las Medias Muestrales\",\n       x = \"Medias Muestrales\", y = \"Densidad\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  annotate(\"text\", x = mean(sample_means), y = max(density(sample_means)$y) / 2,\n           label = \"Distribución Normal\", color = \"red\")\n\n\n\n\n\n\n\n\nPara visualizar el resultado, graficamos un histograma de las medias muestrales. Luego, superponemos una curva de densidad de una distribución normal con la misma media y desviación estándar que las medias muestrales para observar cómo se ajusta a una distribución normal.\nEl histograma de las medias muestrales se aproxima a una distribución normal, a pesar de que las muestras originales provienen de una distribución uniforme.\n\n\n\n\n\n\n\nHao, Jiangang, y Tin Kam Ho. 2019. «Machine learning made easy: a review of scikit-learn package in python programming language». Journal of Educational and Behavioral Statistics 44 (3): 348-61.\n\n\nHastie, Trevor, Robert Tibshirani, Jerome H Friedman, y Jerome H Friedman. 2009. The elements of statistical learning: data mining, inference, and prediction. Vol. 2. Springer.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al. 2013. An introduction to statistical learning. Vol. 112. Springer.\n\n\nKelleher, John D, Brian Mac Namee, y Aoife D’arcy. 2020. Fundamentals of machine learning for predictive data analytics: algorithms, worked examples, and case studies. MIT press.\n\n\nOsmani, Addy. 2012. Learning JavaScript Design Patterns: A JavaScript and jQuery Developer’s Guide. \" O’Reilly Media, Inc.\".\n\n\nOualline, Steve. 2003. Practical C++ programming. \" O’Reilly Media, Inc.\".\n\n\nWirth, Rüdiger, y Jochen Hipp. 2000. «CRISP-DM: Towards a standard process model for data mining». En Proceedings of the 4th international conference on the practical applications of knowledge discovery and data mining, 1:29-39. Manchester.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "",
    "text": "2.1 Preguntas\nTu objetivo principal durante el EDA es adquirir una comprensión profunda de los datos que se están analizando. La forma más sencilla de hacerlo es utilizar preguntas como herramientas para guiar la investigación. Cuando planteas una pregunta, ésta centra tu atención en una parte específica del conjunto de datos y te ayuda a decidir qué gráficos, modelos o transformaciones realizar.\nEDA es un proceso creativo y como tal, la clave para llevarlo a cabo consiste en el planteamiento de preguntas de calidad. ¿Qué preguntas son las correctas? La respuesta es que depende del conjunto de datos con el que se trabaje.\nAl inicio del análisis, puede resultar todo un desafío formular preguntas reveladoras, ya que aún no se conoce completamente la información contenida en el conjunto de datos. Si estás involucrado en un proceso de inferencia estadística, en muchas ocasiones, esas preguntas vendrán formuladas por un experto del dominio o por un superior con conocimientos estadísticos. Cada nueva pregunta que se plantee te llevará a explorar un nuevo aspecto de tus datos, aumentando así las posibilidades de hacer descubrimientos importantes.\nAlgunas de las preguntas que, generalmente, deberían de abordarse durante el EDA son:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#preguntas",
    "href": "eda.html#preguntas",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "",
    "text": "John Tukey\n\n\n\nMucho mejor una respuesta aproximada a la pregunta correcta, que a menudo es vaga, que una respuesta exacta a la pregunta incorrecta, que siempre se puede precisar.\n\n\n\n\n\n\n\n\n\nPara recordar\n\n\n\nDurante la preparación y limpieza de los datos acumulamos pistas sobre los modelos de aprendizaje más adecuados que podrán ser aplicados en etapas posteriores.\n\n\n\n\n¿Cuál es el tamaño de la base de datos? Es decir:\n\n¿Cuántas observaciones hay?\n¿Cuántas variables/características están medidas?\n¿Disponemos de capacidad de cómputo en nuestra máquina para procesar la base de datos o necesitamos más recursos?\n¿Existen valores faltantes?\n\n¿Qué tipo variables aparecen en la base de datos?\n\n¿Qué variables son discretas?\n¿Cuáles son continuas?\n¿Qué categorías tienen las variables?\n¿Hay variables tipo texto?\n\nVariable objetivo: ¿Existe una variable de “respuesta”?\n\n¿Binaria o multiclase?\n\n¿Es posible identificar variables irrelevantes?. Estudiar variables relevantes requiere, habitualmente, métodos estadísticos.\n¿Es posible identificar la distribución que siguen las variables?\nCalcular estadísticos resumen (media, desviación típica, frecuencia,…) de todas las variables de interés. Estudiaremos las propiedades de estos estimadores en el próximo capítulo.\nDetección y tratamiento de valores atípicos.\n\n¿Son errores de media?\n¿Podemos eliminarlos?\n\n¿Existe correlación entre variables?\n\n\n\n\n\n\n\nPara recordar\n\n\n\nUna correcta preparación y limpieza de datos implica, sin duda, un ahorro de tiempo en etapas posteriores del proyecto.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#entender-el-negocio",
    "href": "eda.html#entender-el-negocio",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.2 Entender el negocio",
    "text": "2.2 Entender el negocio\nLa comprensión del problema que estamos abordando representa una de las primeras etapas en cualquier proyecto de ciencia de datos. En la mayoría de los casos, esta tarea se realiza en estrecha colaboración con expertos en el dominio correspondiente, quienes a menudo son las personas que han solicitado (y a menudo financian) el análisis de datos. Es importante recordar que cualquier estudio que involucre ciencia de datos requiere un conocimiento profundo del dominio, el cual debe ser compartido con el científico de datos. Por lo tanto, el profesional de la ciencia de datos debe poseer un conocimiento suficiente para enfrentar con confianza los diversos desafíos que puedan surgir. Esta comprensión inicial permite establecer los objetivos del proyecto y procesar los datos de manera correcta para obtener información valiosa. A través de esta información, se busca derivar conocimientos aplicables. Este conocimiento puede ser aprendido y almacenado para su uso futuro, lo que lleva a la sabiduría, según la jerarquía de conocimiento presentada en la Figura Figura 2.1.\n\n\n\n\n\n\nFigura 2.1: Jerarquía de Conocimiento\n\n\n\n\n\n\n\n\n\nClaude Lévi-Strauss\n\n\n\n“El científico no es una persona que da las respuestas correctas, sino una persona que hace las preguntas correctas.”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#un-primer-vistazo-a-los-datos",
    "href": "eda.html#un-primer-vistazo-a-los-datos",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.3 Un primer vistazo a los datos",
    "text": "2.3 Un primer vistazo a los datos\nEn este capítulo vamos a trabajar con los datos de Bank Marketing del repositorio UCI. En primer lugar debemos comprender el problema. ¿Qué sabes del marketing bancario? En el caso que nos ocupa, los datos están relacionados con campañas de marketing directo (llamadas telefónicas) de una entidad bancaria portuguesa. El objetivo final, que abordaremos en cursos posteriores, es predecir si el cliente suscribirá un depósito a plazo (variable objetivo). En este curso nos conformamos con adquirir conocimiento de los datos, planteando algunas hipótesis de interés.\nLas variables que debemos estudiar son:\nVariables de entrada:\n\nDatos del cliente bancario:\n\n\nedad (variable numérica)\nempleo : tipo de empleo (variable categórica con las siguientes categorías: “admin.”, “desconocido”, “desempleado”, “directivo”, “empleada del hogar”, “empresario”, “estudiante”, “obrero”, “autónomo”, “jubilado”, “técnico”, “servicios”)\nestado civil : estado civil (variable categórica con categorías: “casado”, “divorciado”, “soltero”; nota: “divorciado” significa divorciado o viudo)\neducación (variable categórica con categorías: “desconocida”, “secundaria”, “primaria”, “terciaria”)\nimpago: ¿tiene un crédito impagado? (variable binaria con dos posibles valores: “sí”, “no”)\nsaldo: saldo medio anual, en euros (variable numérica)\nvivienda: ¿tiene préstamo para vivienda? (variable binaria: “sí”, “no”)\npréstamo: ¿tiene préstamo personal? (variable binaria: “sí”, “no”)\n# relacionado con el último contacto de la campaña actual:\ncontacto: tipo de comunicación del contacto (variable categórica: “desconocido”, “teléfono”, “móvil”)\ndía: día del mes del último contacto (variable numérica)\nmes: mes del año del último contacto (variable categórica: “ene”, “feb”, “mar”, …, “nov”, “dic”)\nduración: duración del último contacto, en segundos (variable numérica)\n\n\nOtros atributos\n\n\ncampaña: número de contactos realizados durante esta campaña y para este cliente (variable numérica, incluye el último contacto)\npdays: número de días transcurridos desde que el cliente fue contactado por última vez en una campaña anterior (variable numérica, -1 significa que el cliente no fue contactado previamente)\nprevious: número de contactos realizados antes de esta campaña y para este cliente (variable numérica)\npoutcome: resultado de la campaña de marketing anterior (variable categórica: “desconocido”, “otro”, “fracaso”, “éxito”)\n\n\nVariable de salida (objetivo deseado):\n\n17 - y: ¿ha suscrito el cliente un depósito a plazo? (variable binaria: “sí”, “no”)\n\n\n\n\n\n\nPara recordar\n\n\n\nA veces (muchas veces) la descripción que encontramos en una primera etapa no coincide al completo con los datos que luego nos entrega el cliente. Los datos suelen ser más complejos que la teoría detrás de los datos.\nEn otras ocasiones no se dispone de la descripción de las variables. En ese caso, ¡hay que hacer lo imposible por conseguirla! Si no conocemos el significado de una variable, difícilmente podremos interpretar los resultados asociados a ella.\n\n\nLeemos los datos con R.\n\nlibrary(tidyverse)\nbank = read.csv('https://raw.githubusercontent.com/rafiag/DTI2020/main/data/bank.csv')\ndim(bank)\n\n[1] 11162    17\n\nbank=as.tibble(bank)\nbank\n\n# A tibble: 11,162 × 17\n     age job       marital education default balance housing loan  contact   day\n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;int&gt;\n 1    59 admin.    married secondary no         2343 yes     no    unknown     5\n 2    56 admin.    married secondary no           45 no      no    unknown     5\n 3    41 technici… married secondary no         1270 yes     no    unknown     5\n 4    55 services  married secondary no         2476 yes     no    unknown     5\n 5    54 admin.    married tertiary  no          184 no      no    unknown     5\n 6    42 manageme… single  tertiary  no            0 yes     yes   unknown     5\n 7    56 manageme… married tertiary  no          830 yes     yes   unknown     6\n 8    60 retired   divorc… secondary no          545 yes     no    unknown     6\n 9    37 technici… married secondary no            1 yes     no    unknown     6\n10    28 services  single  secondary no         5090 yes     no    unknown     6\n# ℹ 11,152 more rows\n# ℹ 7 more variables: month &lt;chr&gt;, duration &lt;int&gt;, campaign &lt;int&gt;, pdays &lt;int&gt;,\n#   previous &lt;int&gt;, poutcome &lt;chr&gt;, deposit &lt;chr&gt;\n\n\nDisponemos de más de 10000 observaciones y un total de 17 variables.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#tipo-de-variables",
    "href": "eda.html#tipo-de-variables",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.4 Tipo de variables",
    "text": "2.4 Tipo de variables\nPara averiguar qué tipo de variables manejamos, ejecutar:\n\nstr(bank)\n\ntibble [11,162 × 17] (S3: tbl_df/tbl/data.frame)\n $ age      : int [1:11162] 59 56 41 55 54 42 56 60 37 28 ...\n $ job      : chr [1:11162] \"admin.\" \"admin.\" \"technician\" \"services\" ...\n $ marital  : chr [1:11162] \"married\" \"married\" \"married\" \"married\" ...\n $ education: chr [1:11162] \"secondary\" \"secondary\" \"secondary\" \"secondary\" ...\n $ default  : chr [1:11162] \"no\" \"no\" \"no\" \"no\" ...\n $ balance  : int [1:11162] 2343 45 1270 2476 184 0 830 545 1 5090 ...\n $ housing  : chr [1:11162] \"yes\" \"no\" \"yes\" \"yes\" ...\n $ loan     : chr [1:11162] \"no\" \"no\" \"no\" \"no\" ...\n $ contact  : chr [1:11162] \"unknown\" \"unknown\" \"unknown\" \"unknown\" ...\n $ day      : int [1:11162] 5 5 5 5 5 5 6 6 6 6 ...\n $ month    : chr [1:11162] \"may\" \"may\" \"may\" \"may\" ...\n $ duration : int [1:11162] 1042 1467 1389 579 673 562 1201 1030 608 1297 ...\n $ campaign : int [1:11162] 1 1 1 1 2 2 1 1 1 3 ...\n $ pdays    : int [1:11162] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...\n $ previous : int [1:11162] 0 0 0 0 0 0 0 0 0 0 ...\n $ poutcome : chr [1:11162] \"unknown\" \"unknown\" \"unknown\" \"unknown\" ...\n $ deposit  : chr [1:11162] \"yes\" \"yes\" \"yes\" \"yes\" ...\n\n\nCreamos las particiones sobre los datos y trabajamos sobre la partición de entrenamiento. Este paso cobrará significado en asignaturas posteriores, especialmente en las enfocadas en el aprendizaje automático (Machine Learning). Sin entrar en más detalles, podemos decir que la idea fundamental es estudiar las hipótesis en un subconjunto de la muestra disponible y evaluar la veracidad (o no) de dichas hipótesis en muestras diferentes.\n\n# Parciticionamos los datos\nset.seed(2138)\nn=dim(bank)[1]\nindices=seq(1:n)\nindices.train=sample(indices,size=n*.5,replace=FALSE)\nindices.test=sample(indices[-indices.train],size=n*.25,replace=FALSE)\nindices.valid=indices[-c(indices.train,indices.test)]\n\nbank.train=bank[indices.train,]\nbank.test=bank[indices.test,]\nbank.valid=bank[indices.valid,]\n\n\n\n\n\n\n\nAtrévete\n\n\n\n¿Te has hecho (ya) alguna pregunta sobre los datos? Si es así, no esperes más, busca la respuesta!\n\n\nPor ejemplo, ¿qué te parecen estas preguntas que nosotros proponemos?\n¿Qué día del año se producen más depósitos por parte de los estudiantes?\n\n\nClick para ver el código\nbank.train %&gt;% \n  filter(deposit==\"yes\") %&gt;% \n  count(month, day) %&gt;% \n  top_n(1,n)\n\n\n# A tibble: 1 × 3\n  month   day     n\n  &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n1 apr      30    79\n\n\n¿En qué mes del año realizan más depósitos los estudiantes?\n\n\nClick para ver el código\nbank.train %&gt;% \n  filter(deposit==\"yes\" & job==\"student\") %&gt;% \n  count(month) %&gt;% \n  top_n(1,n)\n\n\n# A tibble: 1 × 2\n  month     n\n  &lt;chr&gt; &lt;int&gt;\n1 apr      21\n\n\n¿Qué trabajo está asociado con el mayor porcentaje de depósitos?\n\n\nClick para ver el código\nbank.train %&gt;%\n  group_by(job) %&gt;%\n  mutate(d = n()) %&gt;%\n  group_by(job, deposit) %&gt;%\n  summarise(Perc = n()/first(d), .groups = \"drop\") %&gt;%\n  pivot_wider(\n    id_cols = job,\n    names_from = deposit,\n    values_from = Perc\n  ) %&gt;%\n  top_n(1)\n\n\n# A tibble: 1 × 3\n  job        no   yes\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 student 0.218 0.782\n\n\n\n\n\n\n\n\nRepaso\n\n\n\ndplyr es un paquete en R diseñado para facilitar la manipulación y transformación de datos de manera eficiente y estructurada. Fue desarrollado por Hadley Wickham y se ha convertido en una de las herramientas más populares en la ciencia de datos y análisis de datos.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#variable-objetivo",
    "href": "eda.html#variable-objetivo",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.5 Variable objetivo",
    "text": "2.5 Variable objetivo\nEn el ámbito del Aprendizaje Automático, en problemas de clasificación (Aprendizaje Supervisado) existe una variable de interés fundamental, es la variable respuesta o variable objetivo. En el próximo cuatrimestre se trata con detalle este tipo de problemas. En el caso que nos ocupa dicha variable es la característica: “deposit”. Vamos a estudiar la información que nos proporciona dicha variable.\n\nlibrary(ggplot2)\ntable(bank.train$deposit)\n\n\n  no  yes \n2908 2673 \n\nggplot(data=bank.train,aes(x=deposit,fill=deposit)) +\n  geom_bar(aes(y=(..count..)/sum(..count..))) +\n  scale_y_continuous(labels=scales::percent) +\n  theme(legend.position=\"none\") +\n  ylab(\"Frecuencia relativa\") +\n  xlab(\"Variable respuesta: deposit\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#visualizar-distribuciones",
    "href": "eda.html#visualizar-distribuciones",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.6 Visualizar distribuciones",
    "text": "2.6 Visualizar distribuciones\nLa forma de visualizar la distribución de una variable dependerá de si la variable es categórica o continua. Una variable es categórica si sólo puede tomar uno de un pequeño conjunto de valores. En R, las variables categóricas suelen guardarse como factores o vectores de caracteres. Para examinar la distribución de una variable categórica, utiliza un gráfico de barras:\n\nggplot(data = bank.train) +\n  geom_bar(mapping = aes(x = contact))\n\n\n\n\n\n\n\n\nPuedes obtener los valores exactos en cada categoría como sigue:\n\nbank.train%&gt;% \n  count(contact)\n\n# A tibble: 3 × 2\n  contact       n\n  &lt;chr&gt;     &lt;int&gt;\n1 cellular   4025\n2 telephone   383\n3 unknown    1173\n\n\nUna variable es continua si puede tomar cualquiera de un conjunto infinito de valores ordenados. Para examinar la distribución de una variable continua, utiliza un histograma:\n\nggplot(data = bank.train) +\n  geom_histogram(mapping = aes(x = age), binwidth = 5)\n\n\n\n\n\n\n\n\nUn histograma divide el eje \\(x\\) en intervalos equidistantes y, a continuación, utiliza la altura de una barra para mostrar el número de observaciones que se encuentran en cada intervalo. En el gráfico anterior, la primera barra muestra unas \\(100\\) observaciones (realmente son \\(119\\)) tienen un valor de edad por debajo de \\(22.5\\) años. Puede establecer la anchura de los intervalos en un histograma con el argumento binwidth, que se mide en las unidades de la variable \\(x\\).\n\n\n\n\n\n\nPara recordar\n\n\n\nSiempre se deben explorar una variedad de anchos de intervalo cuando trabajamos con histogramas, ya que diferentes anchos de intervalo pueden revelar diferentes patrones.\n\n\nPodemos representar funciones de densidad de probabilidad.\n\nggplot(bank.train, aes(x = age)) +\ngeom_density() +\nggtitle('KDE de edad en datos bank')\n\n\n\n\n\n\n\n\nOtro gráfico muy utilizado para variables cuantitativas univariantes es el boxplot, también llamado box-and-whisker plot (diagrama de caja y bigotes). Es especialmente útil para detectar posibles datos atípicos en los valores de una variable, siempre que su distribución sea parecida a una distribución Normal. El gráfico muestra:\n\nUna caja cuyos límites son el primer y el tercel cuartil de la distribución de valores.\nUna línea central, que marca la mediana.\nLos bigotes, que por defecto (en R) se extienden hasta 1.5 veces el valor del rango intercuartílico (IQR) por encima y por debajo de la caja.\nPuntos individuales, que quedan más allá del límite de los bigotes, marcan posibles datos atípicos.\n\nEn distribuciones muy asimétricas o con muchos valores extremos, muy diferentes a una distribución Normal, aparecerán demasiados puntos más allá de los bigotes y no se podrán apreciar fácilmente los atípicos (demasiados puntos considerados como tales). En ese caso, es conveniente intentar una transformación de la variable antes de representar el boxplot.\n\nggplot(bank.train, aes(x=deposit, y=age, color=deposit)) +\n  geom_boxplot()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#transformación-de-variables",
    "href": "eda.html#transformación-de-variables",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.7 Transformación de variables",
    "text": "2.7 Transformación de variables\n\n2.7.1 Transformación de variables cuantitativas\nEn algunos métodos de inferencia estadística y aprendizaje automático será necesario contar con variables que cumplan requisitos de normalidad. Por ejemplo, si tomamos la transformación \\(log\\) sobre la variable edad obtenemos una distribución multimodal que, probablemente, corresponda a la combinación de dos (o más) normales.\n\nggplot(data = bank.train) +\n  geom_histogram(mapping = aes(x = log(age)), binwidth = .1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPara recordar\n\n\n\nLos modelos de aprendizaje serán tan buenos como lo sean las variables de entrada de dichos algoritmos.\n\n\n\n2.7.1.1 Transformaciones para igualar dispersión\nCon frecuencia, el objetivo de la transformación de variables cuantitativas es obtener una variable cuya distribución de valores sea:\n\nMás simétrica y con menor dispersión que la original.\nMás semejante a una distribución normal (e.g. para algunos modelos lineales).\nRestringida en un intervalo de valores (e.g. \\([0,1]\\) ).\n\nLa forma más sencilla de detectar que alguna de nuestras variables necesita ser transformada es representar un gráfico que muestre la distribución de valores de la variable. Por ejemplo, un histograma o un diagrama de densidad de probabilidad (o ambos).\nEl uso de los logaritmos tiene su propia recomendación en preparación de datos (Fox y Weisberg 2018):\n\n\n\n\n\n\nJohn Fox\n\n\n\n“Si la variable es estrictamente positiva, no tiene un límite superior para sus valores, y su rango abarca dos o más órdenes de magnitud (potencias de \\(10\\)), entonces la transformación logarítmica suele ser útil. A la inversa, cuando la variable tiene un rango de valores pequeño (menor de un orden de magnitud), el logaritmo o cualquier otra transformación simple no ayudará mucho.”\n\n\nLa versión general de esta transformación son las transformaciones de escala-potencia (scaled-power transformations), también denominadas transformaciones de Box-Cox.\n\\[x(\\lambda)= \\begin{cases} \\frac{x^\\lambda-1}{\\lambda},& \\text{cuando } \\lambda \\neq 0,\\\\ log_e(x), & \\text{cuando } \\lambda = 0 \\end{cases}\\]\nLa función car::symbox(...) permite probar varias combinaciones típicas del parámetro \\(\\lambda\\) , para comprobar con cuál de ellas obtenemos una distribución más simétrica de valores.\n\nlibrary(car)\n\nbank.train %&gt;% symbox(~ age, data = .)\n\n\n\n\n\n\n\n\n\n\n2.7.1.2 Transformaciones para igualar dispersión\nTambién es bastante común aplicar transformaciones en datos cuantitativos para igualar las escalas de representación de las variables. En muchos modelos, si una de nuestras variables tiene una escala mucho mayor que las demás, sus valores tienden a predominar en los resultados, enmascarando la influencia del resto de variables en el modelo.\nPor este motivo, en muchos modelos es importante garantizar que todas las variables se representan en escalas comparables, de forma que ninguna predomine sobre el resto. Conviene aclarar un poco algunos términos que se suelen emplear de forma indistinta:\n\nReescalado o cambio de escala: Consiste en sumar o restar una constante a un vector, y luego multiplicar o dividir por una constante. Por ejemplo, para transformar la unidad de medida de una variable (grados Farenheit → grados Celsius).\nNormalización: Consiste en dividir por la norma de un vector, por ejemplo para hacer su distancia euclídea igual a \\(1\\).\nEstandarización: Consiste en restar a un vector una medida de localización o nivel (e.g. media, mediana) y dividir por una medida de escala (dispersión). Por ejemplo, si restamos la media y dividimos por la desviación típica hacemos que la distribución tenga media \\(0\\) y desviación típica \\(1\\).\n\nAlgunas aternativas comunes son:\n\\[\nEstandarización \\rightarrow Y=\\frac{X-\\overline{x}}{s_x}\n\\]\n\\[\nEscalado \\space min-max \\rightarrow Y=\\frac{X-min_x}{max_x-min_x}\n\\]\nEn R, la función scale() se puede utilizar para realizar estas operaciones de estandarización. Automáticamente, puede actuar sobre las columnas de un data.frame, aplicando la misma operación a todas ellas (siempre que todas sean cuantitativas).\n\n\n\n2.7.2 Transformación de variables cualitativas\nA diferencia de las variables cuantitativas, que representan cantidades numéricas, las variables cualitativas, también conocidas como variables categóricas, se utilizan para describir características o cualidades que no tienen un valor numérico intrínseco. Las variables cualitativas son esenciales en la investigación y el análisis de datos, ya que a menudo se utilizan para clasificar, segmentar y comprender información sobre grupos, categorías o características. Algunas técnicas comunes para analizar variables cualitativas incluyen la creación de tablas de frecuencia para contar la ocurrencia de cada categoría y el uso de gráficos como gráficos de barras o diagramas de sectores para visualizar la distribución de categorías. Estos análisis pueden proporcionar información valiosa sobre patrones, tendencias y relaciones en los datos cualitativos, lo que puede ser fundamental para tomar decisiones informadas en una amplia gama de campos, desde marketing hasta investigación social y más.\nLas variables cualitativas se dividen en dos categorías principales:\n\n\n\n\n\n\nVariables Cualitativas Nominales\n\n\n\n\n\nLas variables nominales representan categorías o etiquetas que no tienen un orden inherente. Ejemplos comunes incluyen el género (masculino, femenino, otro), el estado civil (soltero, casado, divorciado) o los colores (rojo, azul, verde). No se pueden realizar operaciones matemáticas en variables nominales, como sumar o restar.\n\n\n\n\n\n\n\n\n\nVariables Cualitativas Ordinales\n\n\n\n\n\nLas variables ordinales representan categorías con un orden natural o jerarquía, pero la distancia entre las categorías no es necesariamente uniforme ni conocida. Ejemplos incluyen la calificación de satisfacción del cliente (muy insatisfecho, insatisfecho, neutral, satisfecho, muy satisfecho) o el nivel de educación (primaria, secundaria, universitaria). Aunque se pueden establecer comparaciones de orden (por ejemplo, “mayor que” o “menor que”), no es apropiado realizar operaciones matemáticas en variables ordinales.\n\n\n\nEn R, las variables categóricas se denominan factores (factors) y sus categorías niveles (levels). Es importante procesarlos adecuadamente para que los modelos aprovechen la información que contienen estas variables. Por otro lado, si se codifica incorrectamente esta información los modelos pueden estar realizando operaciones absurdas aunque nos devuelvan resultados aparentemente válidos.\n\n\n\n\n\n\nR\n\n\n\nPor defecto, R transforma columnas tipo string en factores al leer los datos de un archivo. Además, por defecto, R ordena los niveles de los factores alfabéticamente, según sus etiquetas. Debemos tener cuidado con esto, puesto que en muchos análisis es muy importante saber qué nivel se está tomando como referencia, de entre los valores posibles de un factor, para comparar con los restantes. En ciertos modelos, la elección como referencia de uno de los valores del factor (típicamente el primero que aparece en la lista de niveles) cambia por completo los resultados, así como la interpretación de los mismos.\n\n\nEn variables ordinales se debe respetar estrictamente el orden preestablecido de los niveles. Por ejemplo, una ordenación (“regular” &lt; “bueno” &lt; “malo”) es inaceptable. Para establecer una ordenación explícita entre los niveles hay que especificarla manualmente si no coincide con la alfabética, y además configurar el argumento ordered = TRUE en la función factor():\n\nsatisfaccion &lt;- rep(c(\"malo\", \"bueno\", \"regular\"), c(3,3,3))\nsatisfaccion &lt;- factor(satisfaccion, ordered = TRUE, levels = c(\"malo\", \"regular\", \"bueno\"))\nsatisfaccion\n\n[1] malo    malo    malo    bueno   bueno   bueno   regular regular regular\nLevels: malo &lt; regular &lt; bueno\n\n\nPara comprobar qué nivel se toma como referencia en cada uno de los factores de una base de datos usamos la funión levels():\n\nlevels(bank.train$marital)\n\nNULL\n\n\nY esto, ¿es correcto? Veamos la distribución de las observaciones en las categorías de la variable marital:\n\nggplot(data = bank.train) +\n  geom_bar(mapping = aes(x = marital))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecomendación\n\n\n\nHabitualmente será más recomendable elegir como categoría de referencia para variables categóricas aquella categoría con mayor número de observaciones.\n\n\nPor tanto, en este caso particular deberíamos modificar la categoría de referencia como sigue:\n\nreorder_marital = factor(bank.train$marital, levels=(c(' married', ' single', ' divorced')))\nlevels(reorder_marital)\n\n[1] \" married\"  \" single\"   \" divorced\"\n\n\nNótese que la nueva variable aquí creada, reorder_marital, no ha sido incluida (aún) en el tibble bank. Para ello:\n\nbank.train$marital = reorder_marital\n\n\n2.7.2.1 Conversión de variables cuantitativas a variables categóricas\nLa conversión de variables cuantitativas a variables categóricas es un proceso importante en EDA que implica transformar datos numéricos en categorías. Esto se realiza con el propósito de simplificar el análisis, resaltar patrones específicos y facilitar la interpretación de los resultados. A continuación, se destacan algunas situaciones comunes en las que se realiza esta conversión y cómo se lleva a cabo:\n\nAgrupación de datos numéricos: En ocasiones, es útil agrupar datos numéricos en intervalos o categorías para resaltar tendencias generales. Por ejemplo, en un estudio de edades de una población, en lugar de analizar cada edad individual, se pueden crear grupos como “menos de 18 años”, “18-30 años”, “31-45 años” y así sucesivamente.\nCreación de variables binarias: A menudo, se convierten variables numéricas en variables binarias (\\(1\\) o \\(0\\)) para simplificar el análisis. Por ejemplo, en un estudio de satisfacción del cliente, se puede crear una variable binaria donde “\\(1\\)” indica clientes satisfechos y “\\(0\\)” indica clientes insatisfechos.\nCategorización de variables continuas: Las variables continuas, como ingresos o puntuaciones, se pueden convertir en categorías para segmentar la población. Esto puede ser útil en análisis demográficos o de segmentación de mercado.\nSimplificación de modelos: Algunos modelos de ML pueden beneficiarse de la conversión de variables cuantitativas a categóricas para mejorar la interpretación y la eficacia del modelo.\n\n\n\n\n\n\n\nPara recordar\n\n\n\nEl proceso de conversión de variables cuantitativas a categóricas generalmente implica definir criterios o reglas claras para agrupar los valores numéricos en categorías significativas. Estos criterios pueden basarse en conocimiento previo del dominio, EDA o consideraciones específicas del problema. En esta etapa te vendrá genial contar con la ayuda de un experto en el dominio de aplicación, y puedes llevar a cabo cambios catastróficos en caso de no contar con esa ayuda.\n\n\nEs importante tener en cuenta que la conversión de variables cuantitativas a categóricas debe realizarse de manera cuidadosa y considerar el impacto en el análisis. La elección de cómo categorizar los datos debe estar respaldada por una comprensión sólida del problema y los objetivos del estudio. Además, se debe documentar claramente el proceso de conversión para que otros puedan replicarlo y comprender las categorías resultantes.\nA modo de ejemplo, vamos a categorizar la varible age en la base de datos bank. Para ello elegimos (elegimos!!!) las siguientes agrupaciones en la variable edad: (0,40],(40,60],(60,100].\n\n bank.train &lt;- within(bank.train, {   \n  age.cat &lt;- NA # need to initialize variable\n  age.cat[age &lt;= 40] &lt;- \"Low\"\n  age.cat[age &gt; 40 & age &lt;= 60] &lt;- \"Middle\"\n  age.cat[age &gt; 60] &lt;- \"High\"\n   } )\n\nbank.train$age.cat &lt;- factor(bank.train$age.cat, levels = c(\"Low\", \"Middle\", \"High\"))\nsummary(bank.train$age.cat)\n\n   Low Middle   High \n  3116   2151    314",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#valores-comunes-y-atípicos",
    "href": "eda.html#valores-comunes-y-atípicos",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.8 Valores comunes y atípicos",
    "text": "2.8 Valores comunes y atípicos\nLos gráficos de barras relacionados con variables cualitativas nos han ayudado a identificar los valores más frecuentes o las categorías más repetidas en esas variables. Estos gráficos reflejan la frecuencia de cada categoría, es decir, el número de veces que aparece en el conjunto de datos. A veces ese número se representa en porcentaje respecto al número total de observaciones, proporcionando una visión relativa de la prevalencia en cada categoría. La moda es la categoría que aparece con mayor frecuencia en el conjunto de datos. Es especialmente útil para identificar la categoría más común y es aplicable a variables categóricas.\nEn el caso de las variables cuantitativas, el histograma de frecuencias se convierte en una herramienta gráfica sumamente útil para alcanzar este mismo objetivo.\n\n2.8.1 Estadísticos resumen\nA continuación te explicamos algunas medidas que resumen el comportamiento de una variable aleatoria cuantitativa:\n\n\n\n\n\n\nMedia\n\n\n\n\n\nLa media aritmética es el promedio de todos los valores de la variable. Se calcula sumando todos los valores y dividiendo por el número de observaciones. La media proporciona una indicación de la tendencia central de los datos.\n\n\n\n\n\n\n\n\n\nMediana\n\n\n\n\n\nLa mediana es el valor central en un conjunto de datos ordenados en forma ascendente o descendente. Divide el conjunto de datos en dos mitades iguales. La mediana es menos sensible a valores extremos que la media y es especialmente útil cuando los datos no siguen una distribución (aproximadamente) normal.\n\n\n\n\n\n\n\n\n\nModa\n\n\n\n\n\nLa moda es el valor que ocurre con mayor frecuencia en un conjunto de datos. Puede haber una o más modas en un conjunto de datos, y esta medida es especialmente útil para variables discretas.\n\n\n\n\n\n\n\n\n\nRango\n\n\n\n\n\nEl rango es la diferencia entre el valor máximo y el valor mínimo en un conjunto de datos. Proporciona una indicación de la dispersión o variabilidad de los datos.\n\n\n\n\n\n\n\n\n\nDesviación Estándar\n\n\n\n\n\nLa desviación estándar mide la dispersión de los datos con respecto a la media, y tiene sus mismas unidades de medida. Valores más altos indican mayor variabilidad. Es especialmente útil cuando se asume una distribución normal.\n\n\n\n\n\n\n\n\n\nCuartiles y Percentiles\n\n\n\n\n\nLos cuartiles dividen un conjunto de datos en cuatro partes iguales, mientras que los percentiles dividen los datos en cien partes iguales. Los cuartiles y percentiles son útiles para identificar valores atípicos y comprender la distribución de los datos.\n\n\n\n\n\n\n\n\n\nCoeficiente de Variación\n\n\n\n\n\nEl coeficiente de variación es una medida de la variabilidad relativa de los datos y se calcula como la desviación estándar dividida por la media. Se expresa como un porcentaje y es útil para comparar la variabilidad entre diferentes conjuntos de datos.\n\n\n\nEn R, podemos obtener algunos estadísticos resumen mediante la opción summary.\n\nsummary(bank.train$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.00   32.00   39.00   41.24   49.00   95.00 \n\n\nCuriosamente, R no tiene una función estándar incorporada para calcular la moda. Así que creamos una función de usuario para calcular la moda de un conjunto de datos en R. Esta función toma el vector como entrada y da el valor de la moda como salida.\n\n# Create the function.\nsummary_moda &lt;- function(v) {\n   uniqv &lt;- unique(v)\n   uniqv[which.max(tabulate(match(v, uniqv)))]\n}\n\nsummary_moda(bank.train$age)\n\n[1] 31\n\n\n\n\n2.8.2 Valores atípicos\nLos valores atípicos (outliers en inglés) son observaciones inusuales, puntos de datos que no parecen encajar en el patrón o el rango de la variable estudiada. A veces, los valores atípicos son errores de introducción de datos; otras veces, sugieren nuevos datos científicos importantes.\nCuando es posible, es una buena práctica llevar a cabo el análisis con y sin los valores atípicos. Si se determina que su influencia en los resultados es insignificante y no se puede identificar su origen, puede ser razonable reemplazarlos con valores faltantes y continuar con el análisis. Sin embargo, si estos valores atípicos tienen un impacto sustancial en los resultados, no se deben eliminar sin una justificación adecuada. En este caso, será necesario investigar la causa subyacente (por ejemplo, un error en la entrada de datos) y documentar su exclusión en el informe correspondiente.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#valores-faltantes",
    "href": "eda.html#valores-faltantes",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.9 Valores faltantes",
    "text": "2.9 Valores faltantes\nLos valores faltantes (missing), también conocidos como valores nulos o valores ausentes, son observaciones o datos que no están disponibles o que no han sido registrados para una o más variables en un conjunto de datos. Estos valores pueden surgir por diversas razones, como errores de entrada de datos, respuestas incompletas en una encuesta, fallos en la medición o simplemente porque cierta información no está disponible en un momento dado.\n\n\n\n\n\n\nPara recordar\n\n\n\nLa presencia de valores faltantes en un conjunto de datos es un problema común en el análisis de datos y puede tener un impacto significativo en la calidad de los resultados. Es importante abordar adecuadamente los valores faltantes, ya que pueden sesgar los análisis y conducir a conclusiones incorrectas si no se manejan correctamente.\n\n\nAlgunas de las estrategias comunes para tratar los valores faltantes incluyen:\n\nEliminación de filas o columnas: Si la cantidad de valores faltantes es pequeña en comparación con el tamaño total del conjunto de datos, una opción es eliminar las filas o columnas que contengan valores faltantes. Sin embargo, esta estrategia puede llevar a la pérdida de información importante.\nImputación de valores: Esta estrategia implica estimar o llenar los valores faltantes con valores calculados a partir de otros datos disponibles. Esto puede hacerse utilizando técnicas como la imputación media (rellenar con la media de la variable), imputación mediana (rellenar con la mediana), imputación de vecinos más cercanos o técnicas más avanzadas como regresión u otras técnicas de modelado.\nMarcadores especiales: En algunos casos, es útil asignar un valor específico (como “N/A” o “-999”) para indicar que un valor está ausente. Esto puede ser útil cuando se desea mantener un registro explícito de los valores faltantes sin eliminarlos o imputarlos. Es importante que, en este caso, el valor asignado no tenga otro significado. Por ejemplo, asignamos “-999” como marcador de valor faltante y sin embargo, es un valor plausible dentro del rango de valores de la variable.\nMétodos basados en modelos: Utilizar modelos estadísticos o de aprendizaje automático para predecir los valores faltantes en función de otras variables disponibles. Esto puede ser especialmente eficaz cuando los datos faltantes siguen un patrón que puede ser capturado por el modelo.\n\nLa elección de la estrategia adecuada para tratar los valores faltantes depende del contexto del análisis, la cantidad de datos faltantes y la naturaleza de los datos. Es fundamental abordar este problema de manera cuidadosa y transparente, documentando cualquier procedimiento de imputación o tratamiento de valores faltantes utilizado en el análisis para garantizar la integridad y la validez de los resultados.\n\n\n\n\n\n\nPeligro\n\n\n\nSustituir valores faltantes por otros obtenidos con técnicas y métodos estadísticos o de aprendizaje automático siempre es un riesgo, pues implica “inventar” datos allá donde no los hay.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#correlación-entre-variables",
    "href": "eda.html#correlación-entre-variables",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.10 Correlación entre variables",
    "text": "2.10 Correlación entre variables\nExisten varios métodos y técnicas para estudiar la correlación entre variables, lo que ayuda a comprender las relaciones entre las diferentes características en un conjunto de datos. En próximos cursos estudiarás que es de especial interés estudiar las relaciones entre la variable objetivo y las variables explicativas.\nPuedes desplegar los paneles siguientes para averiguar alguno de los métodos más comunes.\n\n\n\n\n\n\nMatriz de correlación\n\n\n\n\n\nLa matriz de correlación es una tabla que muestra las correlaciones entre todas las combinaciones de variables en un conjunto de datos. Los valores de correlación varían entre \\(-1\\) y \\(1\\), donde \\(-1\\) indica una correlación negativa perfecta, \\(1\\) indica una correlación positiva perfecta y \\(0\\) indica la ausencia de correlación. Este método es especialmente útil para identificar relaciones lineales entre variables numéricas.\n\n\n\n\n\n\n\n\n\nGráficos de dispersión\n\n\n\n\n\nLos gráficos de dispersión muestran la relación entre dos variables numéricas mediante puntos en un plano cartesiano. Estos gráficos permiten visualizar patrones de dispersión y tendencias entre las variables. Si los puntos se agrupan en una forma lineal, indica una posible correlación lineal.\n\n\n\n\n\n\n\n\n\nMapas de calor\n\n\n\n\n\nLos mapas de calor son representaciones visuales de la matriz de correlación en forma de un gráfico de colores. Permiten identificar rápidamente las relaciones fuertes o débiles entre variables y son útiles para resaltar patrones en grandes conjuntos de datos.\n\n\n\n\n\n\n\n\n\nCoeficiente de correlación de Pearson\n\n\n\n\n\nEste coeficiente mide la correlación lineal entre dos variables numéricas. Varía entre \\(-1\\) y \\(+1\\), donde valores cercanos a \\(-1\\) o \\(+1\\) indican una correlación fuerte, mientras que valores cercanos a \\(0\\) indican una correlación débil o nula.\n\n\n\n\n\n\n\n\n\nCoeficiente de correlación de Spearman\n\n\n\n\n\nEste coeficiente evalúa la correlación monotónica entre dos variables, lo que significa que puede detectar relaciones no lineales. Es útil cuando las variables no siguen una distribución normal.\n\n\n\n\n\n\n\n\n\nCoeficiente de correlación de Kendall\n\n\n\n\n\nSimilar al coeficiente de Spearman, evalúa la correlación entre variables, pero se centra en la concordancia de los rangos de datos, lo que lo hace útil para datos no paramétricos y muestras pequeñas.\n\n\n\n\n\n\n\n\n\nPruebas estadísticas\n\n\n\n\n\nLas pruebas estadísticas, como la prueba t de Student o la ANOVA, pueden utilizarse para evaluar si existe una diferencia significativa en los promedios de una variable entre diferentes categorías de otra variable. Si la diferencia es significativa, puede indicar una correlación entre las variables.\n\n\n\nVamos a estudiar la relación existente entre la variable objetivo deposit y la variable duration de la base de datos bank.\n\nggplot(bank.train, aes(x = log(duration), colour = deposit)) +\n  geom_density(lwd=2, linetype=1)\n\n\n\n\n\n\n\n\nPuede observarse una relación. Valores altos de la variable duración parecen estar relacionados con observaciones con deposit igual a ‘yes’.\n\ndf = bank.train %&gt;% \n      select(duration,deposit)%&gt;%\n      mutate(log.duration=log(duration))\n\n# Resumen para los casos de depósito\nsummary(df %&gt;% filter(deposit==\"yes\") %&gt;% .$log.duration)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.079   5.497   6.073   6.046   6.593   8.087 \n\n# Resumen para los casos de no depósito\nsummary(df %&gt;% filter(deposit==\"no\") %&gt;% .$log.duration)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.6931  4.5433  5.0999  5.0308  5.6276  7.5022 \n\n\nGráficamente, podemos comparar los boxplots.\n\nggplot(df, aes(deposit, log.duration)) +\n        geom_boxplot()\n\n\n\n\n\n\n\n\nPodemos determinar la importancia de relación. Por ejemplo, podemos realizar un test de la T para igualdad de medias. Estudiaremos estos conceptos en el Capítulo 3.\n\nt.test(log.duration ~ deposit, data = df)\n\n\n    Welch Two Sample t-test\n\ndata:  log.duration by deposit\nt = -45.828, df = 5464.5, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group no and group yes is not equal to 0\n95 percent confidence interval:\n -1.0583835 -0.9715488\nsample estimates:\n mean in group no mean in group yes \n         5.030821          6.045787 \n\n\n\n\n\n\n\n\nEjercicio\n\n\n\nComprenderás este resultado a lo largo del curso. De momento, puedes preguntar al profesor. Dejamos como ejercicio para el alumno la interpretación del resultado del test.\n\n\nEs posible estudiar la relación entre dos variables categóricas de manera gráfica.\n\nggplot(data = bank.train, aes(x = housing, fill = deposit)) +\n    geom_bar()\n\n\n\n\n\n\n\n\nParece haber una relación, estando asociados las observaciones de personas con casa propia a un mayor porcentaje de `no’ en la variable respuesta. Podemos obtener la tabla de contingencia:\n\ndata1=table(bank.train$housing, bank.train$deposit)\n\n\ndimnames(data1) &lt;- list(housing = c(\"no\", \"yes\"),\n                        deposit = c(\"no\", \"yes\"))\ndata1\n\n       deposit\nhousing   no  yes\n    no  1246 1688\n    yes 1662  985\n\n\nY el contraste correspondiente para la hipótesis nula de no existencia de relación. Estudiaremos estos conceptos en el Capítulo 3.\n\nchisq.test(bank.train$housing, bank.train$deposit)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  bank.train$housing and bank.train$deposit\nX-squared = 229.44, df = 1, p-value &lt; 2.2e-16\n\n\n\n\n\n\n\n\nEjercicio\n\n\n\nDejamos como ejercicio para el alumno la interpretación del resultado del test.\n\n\n\n\n\n\nFox, John, y Sanford Weisberg. 2018. An R companion to applied regression. Sage publications.\n\n\nTukey, John W et al. 1977. Exploratory data analysis. Vol. 2. Reading, MA.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "para.html",
    "href": "para.html",
    "title": "3  Estimación y contraste paramétrico",
    "section": "",
    "text": "3.1 Definición de estadístico\nUn estadístico es una medida calculada a partir de una muestra de datos que se utiliza para describir o resumir características de la muestra. En otras palabras, un estadístico es un valor numérico que resume o describe algún aspecto de los datos recolectados. Los estadísticos se utilizan ampliamente en análisis de datos, inferencia estadística y para hacer estimaciones sobre poblaciones más grandes basadas en la información obtenida de una muestra.\nSi te fijas bien, verás que en el Capítulo 2 ya hemos estado trabajando con estadísticos. Los estadísticos juegan un papel crucial en la inferencia estadística, donde se utilizan para hacer estimaciones o probar hipótesis sobre una población a partir de la información contenida en una muestra.\nEjemplos comunes de estadísticos incluyen:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimación y contraste paramétrico</span>"
    ]
  },
  {
    "objectID": "para.html#definición-de-estadístico",
    "href": "para.html#definición-de-estadístico",
    "title": "3  Estimación y contraste paramétrico",
    "section": "",
    "text": "Media: Promedio aritmético de los valores de una variable en la muestra.\nMediana: Valor que divide la muestra en dos partes iguales respecto a una variable.\nModa: Valor de una variable que aparece con mayor frecuencia en la muestra.\nVarianza: Medida de la dispersión de los datos respecto a la media.\nDesviación estándar: Raíz cuadrada de la varianza, que también mide la dispersión.\nCoeficiente de correlación: Medida de la relación entre dos variables.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimación y contraste paramétrico</span>"
    ]
  },
  {
    "objectID": "para.html#estimación-puntual",
    "href": "para.html#estimación-puntual",
    "title": "3  Estimación y contraste paramétrico",
    "section": "3.2 Estimación puntual",
    "text": "3.2 Estimación puntual\nLa estimación puntual es una técnica estadística que consiste en utilizar los datos de una muestra para calcular un valor único, denominado estimador puntual, que se usa como mejor aproximación de un parámetro desconocido de la población. Este parámetro puede ser, por ejemplo, la media, la varianza, la proporción, entre otros. La estimación puntual proporciona una forma simple y directa de hacer inferencias sobre parámetros poblacionales a partir de una muestra, aunque su simplicidad también implica que no proporciona información sobre la precisión o variabilidad de la estimación, aspectos que se abordan mediante la estimación por intervalos y otras técnicas inferenciales.\n\n3.2.1 Conceptos clave en la estimación puntual\nEstimador: Es una fórmula o función que se aplica a los datos de la muestra para obtener la estimación puntual. Por ejemplo, la media muestral (\\(\\bar{x}\\)) es un estimador de la media poblacional (\\(\\mu\\)). Formalmente, dada una variable aleatoria \\(X\\) con función de distribución \\(F_\\theta\\), con parámetro \\(\\theta\\) desconocido, un estadístico o estimador \\(T = T(X_1,\\ldots,X_n)\\) es una función real de la muestra aleatoria simple (m.a.s.) \\((X_1,\\ldots, X_n)\\) que estima el valor del parámetro desconocido. \\[\nT = T(X_1,\\ldots, X_n) = \\hat{\\theta}\n\\] Un estadístico es una variable aleatoria, y por lo tanto, tiene asociada una distribución que se denomina distribución muestral.\nPor ejemplo, la media \\[T_1(X_1,\\ldots,X_n)=\\bar{X}=\\frac{X_1+\\ldots+X_n}{n}\\] y la mediana \\[T_2(X_1,\\ldots,X_n)=\\frac{X_{(n/2)}+X_{(n/2+1)}}{2}\\] son estimadores.\nEstimación: Es el valor numérico específico obtenido al aplicar el estimador a una muestra concreta de datos. Por ejemplo, si \\(\\bar{x} = 5.4\\), esa es la estimación puntual de \\(\\mu\\).\n\n\n3.2.2 Ejemplos de estimadores puntuales\n\nMedia muestral (\\(\\bar{x}\\)): Utilizada para estimar la media poblacional (\\(\\mu\\)). Aplicamos el estimador a una realización de la muestra (\\(x_1,\\ldots,x_n\\)), obteniendo el estimador muestral: \\[\n\\bar{x} = T_1(x_1,\\ldots,x_n)= \\frac{1}{n} \\sum_{i=1}^n x_i\n\\]\nVarianza muestral (\\(s^2\\)): Utilizada para estimar la varianza poblacional (\\(\\sigma^2\\)). \\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2\n\\]\nDesviación típica muestral (\\(s\\)). Es la raíz cuadrada de la varianza muestral. Tiene las mismas unidades de medida que la variable original.\nProporción muestral (\\(\\hat{p}\\)): Utilizada para estimar la proporción poblacional (\\(p\\)). \\[\n\\hat{p} = \\frac{x}{n}\n\\] donde \\(x\\) es el número de éxitos en la muestra y \\(n\\) es el tamaño de la muestra.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimación y contraste paramétrico</span>"
    ]
  },
  {
    "objectID": "para.html#propiedades-de-los-estimadores",
    "href": "para.html#propiedades-de-los-estimadores",
    "title": "3  Estimación y contraste paramétrico",
    "section": "3.3 Propiedades de los estimadores",
    "text": "3.3 Propiedades de los estimadores\nExisten diferentes métodos para obtener estimadores de un parámetro poblacional. ¿Cómo elegir el estimador más adecuado para un parámetro desconocido? ¿Cuáles son las propiedades de un buen estimador?\nPara que un estimador sea considerado adecuado, generalmente debe cumplir con ciertas propiedades:\n\nInsesgadez: Un estimador es insesgado (o centrado) si, en promedio, coincide con el valor verdadero del parámetro que se estima. Es decir, el valor esperado del estimador es igual al parámetro poblacional.\n\n\\[E(\\hat{\\theta}) = \\theta\\] La comparaciones que implican estimadores sesgados a menudo se basan en el error cuadrático medio definido como: \\[\nECM(\\hat{\\theta})=E[(\\hat{\\theta}- \\theta)^2]=Var(\\hat{\\theta})+(E(\\hat{\\theta})- \\theta)^2=Eficiencia+  Sesgo\n\\] En este grado vas a volver a oir hablar de esta medida en la asignatura de regresión. En ese caso, la medida de error más empleada es: \\[\nECM=\\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{f}(x_i))^2\n\\] donde \\(\\hat{f}(x_i)\\) e sla predicción que hace un modelo de regresión mediante una función \\(\\hat{f}\\) para la i-ésima observación muestral \\(x_i\\).\n\nConsistencia: Un estimador es consistente si, a medida que el tamaño de la muestra aumenta, la estimación se aproxima al valor verdadero del parámetro. Es decir: \\[\nlim_{n  \\rightarrow \\infty}P(|\\hat{\\theta}-\\theta|\\geq\\delta)=0, \\forall\\delta&gt;0\n\\] donde \\(n\\) es el tamaño muestral.\nEficiencia: La varianza de un estimador debe ser lo más pequeña posible. Entre dos estimadores insesgados, el más eficiente es el que tiene menor varianza, es decir, el que proporciona estimaciones más precisas.\nSuficiencia: Un estimador es suficiente si utiliza toda la información contenida en la muestra sobre el parámetro que se está estimando.\n\n\n\n\n\n\n\nEjemplo Práctico. Insesgadez\n\n\n\n\n\nPara entender la propiedad de insesgadez en inferencia estadística, es útil realizar una simulación en R. Como hemos visto, la insesgadez de un estimador significa que, en promedio, el estimador coincide con el parámetro verdadero de la población.\nVamos a realizar una simulación para ilustrar esta propiedad utilizando la media muestral como estimador de la media poblacional. Generaremos muchas muestras aleatorias de una distribución normal y compararemos la media de las medias muestrales con la media verdadera de la población.\n\n# Cargar la librería ggplot2\nlibrary(ggplot2)\n\n# Parámetros de la simulación\nset.seed(123)  # Para reproducibilidad\nn_muestras &lt;- 1000  # Número de muestras\ntamano_muestra &lt;- 30  # Tamaño de cada muestra\nmedia_poblacional &lt;- 50  # Media verdadera de la población\ndesviacion_estandar &lt;- 10  # Desviación estándar de la población\n\n# Generar muestras y calcular medias muestrales\nmedias_muestrales &lt;- numeric(n_muestras)\nfor (i in 1:n_muestras) {\n  muestra &lt;- rnorm(tamano_muestra, mean = media_poblacional, sd = desviacion_estandar)\n  medias_muestrales[i] &lt;- mean(muestra)\n}\n\n# Calcular la media de las medias muestrales\nmedia_de_medias_muestrales &lt;- mean(medias_muestrales)\n\n# Imprimir resultados\ncat(\"Media verdadera de la población:\", media_poblacional, \"\\n\")\n\nMedia verdadera de la población: 50 \n\ncat(\"Media de las medias muestrales:\", media_de_medias_muestrales, \"\\n\")\n\nMedia de las medias muestrales: 49.93807 \n\n# Crear un data frame para ggplot\ndatos &lt;- data.frame(medias_muestrales)\n\n# Graficar las medias muestrales usando ggplot2\nggplot(datos, aes(x = medias_muestrales)) +\n  geom_histogram(bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  geom_vline(aes(xintercept = media_poblacional), color = \"red\", linetype = \"dashed\", size = 1.2) +\n  geom_vline(aes(xintercept = media_de_medias_muestrales), color = \"blue\", linetype = \"dashed\", size = 1.2) +\n  labs(title = \"Distribución de las Medias Muestrales\",\n       x = \"Medias Muestrales\",\n       y = \"Frecuencia\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  annotate(\"text\", x = media_poblacional, y = max(table(datos$medias_muestrales)) * 0.9, label = \"Media Verdadera\", color = \"red\", angle = 90, vjust = -0.5) +\n  annotate(\"text\", x = media_de_medias_muestrales, y = max(table(datos$medias_muestrales)) * 0.9, label = \"Media de las Medias Muestrales\", color = \"blue\", angle = 90, vjust = 1.5)\n\n\n\n\n\n\n\n\nHemos creado un bucle para generar n_muestras muestras aleatorias de una distribución normal con la media y desviación estándar especificadas. Para cada muestra, calculamos la media muestral y la almacenamos en el vector medias_muestrales. Calculamos la media de todas las medias muestrales generadas y mostramos la media verdadera de la población y la media de las medias muestrales.\nEl gráfico resultante muestra un histograma de las medias muestrales con líneas verticales indicando la media verdadera de la población y la media de las medias muestrales. Esto ilustra visualmente la propiedad de insesgadez del estimador de la media.\n\n\n\n\n\n\n\n\n\nEjemplo Práctico. Consistencia\n\n\n\n\n\nPara ilustrar la propiedad de consistencia de un estimador, podemos realizar una simulación similar a la anterior, pero en lugar de enfocarnos en la media de las medias muestrales, nos centraremos en cómo el estimador se aproxima a la verdadera media poblacional a medida que aumenta el tamaño de la muestra. En otras palabras, mostraremos cómo el estimador se vuelve más preciso a medida que se incrementa el tamaño de la muestra.\n\n# Cargar la librería ggplot2\nlibrary(ggplot2)\n\n# Parámetros de la simulación\nset.seed(12443)  # Para reproducibilidad\nn_simulaciones &lt;- 1000  # Número de simulaciones\ntamanos_muestra &lt;- seq(10, 1000, by = 10)  # Tamaños de las muestras\nmedia_poblacional &lt;- 50  # Media verdadera de la población\ndesviacion_estandar &lt;- 10  # Desviación estándar de la población\nmedias_estimadas &lt;- numeric(length(tamanos_muestra))  # Vector para almacenar medias estimadas\n\n# Realizar simulaciones para diferentes tamaños de muestra\nfor (i in 1:length(tamanos_muestra)) {\n  # Generar muestras y calcular medias muestrales\n  medias_muestrales &lt;- replicate(n_simulaciones, mean(rnorm(tamanos_muestra[i], mean = media_poblacional, sd = desviacion_estandar)))\n  # Calcular la media de las medias muestrales\n  medias_estimadas[i] &lt;- mean(medias_muestrales)\n}\n\n# Crear un data frame para ggplot\ndatos &lt;- data.frame(tamanos_muestra, medias_estimadas)\n\n# Graficar las medias estimadas vs. el tamaño de la muestra usando ggplot2\nggplot(datos, aes(x = tamanos_muestra, y = medias_estimadas)) +\n  geom_line(color = \"blue\") +\n  geom_hline(yintercept = media_poblacional, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Convergencia del Estimador a la Media Verdadera\",\n       x = \"Tamaño de la Muestra\",\n       y = \"Media Estimada\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nEl gráfico resultante muestra cómo las medias estimadas convergen hacia la media verdadera de la población a medida que aumenta el tamaño de la muestra. Esto ilustra la propiedad de consistencia del estimador. La línea roja representa la media verdadera de la población, mientras que la línea azul representa las medias estimadas en función del tamaño de la muestra. A medida que el tamaño de la muestra aumenta, las medias estimadas se acercan cada vez más a la media verdadera.\n\n\n\n\n3.3.1 Distribuciones muestrales\nHemos visto que usamos estadísticos para estimar los parámetros desconocidos de la población. Estamos interesados en estadísticos con buenas propiedades. Además, estos estadísticos son variables aleatorias con distribución de probabilidad.\nSabemos, por el TCL visto en la Introducción, que, teniendo el tamaño muestral adecuado, la distribución de los estadísticos será una Normal.\nPor ejemplo, dadas \\(X_1,\\ldots,X_n\\) variables aleatorias independientes e idénticamente distribuidas con media \\(\\mu\\) y varianza \\(\\sigma^2\\) conocida, la media muestral: \\[\n\\bar{X}=\\frac{X_1+\\ldots+X_n}{n}\n\\] tiene media igual a \\(E[\\bar{X}]=\\mu\\) y varianza igual a \\(V[\\bar{X}]=\\sigma^2/n\\).\nEntonces, por el TCL, con \\(n\\) suficientemente grande: \\[\n\\bar{X} \\sim N \\left ( \\mu,\\frac{\\sigma^2}{n} \\right )\n\\]\nOtro ejemplo sería, dadas \\(X_1,\\ldots,X_n\\) variables aleatorias independientes e idénticamente distribuidas con una distribución Bernoulli de parámetro \\(p\\). Para \\(n\\) suficientemente grande se tiene que: \\[\n\\hat{p} \\sim N \\left (p,\\frac{p(1-p)}{n} \\right ) \\Leftrightarrow \\frac{\\hat{p}-p}{\\sqrt{p(1-p)/n}} \\sim N(0,1)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimación y contraste paramétrico</span>"
    ]
  },
  {
    "objectID": "para.html#método-de-los-momentos",
    "href": "para.html#método-de-los-momentos",
    "title": "3  Estimación y contraste paramétrico",
    "section": "3.4 Método de los momentos",
    "text": "3.4 Método de los momentos\nEl método de los momentos es una técnica utilizada en estadística para estimar los parámetros desconocidos de una distribución de probabilidad. Fue introducido por el estadístico Karl Pearson en 1984. Este método se basa en igualar los momentos muestrales (calculados a partir de los datos observados) con los momentos teóricos (expresados en términos de los parámetros de la distribución).\n\n3.4.1 Definición de Momentos\nEn estadística, los momentos de una distribución son medidas que describen diversas características de la distribución, como su media, varianza, simetría y curtosis. Los momentos más comunes son:\n\nPrimer Momento (Media): \\(\\mu = E[X]\\)\nSegundo Momento (Varianza):\\(\\mu_2 = E[X^2]\\)\nTercer Momento (Asimetría):\\(\\mu_3 = E[X^3]\\)\nCuarto Momento (Curtosis):\\(\\mu_4 = E[X^4]\\)\n\n\n\nk-esimo Momento:\\(\\mu_k = E[X^k]\\)\n\nLos momentos poblacionales pueden ser vistos como funciones de los parámetros desconocidos \\(\\theta_1,\\ldots,\\theta_k\\). Se asume que se conoce el modelo de probabilidad de la variable objeto de estudio.\n\n\n3.4.2 Pasos del Método de los Momentos\nEl método de los momentos consiste en resolver un conjunto de ecuaciones y tiene los siguientes pasos:\n\nCalcular momentos muestrales: Se calculan los momentos muestrales de los datos observados. El (k)-ésimo momento muestral se define como: \\(m_k = \\frac{1}{n} \\sum_{i=1}^{n}X_i^k\\) donde \\(n\\) es el tamaño de la muestra y \\(X_i\\) son los valores de la muestra.\nIgualar momentos muestrales y teóricos: Se igualan los momentos muestrales con los momentos teóricos de la distribución. Los momentos teóricos se expresan en términos de los parámetros desconocidos que se desean estimar.\nResolver el sistema de ecuaciones: Se resuelve el sistema de ecuaciones resultante para encontrar los estimadores de los parámetros desconocidos. Fíjate que tenemos \\(k\\) ecuaciones y \\(k\\) parámetros (\\(\\theta_1,\\ldots,\\theta_k\\)). De modo que es posible despejar los parámetros de estas ecuaciones, que quedando estos parámetros en función de los momentos. En estas ecuaciones se sustituyen los momentos poblacionales por sus correspondientes momentos poblacionales. Esto da como resultado estimaciones de esos parámetros.\n\n\n\n\n\n\n\nEjemplo, distribución Normal\n\n\n\n\n\nSupongamos que deseamos estimar los parámetros (\\(\\mu\\)) y (\\(\\sigma^2\\)) de una distribución Normal (\\(N(\\mu, \\sigma^2)\\)).\n\nCalcular los momentos muestrales: \\[m_1 = \\frac{1}{n} \\sum_{i=1}^n X_i \\] \\[m_2 = \\frac{1}{n}\\sum_{i=1}^nX_i^2 \\]\nIgualar los momentos muestrales con los momentos teóricos: Para una distribución Normal, el primer momento teórico (media) es\n\n\\[\\mu_1=E(X)=\\mu\\]\ny el segundo momento teórico es:\n\\[\\mu_2=E(X^2)=Var(X)+E(X)^2=\\mu^2 + \\sigma^2\\]\nIgualando estos con los momentos muestrales obtenidos de los datos: \\[m_1 = \\bar{X}=\\mu\\] \\[m_2 = \\frac{1}{n}\\sum_{i=1}^n X_i^2=\\mu_2= \\mu^2 + \\sigma^2\\]\n\nResolver el sistema de ecuaciones: De la primera ecuación, tenemos:\n\n\\[\\hat{\\mu} = \\bar{X}\\]\nSustituyendo en la segunda ecuación: \\[\\hat{\\sigma}^2=\\frac{1}{n}\\sum_{i=1}^n X_i^2-\\bar{X}^2=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar{X})^2\\]\nQue son los estimadores de los parámetros.\n\n\n\n\n\n\n\n\n\nEjemplo, distribución Binomial\n\n\n\n\n\nSea \\(X_1,\\ldots,X_n\\) una muestra aleatoria simple de una \\(Binom(k,p)\\), con \\(k\\) y \\(p\\) desconocidos. Entonces, los momentos poblacionales son:\n\\[\\mu_1=E(X)=kp\\] \\[\\mu_2=E(X^2)=Var(X)+E(X)^2=kp(1-p)+k^2p^2\\] Los momentos muestrales son:\n\\[\nm_1=\\bar{X}\n\\] \\[\nm_2=\\frac{1}{n}\\sum_{i=1}^nX_i^2\n\\] Igualando los momentos poblacionales a los muestrales, obtenemos:\n\\[\nm_1=\\bar{X}=kp\n\\]\n\\[\nm_2=\\frac{1}{n}\\sum_{i=1}^nX_i^2=\\mu_2=kp(1-p)+k^2p^2\n\\] Despejando \\(k\\) y \\(p\\), obtenemos los estimadores:\n\\[\n\\hat{k}^2=\\frac{\\bar{X}^2}{\\bar{X}-(1/n)\\sum_{i=1}^n(X_i- \\bar{X})^2}\n\\] \\[\\hat{p}=\\frac{\\bar{X}}{\\hat{k}}\\]\n\n\n\n\n\n3.4.3 Ventajas y limitaciones\nLos estimadores de los momentos presentan interesantes propiedades estadísticas, aunqeu también tienen sus limitaciones.\nVentajas:\n\nSimplicidad: El método de los momentos es relativamente sencillo de aplicar y no requiere técnicas complejas de optimización.\nIntuición: Ofrece una interpretación intuitiva de los parámetros en términos de momentos.\n\nLimitaciones:\n\nPrecisión: Los estimadores de los momentos no siempre son los estimadores más eficientes (no tienen la mínima varianza posible).\nAplicabilidad: En algunas distribuciones complejas, los momentos pueden no existir o ser difíciles de calcular.\nConsistencia: Los estimadores de momentos no siempre son consistentes, especialmente en muestras pequeñas.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimación y contraste paramétrico</span>"
    ]
  },
  {
    "objectID": "para.html#método-de-la-máxima-verosimilud",
    "href": "para.html#método-de-la-máxima-verosimilud",
    "title": "3  Estimación y contraste paramétrico",
    "section": "3.5 Método de la máxima verosimilud",
    "text": "3.5 Método de la máxima verosimilud\nEl método de la máxima verosimilitud es una técnica estadística ampliamente utilizada para estimar los parámetros desconocidos de una distribución de probabilidad. Este método se basa en encontrar los valores de los parámetros que maximicen la función de verosimilitud, la cual mide la probabilidad de observar los datos dados los parámetros. El método de máxima verosimilitud es el método más popular para obtener un estimador. La idea básica es seleccionar el valor del parámetro que hace que los datos sean más probables.\nDado un modelo estadiıstico (es decir, una familia de distribuciones \\(f(·|\\theta)| \\theta \\in \\Theta\\) donde \\(\\theta\\) es el parámetro del modelo), el método de máxima verosimilitud encuentra el valor del parámetro del modelo \\(\\theta\\) que maximiza la función de verosimilitud:\n\\[\n\\hat{\\theta}(x)= \\max_{\\theta \\in \\Theta}L(\\theta|\\mathbf{x})\n\\]\nPara una muestra aleatoria \\(\\mathbf{x}=(x_1,\\ldots,x_n)\\) de una variable aleatoria \\(X\\), la verosimilitud es proporcional al producto de las probabilidades asociadas a los valores individuales: \\[\n\\prod_jP(X=x_j)\n\\] El término verosimilitud fue acuñado por Sir Roland Fisher.\nCuando \\(X\\) es una variable aleatoria continua, un valor muestral \\(x_j\\) debe considerarse como que está (en general) en el intervalo \\((x_j-\\delta,x_j+\\delta)\\), donde \\(\\delta\\) representa la precisión de la medición. La verosimilutd es entonces proporcional a: \\[\n\\prod_jP(x_j-\\delta&lt;X&lt;x_j+\\delta).\n\\] Si \\(\\delta\\) es suficientemente pequeño, esta expresión es aproximadamente proporcional a: \\[\n\\prod_jf(x_j).\n\\] donde \\(f\\) es la función de densidad de \\(X\\). Por lo tanto, la verosimilitd describe lo plausible que es un valor del parámetro poblacional, dadas unas observaciones concretas de la muestra.\n\n3.5.1 Conceptos básicos\n\nFunción de verosimilitud: La función de verosimilitud, \\(L(\\theta|\\mathbf{x})\\), para un conjunto de datos \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)\\) y un vector de parámetros \\(\\theta\\), es el producto de las funciones de densidad (o de probabilidad) de los datos observados, dadas las posibles realizaciones de \\(\\theta\\): \\[\nL(\\theta|\\mathbf{x}) = f(\\mathbf{x}|\\theta)=f(x_1,\\ldots,x_n|\\theta)=f(x_1|\\theta)f(x_2|\\theta)\\ldots f(x_n|\\theta)=\\prod_{i=1}^n f(x_i| \\theta)\n\\] donde \\(f(x_i|\\theta)\\) es la función de densidad (o de probabilidad) de \\(x_i\\) dado \\(\\theta\\).\nLog-Verosimilitud: Debido a que la función de verosimilitud puede implicar productos de muchos términos, es más práctico trabajar con su logaritmo natural, conocido como la log-verosimilitud: \\[\n\\ell(\\theta|\\mathbf{x}) = \\log L(\\theta|\\mathbf{x}) = \\sum_{i=1}^n \\log f(x_i|\\theta)\n\\]\n\n\n\n3.5.2 Procedimiento del método de Máxima Verosimilitud\n\nEspecificar la función de verosimilitud: Identificar la función de verosimilitud correspondiente a los datos observados y a la distribución supuesta.\nCalcular la Log-Verosimilitud: Tomar el logaritmo natural de la función de verosimilitud para obtener la función de log-verosimilitud.\nDerivar y resolver: Derivar la función de log-verosimilitud con respecto a cada parámetro y resolver las ecuaciones obtenidas igualando a cero (puntos críticos) para encontrar los estimadores de máxima verosimilitud (EMV).\nVerificar máximos: Asegurarse de que las soluciones encontradas corresponden a máximos y no a mínimos o puntos de inflexión, típicamente verificando la segunda derivada.\n\n\n\n\n\n\n\nEjemplo, distribución Normal\n\n\n\n\n\nSupongamos que tenemos una muestra \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)\\) de una distribución Normal con media \\(\\mu\\) y varianza \\(\\sigma^2\\), y queremos estimar estos parámetros.\n\nFunción de verosimilitud: La función de densidad para una distribución normal es: \\[\nf(x_i|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n\\] Por lo tanto, la función de verosimilitud es: \\[\nL(\\mu, \\sigma^2| \\mathbf{x}) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n\\]\nLog-Verosimilitud: Tomamos el logaritmo natural de la función de verosimilitud: \\[\n\\ell(\\mu, \\sigma^2|\\mathbf{x}) = \\sum_{i=1}^n \\left[ -\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right]\n\\]\nDerivadas y resolución: Derivamos la log-verosimilitud con respecto a \\(\\mu\\) y \\(\\sigma^2\\) y las igualamos a cero: \\[\n\\frac{\\partial \\ell}{\\partial \\mu} = \\sum_{i=1}^n \\frac{x_i - \\mu}{\\sigma^2} = 0 \\implies \\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n x_i\n\\] \\[\n\\frac{\\partial \\ell}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i=1}^n (x_i - \\mu)^2 = 0 \\implies \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\hat{\\mu})^2\n\\]\nAsí, los estimadores de máxima verosimilitud para \\(\\mu\\) y \\(\\sigma^2\\) son la media muestral y la varianza muestral, respectivamente.\n\n\n\n\n\n\n\n\n\n\nEjemplo, distribución Binomial\n\n\n\n\n\nSupongamos que tenemos una muestra de tamaño \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)\\) de una variable aleatoria Binomial con parámetro \\(p\\) que deseamos estimar. Se emplea dicha variable para describir el número de errores en las \\(n\\) pruebas asociadas. Se realiza un experimento y se obtienen un total de \\(4\\) errores en las \\(10\\) pruebas.\n\nFunción de verosimilitud: La verosimilud viene dada por:\n\n\\[\nL(p|\\mathbf{x}) = {10 \\choose 4}p^4(1-p)^6\n\\]\n\nLog-Verosimilitud: Tomamos el logaritmo natural de la función de verosimilitud: \\[\n\\ell(p|\\mathbf{x}) = log\\left({10 \\choose 4}\\right) +4log(p)+6log(1-p)\n\\]\nDerivadas y resolución: Derivamos la log-verosimilitud con respecto a \\(p\\) y las igualamos a cero: \\[\n\\frac{\\partial \\ell}{\\partial p} =\\frac{4}{p} -\\frac{6}{1-p}= 0 \\implies \\frac{4}{p}=\\frac{6}{1-p}\\implies 4-4p=6p\\implies 4=10p \\implies p=4/10=0.4\n\\]\n\nLa función de verosimilud nos informa, dados los datos, sobre los valores más plausibles (o creíbles) para el parámetroo \\(p\\).\n\n\n\n\n\n3.5.3 Ventajas y limitaciones\nLos estimadores de máxima verosimilutd presentan buenas propiedades estadísticas.\nVentajas:\n\nConsistencia: Los estimadores de máxima verosimilitud son consistentes, es decir, convergen en probabilidad al valor verdadero del parámetro a medida que el tamaño de la muestra aumenta.\nEficiencia: En muchos casos, los estimadores de máxima verosimilitud son eficientes, alcanzando la varianza mínima entre los estimadores insesgados (cumplen la igualdad de Cramér-Rao). El estimador máximo verosimil es asintóticamente eficiente y su distribución converge a la distribución Normal con valor esperado \\(\\theta\\) y la varianza es igual al inverso de la información de Fisher. La informacioon de Fisher es la cantidad de información que una muestra proporciona sobre el valor de un parámetro desconocido.\nFlexibilidad: Se puede aplicar a una amplia gama de distribuciones y modelos complejos.\nInvariantes: Si \\(T\\) es el estimador de máxima verosimilitud para \\(\\theta\\), entonces \\(\\tau(T)\\) es el estimador de máxima verosimilutd para \\(\\tau(\\theta)\\) para cualquier función \\(\\tau\\).\n\nLimitaciones:\n\nComplejidad computacional: Encontrar los estimadores de máxima verosimilitud puede implicar resolver ecuaciones no lineales, lo cual puede ser complejo y requerir técnicas numéricas.\nExistencia y unicidad: Los estimadores de máxima verosimilitud no siempre existen y, si existen, no siempre son únicos. En problemas reales, la derivada de la función de verosimilitud es, a veces, analíticamente intratable. En esos casos, se utilizan métodos iterativos para encontrar soluciones numéricas para las estimaciones de los parámetros.\nSesgo en muestras pequeñas: Los estimadores pueden ser sesgados en muestras pequeñas, aunque el sesgo disminuye a medida que el tamaño de la muestra aumenta.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimación y contraste paramétrico</span>"
    ]
  },
  {
    "objectID": "para.html#estimación-por-intervalo",
    "href": "para.html#estimación-por-intervalo",
    "title": "3  Estimación y contraste paramétrico",
    "section": "3.6 Estimación por intervalo",
    "text": "3.6 Estimación por intervalo\nLa estimación puntual proporciona una aproximación razonable para un parámetro de la población, pero no tiene en cuenta la variabilidad debido al tamaño muestral, la variabilidad en la población, el conocimiento de otros parámetros, etc.\nLa estimación por intervalo es una técnica en estadística que, a diferencia de la estimación puntual que proporciona un único valor, ofrece un rango de valores dentro del cual se espera que se encuentre el verdadero parámetro poblacional desconocido con un cierto nivel de confianza. Este rango se denomina intervalo de confianza.\nLa estimación por intervalos es una herramienta esencial en la Inferencia Estadística, ya que no solo ofrece una estimación del parámetro poblacional, sino que también proporciona un marco para entender la precisión y confiabilidad de esa estimación. Esto la convierte en una técnica poderosa para hacer inferencias más robustas y útiles basadas en datos muestrales.\n\n3.6.1 Conceptos clave en la estimación por intervalo\nIntervalo de Confianza (IC): Es un rango de valores calculado a partir de los datos de la muestra, que se utiliza para estimar el parámetro poblacional desconocido. Se expresa comúnmente como \\((\\text{Límite Inferior}, \\text{Límite Superior})\\).\nDada una muestra aleatoria simple \\(\\mathbf{X}=(X_1,X_2,\\ldots,X_n)\\) de una población \\(X\\) con función de distribución \\(F\\) que depende de un parámetro desconocido \\(\\theta\\), diremos que un estimador por intervalos de confianza del parámetro \\(\\theta\\) con un nivel de confianza de \\((1-\\alpha)=100*(1-\\alpha)\\%\\) es un intervalo de la forma \\((T_{inf}(\\mathbf{X}),T_{sup}(\\mathbf{X}))\\) que satisface: \\[P(\\theta \\in (T_{inf}(\\mathbf{X}),T_{sup}(\\mathbf{X})))=1-\\alpha\\]\nNivel de Confianza: Es la probabilidad de que el intervalo de confianza contenga el verdadero valor del parámetro poblacional. Se denota como \\(1 - \\alpha\\), donde \\(\\alpha\\) es el nivel de significancia. Un nivel de confianza común es el \\(95\\%\\), lo que significa que estamos un \\(95\\%\\) seguros de que el intervalo contiene el parámetro verdadero. Si repetimos el experimento \\(N\\) veces, en el \\(95\\%\\) de las ocasiones el verdadero valor del parámetro estará incluido en el intervalo proporcionado. Sin embargo es importante señalar que, dado que el experimento solo suele realizarse en una ocasión, no podemos estar seguros de que el verdadero valor del parámetro está incluido en nuestro intervalo. Estará incluido o no estará incluido, pero no podemos saber en qué situación nos encontramos. Estar seguro sería tanto como decir que conocemos el verdadero valor del parámetro. En ese caso, obviamente, no necesitaríamos estimación ninguna.\nError Estándar (SE): Es una medida de la variabilidad de un estimador. Se utiliza para calcular los límites del intervalo de confianza.\n\n\n3.6.2 Cálculo del Intervalo de Confianza\nEl cálculo de un intervalo de confianza generalmente sigue la fórmula:\n\\[\n\\text{Estimación Puntual} \\pm (\\text{Valor Crítico} \\times \\text{Error Estándar})\n\\] Para alcanzar el intervalo de confianza, generalmente se busca una cantidad (aleatoria) \\(C(\\mathbf{X},\\theta)\\) relacionada con el parámetro desconocido \\(\\theta\\) y con la muestras \\(\\mathbf{X}\\), cuya distribución sea conocida y no dependa del valor del parámetro. Esta cantidad recibe el nombre de pivote o cantidad pivotal para \\(\\theta\\).\nDado que conocemos la distribución del pivote, podemos usar los cuartiles \\(1-\\alpha/2\\) y \\(\\alpha/2\\) de dicha distribución, y la desviación estándar dle estimador por intervalos de confianza, para plantear la siguiente ecuación: \\[\nP(1-\\alpha/2 \\text{ cuantil}&lt; C(\\mathbf{X},\\theta)&lt;\\alpha/2 \\text{ cuantil}) = 1- \\alpha\n\\]Para obtener los extremos (inferior y superior) del estimador por intervalos de confianza \\(T_{inf}(\\mathbf{X})\\) y \\(T_{sup}(\\mathbf{X})\\), se resuelve la doble desigualdad en \\(\\theta\\). De este modo el intervalo de confianza al \\(100(1-\\alpha)\\%\\) para \\(\\theta\\) es \\((T_{inf}(\\mathbf{x}),T_{sup}(\\mathbf{x}))\\)\n\n\n3.6.3 Importancia de la estimación por intervalos\nA diferencia de la estimación puntual, el intervalo de confianza proporciona información sobre la precisión de la estimación y la variabilidad inherente en los datos muestrales.\nAdemás, la estimación por intervalo proporciona un rango de valores que es útil para la toma de decisiones en el dominio de aplicación.\nPodemos señalar que la estimación por intervalos es menos susceptible a errores muestrales y proporciona una medida más realista del parámetro poblacional que la obtenida con la estimación puntual.\n\n\n3.6.4 Intervalo de Confianza para la media (cuando la varianza es conocida)\nSea una muestra aleatoria simple \\(\\mathbf{X}\\) de tamaño \\(n\\) obtenida de \\(X\\). Supongamos que \\(X\\) sigue una distribución Normal con parámetros (\\(\\mu\\)) y varianza conocida (\\(\\sigma^2\\)). Fijate que este último supuesto es muy poco realista (no conocemos la media, pero conocemos la varianza). En este caso, el estadístico \\(\\bar{X}\\) tiene una distribución normal: \\[\n\\bar{X} \\sim N \\left( \\mu,\\sigma_{\\bar{X}}=\\frac{\\sigma}{\\sqrt{n}}\\right )\n\\]La desviación típica de \\(\\bar{X}\\) ( o de cualquier otro estadístico) se conoce como su error estándar.\nLa cantidad pivotal para \\(\\mu\\) es: \\[\nZ=\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\sim N \\left( 0,1 \\right )\n\\] Ahora, si \\(z_{1-\\alpha/2}\\) y \\(z_{\\alpha/2}\\) son los cuartiles \\((1-\\alpha/2)\\) y \\(\\alpha/2\\) de la distribución \\(N(0,1\\), entonces tenemos: \\[\nP(z_{1-\\alpha/2}&lt;Z&lt;z_{\\alpha/2})=1-\\alpha\n\\] Es decir: \\[\nP\\left (z_{1-\\alpha/2}&lt;\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}&lt;z_{\\alpha/2} \\right)=1-\\alpha\n\\] Hay que notar que para la distribución Normal: \\(z_{1-\\alpha/2}=-z_{\\alpha/2}\\)\nResolvemos la doble desigualdad para \\(\\mu\\): \\[\n-z_{\\alpha/2}&lt;\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}&lt;z_{\\alpha/2}\n\\] \\[\n-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}&lt;\\bar{X}-\\mu&lt;z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\n\\] \\[ -z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}-\\bar{X}&lt;-\\mu&lt;-\\bar{X}+z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\] \\[ -z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}+\\bar{X}&gt;\\mu&gt;\\bar{X}-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\n\\] De modo que el estimador por intervalos de confianza es: \\[\n\\left ( \\bar{X}-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}},\\bar{X}+z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\right)\n\\] y por tanto, el intervalo de confianza para la media se calcula como: \\[     \nIC_{1-\\alpha}(\\mu)=\\left (\\bar{x} -z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}},\\bar{x} +z_{\\alpha/2}  \\frac{\\sigma}{\\sqrt{n}} \\right )= \\left( \\bar{x} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\right)\n\\] donde \\(\\bar{x}\\) es la media muestral, \\(z_{\\alpha/2}\\) es el valor crítico del estadístico \\(z\\) para el nivel de confianza deseado, \\(\\sigma\\) es la desviación estándar poblacional, y \\(n\\) es el tamaño de la muestra.\n\n\n\n\n\n\nEjemplo. Intervalo de confianza para la media de una distribución normal\n\n\n\n\n\nSe ha probado que la altura de los alumnas de primer curso de la URJC se puede aproximar mediante una variable aleatoria con distribución normal con desviación típica \\(\\sigma=10\\) cm pero la media \\((\\mu)\\) desconocida. En un estudio con \\(50\\) alumnas se obtiene una media de \\(166\\) cm. Vamos a construir un intervalo de confianza al \\(95\\%\\) para \\(\\mu\\).\nSea \\(X\\) la altura, y sabemos que las variables independientes y identicamente distribuidas: \\[\nX_1,X_2,\\ldots,X_{50}\\sim N(\\mu,\\sigma^2=10^2).\n\\] Dado que: \\[\n\\frac{\\sigma^2}{n}=\\frac{10^2}{50}=2,\n\\] sabemos que: \\[\n\\bar{X}\\sim N(\\mu,2),\n\\] y por tanto: \\[\n\\frac{\\bar{X}-\\mu}{\\sqrt{2}}\\sim N(0,1).\n\\] Además los cuartiles de la distribución normal nos dicen que si \\(Z\\sim N(0,1)\\), entonces: \\[\nP(-1,96&lt;Z&lt;1,96)=0,95.\n\\] Por tanto: \\[\nP\\left(-1,96&lt;\\frac{\\bar{X}-\\mu}{\\sqrt{2}}&lt;1,96\\right)=0,95.\n\\] Despejamos \\(\\mu\\):\n\\[\nP\\left(\\bar{X}-1,96\\sqrt{2}&lt;\\mu&lt;\\bar{X}+1,96\\sqrt{2}\\right)=0,95.\n\\] Por tanto, si \\(\\bar{x}\\) es una realización particular de la variable aleatoria \\(\\bar{X}\\) en la muestra observada, el intervalo de confianza al \\(95\\%\\) será: \\[\nIC_{0.5}(\\mu)=\\bar{x}\\pm1.96 \\sqrt{2} = \\bar{x}\\pm2.77\n\\] En nuestro caso particular como la media era \\(\\bar{x}=166\\) cm, tenemos: \\[\nIC_{0.5}(\\mu)= 166\\pm2.77=(163.23 , 168.77)\n\\]\n\n\n\n\n\n3.6.5 Intervalo de Confianza para la media (cuando la varianza es desconocida)\nSea una muestra aleatoria simple \\(\\mathbf{X}\\) de tamaño \\(n\\) obtenida de \\(X\\). Supongamos que \\(X\\) sigue una distribución Normal con parámetros (\\(\\mu\\)) y varianza desconocida (\\(\\sigma^2\\)). Este supuesto es más realista que el caso anterior. Lo habitual es no disponer de información sobre la varianza poblacional.\nLa cantidad pivotal para \\(\\mu\\) es: \\[\nT=\\frac{\\bar{X}-\\mu}{s/\\sqrt{n}}\\sim t_{n-1}\n\\] donde \\(s^2\\) es la es la cuasi-varianza muestral: \\(s^2=\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2\\) y \\(t_n\\) es la distribución \\(t\\) de Student con \\(n\\) grados de libertad.\n\n\n\n\n\n\nRepaso\n\n\n\nEs posible que hayas estudiado la distribución \\(t\\) de Student en la asignatura de Probabilidad del primer curso del grado en Ciencia e Ingeniería de datos. En cualquier caso, repasamos: Si \\(T\\sim t_n\\) entonces: \\[\nE[T]=0\n\\] y \\[\nVar[T]=\\frac{n}{n-2}\n\\]\n\n\nSi \\(t_{n-1;1-\\alpha/2}\\) y \\(t_{n-1;\\alpha/2}\\) son los cuantiles \\((1-\\alpha/2)\\) y \\((\\alpha/2)\\) respectivamente de una distribución \\(t\\) de Student con \\(n-1\\) grados de libertad: \\[\nP(t_{n-1;1-\\alpha/2}&lt;T&lt;t_{n-1;\\alpha/2})=1-\\alpha\n\\] Es decir: \\[\nP(-t_{n-1;\\alpha/2}&lt;\\frac{\\bar{X}-\\mu}{s/\\sqrt{n}}&lt;t_{n-1;\\alpha/2})=1-\\alpha\n\\] Se resuelve la doble desigualdad para \\(\\mu\\) y se obtiene el estimador por intervalos de confianza: \\[\n\\left ( \\bar{X}-t_{n-1;\\alpha/2}\\frac{s}{\\sqrt{n}}, \\bar{X}+t_{n-1;\\alpha/2}\\frac{s}{\\sqrt{n}} \\right )\n\\] Resultando el intervalo de confianza: \\[\nIC_{1-\\alpha}(\\mu) = \\left ( \\bar{\\mathbf{x}}-t_{n-1;\\alpha/2}\\frac{s}{\\sqrt{n}}, \\bar{\\mathbf{x}}+t_{n-1;\\alpha/2}\\frac{s}{\\sqrt{n}} \\right )\n\\]\n\n\n\n\n\n\nEjemplo. Intervalo de confianza para la media de una distribución normal con varianza desconocida\n\n\n\n\n\nSe ha medido la temperatura media de una muestra aleatoria de \\(10\\) soluciones salinas, obteniendo los siguiente resultados:\n\n\n [1] 37.2 34.1 35.5 34.5 32.9 37.3 32.0 33.1 42.0 34.8\n\n\nSe nos mide calcular el IC al \\(90\\%\\) para la temperatura media, suponiendo que la temperatura de la solución salina se puede aproximar mediante una variable aleatoria con distribución normal.\nVemos que la población a estudiar es “X=temperatura de una solución salina” donde \\(X \\sim N(\\mu,\\sigma^2)\\) con \\(\\sigma^{2}\\) desconocida.\nTenemos: \\(n=10\\), \\(\\bar{x}=\\) 35.3, \\(s^2=\\) 8.5, \\(s=\\) 2.9. Además: \\(t_{n-1;\\alpha/2}=t_{9;0.05}=1.83\\).\nPor tanto el intervalo de confianza que buscamos es: \\[\nIC_{0.9}(\\mu)=\\left (35.3\\pm 1.83\\frac{2.91}{\\sqrt{10}} \\right )=(35.3\\pm 1.68)=(33.62;36.98)\n\\]\n\n\n\n\n\n3.6.6 Media poblacional para muestras grandes\nSea \\(\\mathbf{X}=(X_1,\\ldots,X_n)\\) una muestra aleatoria simple de tamaño \\(n\\) de una variable aleatoria \\(X\\). Supongamos que \\(X\\) sigue una distribución (conocida o no) con parámetros \\(\\mu\\) y \\(\\sigma^2\\). Además, supongamos que \\(n \\geq 30\\). Entonces, por el Teorema Central del Límite se tiene que la cantidad pivotal para \\(\\mu\\) cumple la siguiente propiedad:\n\\[\nZ=\\frac{\\bar{X}-\\mu}{\\hat{\\sigma}/\\sqrt{n}}\\sim  N(0,1)\n\\] Si \\(z_{1-\\alpha/2}\\) y \\(z_{\\alpha/2}\\) son los cuantiles \\((1-\\alpha/2)\\) y \\(\\alpha/2\\) de \\(N(0,1)\\), tenemos: \\[\nP(z_{1-\\alpha/2}&lt;Z&lt;z_{\\alpha/2})=1-\\alpha\n\\]\nY así, tenemos la condición: \\[\nP\\left ( -z_{\\alpha/2}&lt;\\frac{\\bar{X}-\\mu}{\\hat{\\sigma}/\\sqrt{n}}&lt;z_{\\alpha/2} \\right )=1-\\alpha\n\\] Obtenemos el estimador por intervalos de confianza resolviendo la doble desigualdad para \\(\\mu\\): \\[\n\\left ( \\bar{X}-z_{\\alpha/2}\\frac{\\hat{\\sigma}}{\\sqrt{n}}, \\bar{X}+z_{\\alpha/2}\\frac{\\hat{\\sigma}}{\\sqrt{n}} \\right )\n\\] El intervalo de confianza es: \\[\nIC_{1-\\alpha}(\\mu)=\\left ( \\bar{\\mathbf{x}}-z_{\\alpha/2}\\frac{\\hat{\\sigma}}{\\sqrt{n}}, \\bar{\\mathbf{x}}+z_{\\alpha/2}\\frac{\\hat{\\sigma}}{\\sqrt{n}} \\right )\n\\]\n\n\n\n\n\n\nEjemplo. Intervalo de confianza para media de muestras grandes\n\n\n\n\n\nSupongamos que estamos interesados en estimar la media del tiempo diario que las personas pasan en redes sociales en la URJC. Hemos tomado una muestra aleatoria de \\(200\\) estudiantes y medido el tiempo que pasan en redes sociales. Los resultados muestran una media muestral \\(\\bar{x}\\) de \\(2.5\\) horas al día con una desviación estándar muestral \\(s\\) de \\(0.8\\) horas. Es decir, de \\(2\\) horas y \\(30\\) minutos.\nQueremos calcular un intervalo de confianza del \\(95\\%\\) para la media del tiempo que la población pasa en redes sociales.\nCalculamos el error estándar de la media (SE):\n\\[\nSE = \\frac{s}{\\sqrt{n}}= \\frac{0.8}{\\sqrt{200}} \\approx 0.0566\n\\]\nDe este modo el IC al \\(95\\%\\) queda como sigue: \\[\nIC_{0.95}(\\mu)=\\left ( 2.5\\pm1.96 * 0.0566\\right) =(2.389, 2.611)\n\\]\nCon un \\(95\\%\\) de confianza, podemos decir que la media del tiempo diario que las personas pasan en redes sociales en la población está entre \\(2.389\\) y \\(2.611\\) horas. Esto es, entre \\(2\\) horas y \\(23\\) minutos y \\(2\\) horas y \\(37\\) minutos, aproximadamente.\n\n\n\n\n\n3.6.7 Intervalo de Confianza para la proporción\nSea \\(\\mathbf{X}=(X_1,\\ldots,X_n)\\) una muestra aleatoria simple de tamaño \\(n\\) de una variable aleatoria \\(X\\). Supongamos que \\(X\\) sigue una distribución de Bernoulli con parámetro \\(p\\). Esto es: \\[\n\\mu=E[X]=p\n\\] y \\[\n\\sigma^2=Var[X]=p(1-p)\n\\] Además, supongamos que \\(n \\geq 30\\). Entonces, por el Teorema Central del Límite se tiene que la cantidad pivotal para \\(\\hat{p}=\\bar{X}\\) cumple la siguiente propiedad:\n\\[\nZ=\\frac{\\hat{p}-p}{\\sqrt{\\hat{p}(1-\\hat{p})/n}}\\sim  N(0,1)\n\\] EL intervalo de confianza para estimar una proporción poblacional (\\(p\\)) es: \\[\nIC_{1-\\alpha}(p)=\\left ( \\hat{p} \\pm z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}} \\right )\n\\]\n\n\n\n\n\n\nEjemplo. Intervalo de confianza para proporción\n\n\n\n\n\nSupongamos que estamos realizando una encuesta para determinar la proporción de personas que apoyan una nueva política ambiental en una ciudad. Hemos encuestado a \\(1000\\) personas, y \\(560\\) de ellas han respondido que apoyan la nueva política.\nEs decir, la proporción muestral es: \\(\\hat{p} = \\frac{560}{1000} = 0.56\\)\nQueremos calcular un intervalo de confianza del \\(95\\%\\) para la proporción de apoyo en toda la población.\nEl error estándar para la proporción es:\n\\[\nSE = \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}=\\sqrt{\\frac{0.56 \\times (1 - 0.56)}{1000}}  \\approx 0.016\n\\]\nObteniendo: \\[\nIC_{0.95}(p)= 56 \\pm 1.96 * 0.016 =  (0.529, 0.591)\n\\]\nCon un \\(95\\%\\) de confianza, podemos decir que la proporción de personas en la población que apoyan la nueva política ambiental está entre el \\(52.9\\%\\) y el \\(59.1\\%\\).\n\n\n\n\n\n3.6.8 Interpretación de los intervalos de confianza\nSi calculamos un intervalo de confianza del \\(95\\%\\) para la media poblacional y, por ejemplo, obtenemos un intervalo de \\((5, 10)\\), esto no significa que hay un \\(95\\%\\) de probabilidad de que la media poblacional esté en ese intervalo en un caso particular, sino que, si repetimos este procedimiento muchas veces, el \\(95\\%\\) de los intervalos construidos contendrán la verdadera media poblacional. Podríamos decir que estamos un \\(95\\%\\) seguros de que la media poblacional se encuentra entre \\(5\\) y \\(10\\), pero ¡ojo!, la media poblacional (cuyo valor desconocemos) estará o no estará en ese intervalo.\n\n\n\n\n\n\nSimulación Intervalo de Confianza\n\n\n\nVamos a realizar un ejercicio de simulación para interpretar correctamente el concepto frecuentista de intervalo de confianza. Para ello, generamos \\(100\\)muestras de tamaño \\(n=50\\) de una distribución \\(X\\sim N(\\mu=10,sigma^2=1)\\). Para cada una de estas muestras, se construye un intervalo de confianza para la media con \\(\\alpha=0.05\\). Y representamos todos esos intervalos de confianza en un único gráfico. En verde se pintan los intervalos de confianza que incluyen el verdadero valor del parámetro \\(10\\). En rojo los que no.\n\n\nClick para ver el código\nset.seed(3983)\nlibrary(dplyr)\nlibrary(ggplot2)\nic=matrix(0,100,5)\nic[,1]=seq(1:100)\nic=as.data.frame(ic)\ncolnames(ic)=c(\"id\",\"estimador\",\"inferior\",\"superior\",\"resultado\")\nic$resultado=\"in\"\nfor (i in 1:100){\n  muestra=rnorm(50,10,1)\n  ic[i,2]=mean(muestra)\n  ic[i,3]=mean(muestra)-1.96*1/sqrt(50)   \n  ic[i,4]=mean(muestra)+1.96*1/sqrt(50) \n}\nic$resultado=!(ic[,3]&gt;10 | ic[,4]&lt;10)\nic %&gt;%\n  ggplot(aes(estimador, id, color = resultado)) +\n  geom_point() +\n  geom_segment(aes(x =inferior, y = id, xend = superior, yend = id, color = resultado))+\n  geom_vline(xintercept = 10, linetype = \"dashed\") +\n  ggtitle(\"Varios Intervalos de Confianza\") +\n  scale_color_manual(values = c(\"#FF3333\", \"#009900\")) +\n  theme_minimal() +\n  theme(axis.text.y = element_blank(), \n        axis.title.y = element_blank(),\n        axis.title.x = element_blank(),\n        legend.position = \"none\",\n        plot.title.position = \"plot\")\n\n\n\n\n\n\n\n\n\n\n¿Cuántos intervalos de confianza, de entre los \\(100\\), contienen al verdadero valor del parámetro? Razona ese resultado.\n¿Cuándo se toma una única muestra, cómo podrías estar seguro de estar en uno de los intervalos de confianza que recoge el verdadero valor del parámetro?\n¿Cómo crees que afecta a la longitud del intervalo de confianza los siguientes aspectos:?\n\nTamaño muestral\nNivel de confianza\n\n\nDiscute estas cuestiones con tu profesor.\n\n\n\n\n3.6.9 Determinación del tamaño muestral\nDeterminar el tamaño adecuado de una muestra es crucial en la inferencia estadística, ya que un tamaño muestral adecuado garantiza que los intervalos de confianza sean precisos y que las conclusiones obtenidas sean representativas de la población. Las técnicas para determinar el tamaño muestral están relacionadas directamente con los intervalos de confianza y se basan en varios factores, entre los que se incluyen el nivel de confianza deseado, la precisión (o margen de error) deseada y la variabilidad esperada en la población.\nLos factores clave para determinar el tamaño muestral son:\n\nNivel de confianza (\\(1-\\alpha\\)):\n\nEl nivel de confianza indica el grado de certeza de que el intervalo de confianza contiene el parámetro poblacional. Tal y como hemos indicado anteriormente, niveles de confianza comunes son \\(90\\%\\), \\(95\\%\\) y \\(99\\%\\). Un nivel de confianza más alto requiere una muestra más grande para asegurar la misma precisión.\n\nMargen de error (E):\n\nEl margen de error es la máxima diferencia tolerable entre la estimación muestral y el valor real del parámetro poblacional. Un margen de error más pequeño requiere una muestra más grande para asegurar una estimación precisa.\n\nVariabilidad poblacional (\\(\\sigma\\)):\n\nLa variabilidad en la población, medida por la desviación estándar, afecta directamente al tamaño muestral. Una mayor variabilidad requiere una muestra más grande para obtener una estimación precisa.\n\n\nA continuación mostramos algunos ejemplos del cálculo del tamaño muestral para diferentes situaciones.\n\n3.6.9.1 Tamaño muestral para estimar una media poblacional de una Normal\nEl tamaño muestral \\(n\\) necesario para estimar una media poblacional con un margen de error \\(E\\) y un nivel de confianza \\(1 - \\alpha\\) se puede calcular usando la fórmula:\n\\[\nn = \\left( \\frac{z_{\\alpha/2} \\cdot \\sigma}{E} \\right)^2\n\\]\ndonde:\n\n\\(z_{\\alpha/2}\\) es el valor crítico del estadístico \\(z\\) correspondiente al nivel de confianza deseado.\n\\(\\sigma\\) es la desviación estándar de la población (si es desconocida, se puede usar la desviación estándar de la muestra \\(s\\)).\n\nEfectivamente, teníamos que el intervalo de confianza para la media se obtenía mediante la fórmula: \\[\n\\left ( \\bar{x} \\pm z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\right )\n\\] Y buscamos el \\(n\\) tal que la desviación respecto a la media sea menor que \\(E\\), es decir: \\[\nz_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}  &lt; E\n\\] Esto es: \\[\nn &gt;  \\left ( \\frac{z_{\\alpha/2} \\sigma}{\\sqrt{n}} \\right )^2\n\\]\n\n\n\n\n\n\nEjemplo. Tamaño muestral para la estimación de una Media\n\n\n\n\n\nSupongamos que deseamos estimar la media de una población con un nivel de confianza del \\(95\\%\\), un margen de error de \\(5\\) unidades y se estima que la desviación estándar de la población es \\(15\\) unidades. El valor crítico \\(z_{\\alpha/2}\\) para un nivel de confianza del \\(95\\%\\) es aproximadamente \\(1.96\\).\n\\[\nn = \\left( \\frac{1.96 \\cdot 15}{5} \\right)^2 = \\left( \\frac{29.4}{5} \\right)^2 \\approx 34.57\n\\]\nPor lo tanto, necesitamos una muestra de al menos \\(35\\) individuos.\n\n\n\n\n\n3.6.9.2 Tamaño muestral para estimar una proporción poblacional\nEl tamaño muestral \\(n\\) necesario para estimar una proporción poblacional \\(p\\) con un margen de error \\(E\\) y un nivel de confianza \\(1 - \\alpha\\) se puede calcular usando la fórmula:\n\\[\nn = \\frac{z_{\\alpha/2}^2 \\cdot p \\cdot (1 - p)}{E^2}\n\\]\ndonde:\n\n\\(p\\) es la proporción esperada (si no se conoce, se usa \\(p = 0.5\\) para maximizar el tamaño muestral).\n\\(z_{\\alpha/2}\\) es el valor crítico del estadístico \\(z\\) correspondiente al nivel de confianza deseado.\n\n\n\n\n\n\n\nEjemplo. Tamaño muestral para la estimación de una proporción\n\n\n\n\n\nSupongamos que deseamos estimar la proporción de personas que aprueban una nueva ley con un nivel de confianza del \\(95\\%\\), un margen de error del \\(3\\%\\) \\((0.03)\\) y se estima que la proporción esperada es \\(p = 0.5\\). El valor crítico \\(z_{\\alpha/2}\\) para un nivel de confianza del \\(95\\%\\) es aproximadamente \\(1.96\\).\n\\[\nn = \\frac{1.96^2 \\cdot 0.5 \\cdot (1 - 0.5)}{0.03^2} = \\frac{3.8416 \\cdot 0.25}{0.0009} = \\frac{0.9604}{0.0009} \\approx 1067.11\n\\]\nPor lo tanto, necesitamos una muestra de al menos \\(1068\\) individuos.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimación y contraste paramétrico</span>"
    ]
  },
  {
    "objectID": "para.html#contraste-de-hipótesis",
    "href": "para.html#contraste-de-hipótesis",
    "title": "3  Estimación y contraste paramétrico",
    "section": "3.7 Contraste de hipótesis",
    "text": "3.7 Contraste de hipótesis\nLos contrastes de hipótesis son una herramienta fundamental en la inferencia estadística utilizada para tomar decisiones basadas en datos muestrales. Permiten evaluar si los datos disponibles proporcionan suficiente evidencia en contra de una hipótesis previamente establecida sobre una población.\nEl contraste de hipótesis es un proceso estructurado para evaluar afirmaciones sobre parámetros poblacionales utilizando datos muestrales. Mediante la formulación de hipótesis, selección de niveles de significancia, elección de estadísticas de prueba y evaluación del valor p, podemos tomar decisiones informadas y cuantitativamente justificadas. Este enfoque es fundamental en muchas áreas de investigación y análisis de datos, proporcionando un marco riguroso para la Inferencia Estadística.\n\n3.7.1 Conceptos básicos\nHipótesis Nula \\((H_0)\\):\n\nLa hipótesis nula es una afirmación sobre un parámetro poblacional que se asume verdadera hasta que se presente suficiente evidencia en contra. Se asume inicialmente que la hipótesis nula es correcta (semejante a suponer inocencia en un juicio a menos que se pruebe la culpabilidad). Habitualmente corresponde al estatus quo. Esto es, generalmente, la hipótesis nula representa un estado de “no efecto” o “no diferencia”.\n\nEjemplo: \\((H_0: \\mu = 50)\\) (la media poblacional es \\(50\\)). En este ejemplo, la idea fundamental del contraste sería toma una muestra aleatoria simple de la población, estudiar su media, y ver si hay evidencia suficiente como para rechazar la hipótesis nula establecida. La probabilidad de que la media sea exactamente igual a \\(50\\) en la muestra es muy baja. Es decir, probablemente \\(\\bar{\\mathbf{x}} \\neq 50\\). Sin embargo, lo importante para rechazar la hipótesis nula es si la diferencia encontrada entre la media muestral y \\(50\\) es tan grande como para rechazar que podría ser \\(50\\). Esto va a depender fuertemente del tamaño muestral y de la variabilidad de las observaciones en la muestra.\n\n\nHipótesis Alternativa \\((H_1)\\):\n\nLa hipótesis alternativa es una afirmación que contrasta con la hipótesis nula y representa el efecto o diferencia que se desea detectar.\n\nEjemplo: \\((H_1: \\mu \\neq 50)\\) (la media poblacional no es 50).\nEjemplo: \\((H_1: \\mu \\geq 50)\\) (la media poblacional es mayor o igual que 50).\n\n\n\n\n\n\n\n\nEjemplo \\(H_0\\) vs \\(H_1\\). Media poblacional\n\n\n\n\n\nSupongamos que una empresa de educación en línea afirma que sus estudiantes pasan en promedio al menos \\(4\\) horas diarias estudiando en su plataforma. Queremos comprobar si esta afirmación es cierta basándonos en una muestra de estudiantes.\n\nLa hipótesis nula es la afirmación que queremos poner a prueba y que asumimos verdadera inicialmente. En este caso, la hipótesis nula es que la media del tiempo de estudio diario es de \\(4\\) horas. \\[\nH_0: \\mu \\geq 4 \\text{ horas}\n\\]\nLa hipótesis alternativa es lo que queremos demostrar y se contrapone a la hipótesis nula. En este caso, queremos ver si el tiempo de estudio diario es menor de \\(4\\) horas. Fíjate que la empresa podría estar “inflando” sus resultados y lo “intersante” en este caso es “demostrar” que realmente los alumnos pasan menos tiempo en la plataforma. \\[\nH_1: \\mu &lt; 4 \\text{ horas}\n\\]\n\n\n\n\n\n\n\n\n\n\nEjemplo \\(H_0\\) vs \\(H_1\\). Proporción poblacional\n\n\n\n\n\nSupongamos que el rectorado de la URJC afirma que menos del \\(20\\%\\) de los estudiantes de sus grados, fuman. Queremos verificar si la proporción de fumadores es mayor al \\(20\\%\\).\n\nLa hipótesis nula es la afirmación que queremos poner a prueba y que asumimos verdadera inicialmente. En este caso, la hipótesis nula es que la proporción de fumadores es menor o igual al \\(20\\%\\). \\[\nH_0: p \\leq 0.20\n\\]\nLa hipótesis alternativa es lo que queremos demostrar y se contrapone a la hipótesis nula. En este caso, queremos ver si la proporción de fumadores es mayor al \\(20\\%\\). \\[\nH_1: p &gt; 0.20\n\\]\n\n\n\n\n\n\n3.7.2 Pasos en un Contraste de Hipótesis\n\nFormular las hipótesis:\n\nDefinir \\(H_0\\) y \\(H_1\\) claramente.\n\nSeleccionar el nivel de significatividad estadística \\((\\alpha)\\):\n\nEl nivel de significatividad estadística es la probabilidad de rechazar \\(H_0\\) cuando es verdadera. Comúnmente, se utilizan \\(\\alpha = 0.05\\), \\(\\alpha = 0.01\\), o \\(\\alpha = 0.10\\).\n\nElegir el estadístico de prueba:\n\nSeleccionar un estadístico que siga una distribución conocida bajo \\(H_0\\) (por ejemplo, la distribución Normal, t de Student o Chi-cuadrado).\n\nCalcular el \\(p-valor\\):\n\nEl \\(p-valor\\) es la probabilidad de observar un valor tan extremo o más extremo que el observado, bajo la suposición de que \\(H_0\\) es verdadera. Después volveremos sobre este valor.\n\nTomar una decisión:\n\nLa regla de decisión de un contraste de hipótesis se basa en la “distancia” entre los datos muestrales y los valores esperados si \\(H_0\\) es cierta. Esta distancia se calcula a partir de un estadístico del contraste y se considera “grande” o no, en base a la distribución del mismo y a la probabilidad de observar realizaciones “más extremas” de dicho estadístico. Para tomar la decisión, comparamos el valor p con \\(\\alpha\\):\nSi \\(p-valor \\leq \\alpha\\), se rechaza \\(H_0\\). Hay suficiente evidencia en la muestra como para rechazar la hipótesis nula. El valor del parámetro establecido en \\(H_0\\) es poco creíble dada la muestra observada.\nSi \\(p-valor &gt; \\alpha\\), no se rechaza \\(H_0\\).\n\n\n\n\n\n\n\n\n¡Importante!\n\n\n\nNo rechazar la hipótesis nula no significa que la hipótesis nula sea cierta. La interpretación es que no existe, en la muestra que hemos observado, suficiente evidencia en contra de la hipótesis nula como para recharzarla.\n\n\nTenemos por tanto que el \\(p-valor\\) es una medida que nos dice cuán probable sería obtener nuestros datos observados si la hipótesis nula fuera verdadera. En otras palabras, mide la evidencia en contra \\(H_0\\). Si el \\(p-valor\\) es pequeño (generalmente menor que \\(0.05\\)), tenemos razones para rechazar \\(H_0\\). Si es grande, no tenemos suficiente evidencia para rechazarla.\n\n\n\n\n\n\nEjemplo Práctico. Contraste de Hipótesis\n\n\n\n\n\nSupongamos que una empresa afirma que el tiempo promedio de espera en su servicio al cliente es de \\(10\\) minutos. Queremos probar esta afirmación con una muestra de \\(30\\) clientes que tienen un tiempo promedio de espera de \\(12\\) minutos y una desviación estándar de \\(3\\) minutos.\nFormulamos las hipótesis: - \\(H_0: \\mu = 10\\) - \\(H_1: \\mu \\neq 10\\)\nSeleccionamos el nivel de significancia estadística deseado: \\(\\alpha = 0.05\\).\nElegimos el estadístico de prueba: usamos una prueba \\(t\\) (dado que la muestra es pequeña y no conocemos la desviación estándar poblacional) y calculamos su valor:\n\\[\n   t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}} = \\frac{12 - 10}{3 / \\sqrt{30}} \\approx \\frac{2}{0.5477} \\approx 3.65\n\\]\nA continuación, obtenemos el \\(p-valor\\) correspondiente al estadístico para la distribución \\(t\\) de Studento con \\(29\\) grados de libertad. Para \\(t = 3.65\\), el \\(p-valor\\) es menor que \\(0.001\\).\nDado que \\(p &lt; 0.05\\), rechazamos la hipótesis nula \\(H_0\\). Podemos afirmar que los resultados muestrales son “demasiado extraños” para aceptar la hipótesis nula.\n\n\n\n\n\n3.7.3 Errores tipo I y tipo II. Potencia\nComo hemos visto, una vez especificadas las hipótesis nula y alternativa y recogida la información muestral, se toma una decisión sobre la hipótesis nula (rechazar o no rechazar \\(H_0\\)). Sin embargo, existe la posibilidad de llegar a una conclusión equivocada, porque solo se dispone de una muestra aleatoria y no se puede tener la certeza de que \\(H_0\\) sea correcta o no.\nEn la Inferencia Estadística, cuando realizamos un contraste de hipótesis, hay dos tipos de errores que pueden ocurrir: el error de tipo I o \\(\\alpha\\) y el error de tipo II o \\(\\beta\\). Entender estos errores es fundamental para interpretar correctamente los resultados de cualquier prueba estadística. El balance entre \\(\\alpha\\) y \\(\\beta\\), así como el tamaño de la muestra, juegan un papel importante en la fiabilidad de los resultados obtenidos.\n\n3.7.3.1 Error de Tipo I \\((\\alpha)\\)\nEl error de tipo I ocurre cuando rechazamos la hipótesis nula \\(H_0\\) siendo esta verdadera. En otras palabras, concluimos que hay un efecto o una diferencia cuando, en realidad, no la hay. El nivel de significancia \\(\\alpha\\) es la probabilidad de cometer un error de tipo I.\n\\[\n\\alpha=P(\\text{rechazar } H_0 \\text{ | } H_0 \\text{ es correcta})\n\\]\nRecuerda que establecemos de antemano esta valor, comúnmente \\(0.05\\), \\(0.01\\) o \\(0.10\\). Si el \\(p-valor\\) de nuestra prueba es menor o igual a \\(\\alpha\\), rechazamos \\(H_0\\).\nAsí, por ejemplo, Si \\(\\alpha = 0.05\\), esto significa que estamos dispuestos a aceptar un \\(5\\%\\) de probabilidad de rechazar \\(H_0\\) cuando es verdadera.\n\n\n\n\n\n\nAtención\n\n\n\n¿Qué relación existe entre el error de tipo I y el \\(\\%\\) de un IC?\n\n\n\n\n3.7.3.2 Error de Tipo II \\((\\beta)\\)\nEl error de tipo II ocurre cuando no rechazamos la hipótesis nula \\(H_0\\) siendo esta falsa. En otras palabras, concluimos que no hay un efecto o una diferencia cuando, en realidad, sí la hay.\n\\[\n\\beta=P(\\text{No rechazar } H_0 \\text{ | } H_0 \\text{ es incorrecta})\n\\]\nPotencia del test: La potencia de una prueba estadística es la probabilidad de rechazar \\(H_0\\) cuando \\(H_0\\) es falsa. Se calcula como \\(1 - \\beta\\). Una alta potencia es deseable ya que indica una mayor probabilidad de detectar un efecto o diferencia cuando realmente existe.\n\\[\nPotencia=1-\\beta=P(\\text{Rechazar } H_0 \\text{ | } H_1 \\text{ es correcta})\n\\]\nAsí, por ejemplo, si \\(\\beta = 0.20\\), esto significa que hay un \\(20\\%\\) de probabilidad de no rechazar \\(H_0\\) cuando es falsa. La potencia de la prueba sería \\(0.80\\) (o del \\(80\\%\\)). La probabilidad de detectar un efecto cuando relamente existe es del \\(80\\%\\)\n\n\n\n\n\n\nEjemplo Práctico. Errores Tipo I y II\n\n\n\n\n\nSupongamos que estamos evaluando la efectividad de un nuevo medicamento.\n\nHipótesis Nula \\(H_0\\): El medicamento no tiene efecto \\((\\mu = 0)\\).\nHipótesis Alternativa \\(H_1\\): El medicamento tiene un efecto \\((\\mu \\neq 0)\\).\n\n\nError de Tipo I: Si el medicamento no tiene ningún efecto pero el estudio concluye que sí lo tiene, hemos cometido un error de tipo I. Esto podría llevar a la aprobación y uso de un medicamento ineficaz.\nError de Tipo II: Si el medicamento tiene un efecto, pero el estudio concluye que no lo tiene, hemos cometido un error de tipo II. Esto podría llevar a la no aprobación de un medicamento potencialmente beneficioso.\n\n\n\n\n\n\n3.7.3.3 Relación entre errores de tipo I y II\n\nInversamente proporcionales: Reducir \\(\\alpha\\) (haciendo la prueba más conservadora y menos propensa a rechazar \\(H_0\\) generalmente aumenta \\(\\beta\\) (haciendo la prueba más propensa a no detectar un efecto cuando realmente existe), y viceversa. Fíjate que los errores de Tipo I y de Tipo II no se pueden comenter simultáneamente:\n\nEl error de Tipo I solo puede darse si \\(H_0\\) es correcta.\nEl error de Tipo II solo puede darse si \\(H_0\\) es incorrecta.\n\nTamaño de la muestra: Aumentar el tamaño de la muestra puede reducir ambos tipos de errores, incrementando la precisión de la prueba.\n\nLa siguiente tabla refleja la relación entre los dos tipos de errores en relación con la decisión del contraste y la verdadera situación en la población:\n\n\n\n\n\n\n\n\n\nVerdadera situación\n\n\n\n\n\nDecisión\n\\(H_0\\) correcta\n\\(H_0\\) incorrecta\n\n\nNo rechazar \\(H_0\\)\nSin error (\\(1-\\alpha\\))\nError de Tipo II (\\(\\beta\\))\n\n\nRechazar \\(H_0\\)\nError de Tipo I (\\(\\alpha\\))\nSin error (\\(1-\\beta\\)=potencia)\n\n\n\nEs importante notar que, si todo lo demás no cambia, entonces la potencia del contraste disminuye cuando:\n\nLa diferencia entre el valor supuesto para el parámetro y el valor real disminuye.\nLa variabilidad de la población aumenta.\nEl tamaño muestra disminuye.\n\n\n\n\n\n\n\nEjemplo Práctico. Errores Tipo I y II\n\n\n\n\n\nLos errores de tipo I y tipo II son inversamente proporcionales. Al disminuir la probabilidad de cometer un error de tipo I (haciendo la prueba más conservadora), aumentamos la probabilidad de cometer un error de tipo II (haciendo la prueba menos sensible), y viceversa.\nSupongamos que estamos evaluando la efectividad de un nuevo medicamento para reducir la presión arterial. Queremos probar la siguiente hipótesis:\n\nHipótesis Nula \\((H_0)\\): El nuevo medicamento no reduce la presión arterial (\\(\\mu = 0\\)).\nHipótesis Alternativa \\((H_1)\\): El nuevo medicamento reduce la presión arterial \\((\\mu \\neq 0)\\).\n\nInicialmente fijamos el nivel de significancia (\\(\\alpha\\)) en \\(0.05\\). Consideramos un tamaño de muestra inicial de \\(100\\) pacientes.\nEn este caso el error de Tipo I, significa que estamos dispuestos a aceptar un \\(5\\%\\) de probabilidad de concluir que el medicamento es efectivo cuando en realidad no lo es.\nLa probabilidad de error de Tipo II (\\(\\beta\\)) y, por tanto, la pontencia del contraste depende de varios factores, incluidos el tamaño del efecto, el tamaño de la muestra y el nivel de significancia.\nCaso 1: (\\(\\alpha = 0.05\\))\nEn este caso somos bastante conservadores con el riesgo de falso positivo. Supongamos que el poder estadístico de la prueba con \\((\\alpha = 0.05)\\) y una muestra de \\(100\\) es \\(0.80\\), lo que significa que (\\(\\beta = 0.20\\)).\nCaso 2: (\\(\\alpha = 0.01\\))\nAhora somos más conservadores, reduciendo la probabilidad de cometer un error de tipo I. Al ser más conservadores y reducir (\\(\\alpha\\)), la prueba se vuelve menos sensible a detectar el efecto real. Esto aumenta la probabilidad de cometer un error de tipo II, por ejemplo, supongamos que (\\(\\beta\\)) aumenta a \\(0.30\\). Se reduce la potencia del contraste.\nCaso 3: (\\(\\alpha = 0.10\\))\nAhora somos menos conservadores, aumentando la probabilidad de cometer un error de tipo I. Al ser menos conservadores y aumentar (\\(\\alpha\\)), la prueba se vuelve más sensible a detectar el efecto real. Esto disminuye la probabilidad de cometer un error de tipo II, por ejemplo, supongamos que (\\(\\beta\\)) disminuye a 0.10 y, por tanto, aumenta la potencia del contraste.\nEn resumen:\n\n\n\n\\(\\alpha\\)\n\\(\\beta\\)\nPotencia (\\(1-\\beta\\))\n\n\n\n\n0.05\n0.20\n0.80\n\n\n0.01\n0.30\n0.70\n\n\n0.10\n0.10\n0.90\n\n\n\ny como conclusión\n\nCaso 1 (\\(\\alpha = 0.05\\)): Balance estándar entre el riesgo de falso positivo y falso negativo.\nCaso 2 (\\(\\alpha = 0.01\\)): Reducimos el riesgo de falso positivo (\\(\\alpha\\)), pero aumentamos el riesgo de falso negativo (()).\nCaso 3 (\\(\\alpha = 0.10\\)): Aumentamos el riesgo de falso positivo (\\(\\alpha\\)), pero reducimos el riesgo de falso negativo (\\(\\beta\\)).\n\n\n\n\n\n\n\n3.7.4 Contraste para la media de una población normal con varianza conocida\nEl contraste para la media de una población normal con varianza conocida es un procedimiento estadístico utilizado para determinar si la media de una población difiere de un valor específico (hipótesis nula). Sin embargo, como hemos indicado anteriormente, es poco realista pensar que conocemos la varianza de una variable aleatoria en la población.\nEl parámetro de estudio es la media de la variable aleatoria:\n\\[ X \\sim N(\\mu,\\sigma^2) \\]\nEn primer lugar fijamos las hipótesis:\n\n\\(H_0\\): \\(\\mu = \\mu_0\\) (La media de la población es igual a \\(\\mu_0\\))\n\nTenemos varias opciones para la hipótesis alternativa:\n\n\\(H_1\\): \\(\\mu \\neq \\mu_0\\) (Contraste bilateral)\n\\(H_1\\): \\(\\mu &gt; \\mu_0\\) (Contraste unilateral derecho)\n\\(H_1\\): \\(\\mu &lt; \\mu_0\\) (Contraste unilateral izquierdo)\n\nDebemos fijar el nivel de el nivel de significación (\\(\\alpha\\)). Recordemos, \\(\\alpha\\) es la probabilidad de rechazar la hipótesis nula cuando esta es verdadera.\nEl estadístico de prueba se calcula utilizando la distribución Normal estándar (\\(Z\\)), dado que la varianza (\\(\\sigma^2\\)) es conocida. La fórmula para el estadístico de prueba es:\n\\[ Z = \\frac{\\bar{\\mathbf{X}} - \\mu_0}{\\sigma/\\sqrt{n}} \\sim N(0,1) \\]\nDonde \\(\\bar{\\mathbf{X}}\\) es la media poblacional. Calculamos el valor observado del estadístico: \\[ z = \\frac{\\bar{\\mathbf{x}} - \\mu_0}{\\sigma/\\sqrt{n}} \\] donde \\(\\bar{\\mathbf{x}}\\) es la media muestral.\nDespués calculamos el \\(p-valor\\) como sigue:\n\nContraste bilateral: \\(p-valor=P(|Z|\\geq z)\\)\nContraste unilateral derecho: \\(p-valor=P(Z\\geq z)\\)\nContraste unilateral izquierdo: \\(p-valor=P(Z\\leq z)\\)\n\nSi esta probabilidad es menor o igual que el valor de referencia \\(\\alpha\\), entonces rechazamos la hipótesis nula en favor de la alternativa.\nOtra forma práctica de plantear el contraste de hipótesis es determinando el rechazo de \\(H_0\\). Para ello, debemos comparar el valor del estadístico \\(Z\\) con los valores críticos de la distribución Normal estándar.\n\nPara un contraste bilateral (dos colas):\n\nRechaza \\(H_0\\) si \\(|z| &gt; z_{\\alpha/2}\\).\n\nPara un contraste unilateral derecho (una cola):\n\nRechaza \\(H_0\\) si \\(z &gt; z_{\\alpha}\\).\n\nPara un contraste unilateral izquierdo (una cola):\n\nRechaza \\(H_0\\) si \\(z &lt; -z_{\\alpha}\\).\n\n\nAquí, \\(z_{\\alpha}\\) y \\(z_{\\alpha/2}\\) son los valores críticos de la distribución Normal estándar correspondientes al nivel de significación \\(\\alpha\\).\nEs decir, decidimos si rechazamos o no la hipótesis nula del siguiente modo:\n\nSi el valor del estadístico de prueba está en la región crítica, rechaza (H_0).\nSi el valor del estadístico de prueba no está en la región crítica, no rechaces (H_0).\n\n\n\n\n\n\n\nEjemplo Práctico. Contraste de hipótesis media normal, varianza conocida\n\n\n\n\n\nSupón que queremos probar si la edad media de los profesores de la URJC es igual a \\(50\\) años con una desviación estándar conocida de \\(10\\) años, y tienes una muestra de \\(36\\) observaciones con una media muestral de \\(52\\).\nFormulamos las hipótesis:\n\n\\(H_0\\): \\(\\mu = 50\\)\n\\(H_1\\): \\(\\mu \\neq 50\\)\n\nFijamos el nivel de significación: \\(\\alpha = 0.05\\).\nCalculamos el estadístico de prueba: \\[ z = \\frac{52 - 50}{10/\\sqrt{36}}  \\approx 1.20 \\]\nPara \\(\\alpha = 0.05\\) en un contraste bilateral, los valores críticos son \\(z_{0.05/2}=\\pm 1.96\\). Como \\(|1.20| &lt; 1.96\\), no tenemos evidencia en la muestra como para rechazar \\(H_0\\).\nSi hubiéramos calculado el \\(p-valor\\): \\[p-valor=P(|Z| \\geq 1.20)=2*P(Z \\geq 1.20)=2*0.115\\approx0.23\\] Como el \\(p-valor\\) es mayor que el nivel de significatividad estadística, no podemos rechazar la hipótesis nula en favor de la alternativa.\n\n\n\n\n\n3.7.5 Contraste para la media de una población normal con varianza desconocida\nEl contraste de hipótesis para la media de una población normal con varianza desconocida es similar al caso con varianza conocida, pero utilizamos la distribución \\(t\\) de Student en lugar de la distribución Normal estándar.\nEn este caso, el cuando la varianza poblacional es desconocida, se utiliza la desviación estándar muestral (\\(s\\)) y el estadístico de prueba se basa en la distribución \\(t\\) de Student con (\\(n - 1\\)) grados de libertad. La fórmula es:\n\\[ T=\\frac{\\bar{X} - \\mu_0}{s/\\sqrt{n}} \\]\nDonde \\(\\bar{\\mathbf{X}}\\) es la media poblacional. Calculamos el valor observado del estadístico: \\[ t = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}  \\]\nEl p-valor es la probabilidad de obtener un valor del estadístico de prueba al menos tan extremo como el observado, bajo la suposición de que \\(H_0\\) es verdadera. Dependiendo del tipo de contraste, el p-valor se calcula de diferentes formas:\n\nPara un contraste bilateral:\n\np-valor = \\(2 \\cdot P(T_{n-1} &gt; |t|)\\)\n\nPara un contraste unilateral derecho:\n\np-valor = \\(P(T_{n-1} &gt; t)\\)\n\nPara un contraste unilateral izquierdo:\n\np-valor = \\(P(T_{n-1} &lt; t)\\)\n\n\nAquí, \\(T_{n-1}\\) es una variable aleatoria con una distribución \\(t\\) de Student con \\(n - 1\\) grados de libertad.\nLa decisión asociada al contraste es:\n\nSi el \\(p-valor \\leq \\alpha\\), rechazar \\(H_0\\).\nSi el \\(p-valor &gt; \\alpha\\), no rechazar \\(H_0\\).\n\n\n\n\n\n\n\nEjemplo Práctico. Contraste de hipótesis media normal, varianza desconocida\n\n\n\n\n\nSupón que una empresa quiere verificar si el tiempo promedio de entrega de sus pedidos es mayor de \\(30\\) minutos. Toma una muestra aleatoria de \\(16\\) entregas y encuentra que el tiempo promedio de entrega es de \\(32\\) minutos con una desviación estándar muestral de \\(4\\) minutos. Realiza un contraste de hipótesis con un nivel de significación del \\(0.05\\) para ver si el tiempo promedio de entrega es mayor de \\(30\\) minutos.\nEn primer lugar establecemos las hipótesis: - \\(H_0\\): $= 30 (El tiempo promedio de entrega es de 30 minutos) - \\(H_1\\): $&gt; 30 (El tiempo promedio de entrega es mayor de 30 minutos)\nCalculamos el estadístico de Prueba \\[ t = \\frac{\\bar{X} - \\mu_0}{s/\\sqrt{n}} = \\frac{32 - 30}{4/\\sqrt{16}} = \\frac{2}{1} = 2.00 \\]\nEstamos ante un contraste unilateral derecho con \\(n - 1 = 16 - 1 = 15\\) grados de libertad. El \\(p-valor\\) es: \\[P(T_{15} &gt; 2.00) \\approx 0.031\\]\nComo el \\(p-valor\\) es menor que el grado de significatividad estadística \\(0.05\\), entonces podemos rechazar la hipótesis nula en favor de la alternativa. Hay suficiente evidencia para rechazar la hipótesis nula y concluir que el tiempo promedio de entrega es mayor de \\(30\\) minutos con un nivel de significación del \\(5\\%\\).\n\n\n\n\n\n3.7.6 Contraste de hipótesis para la igualdad de medias de dos muestras independientes\nCuando se desea comparar las medias de dos muestras independientes asumiendo que los datos siguen una distribución normal, se puede usar el contraste de hipótesis paramétrico conocido como la prueba \\(t\\) de Student para muestras independientes. Este método es robusto y se basa en suposiciones claras acerca de la normalidad de las distribuciones subyacentes.\nSuposiciones:\n\nLas dos muestras son independientes.\nLos datos de cada muestra se distribuyen normalmente. Si las muestras son suficientemente grandes, se puede invocar el Teorema Central del Límite, que establece que la distribución de la media muestral se aproxima a una distribución normal independientemente de la forma de la distribución original.\nLas varianzas poblacionales son desconocidas, pero se pueden asumir iguales para una versión específica del test t (si esta suposición es razonable).\n\nSupongamos m.a.s. independientes con medias, desviaciones típicas y tamaño muestral igual a: \\(\\bar{\\mathbf{x}}_1\\),\\(\\bar{\\mathbf{x}}_2\\),\\(s_1^2\\),\\(s_2^2\\),\\(n_1\\),y \\(n_2\\), respectivamente.\nFormulamos las hipótesis:\n\nHipótesis nula (\\(H_0\\)): Las medias de las dos poblaciones son iguales (\\(\\mu_1=\\mu_2\\)).\nHipótesis alternativa (\\(H_1\\)): Las medias de las dos poblaciones son diferentes (\\(\\mu_1 \\neq \\mu_2\\)).\n\nConsideramos un nivel de significancia estadística \\(\\alpha\\). Típicamente \\(\\alpha=0.05\\).\nCalculamos el estadístico muestras, en este caso: \\[\nt = \\frac{\\bar{\\mathbf{x}}_1-\\bar{\\mathbf{x}}_2 - \\mu_1 - \\mu_2}{SE}\n\\] donde: \\[\nSE = \\sqrt{S^2_p \\left( \\frac{1}{n₁} + \\frac{1}{n₂} \\right)}\n\\] siendo \\(S_p\\) la desviación típica combinada: \\[\nS^2_p= \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}\n\\]\nPara un nivel de significancia \\(\\alpha= 0.05\\) y grados de libertad \\(df = n_1+n_2-2\\), buscamos el valor crítico \\(t\\) para la distribución \\(t\\) de Student para una prueba de dos colas.\nComparamos el valor del estadístico \\(t\\) calculado con el valor crítico:\n\nSi \\(|t| &gt; T_{df,1-\\alpha/2}\\), rechazamos \\(H_0\\).\nSi \\(|t| \\leq T_{df,1-\\alpha/2}\\), no rechazamos \\(H_0\\).\n\n\n\n\n\n\n\nEjemplo Práctico. Contraste de hipótesis igualdad de medias\n\n\n\n\n\nSupongamos que un investigador quiere comparar la efectividad de dos métodos de enseñanza de matemáticas. Se seleccionan dos grupos de estudiantes al azar, uno para cada método. Después de un semestre, se mide el puntaje de un examen final de matemáticas.\nDatos:\n\nGrupo A (Método 1):\n\nTamaño de la muestra (\\(n_1\\)) = 12\nPuntajes: 85, 78, 92, 88, 75, 84, 90, 91, 83, 79, 87, 86\n\nGrupo B (Método 2):\n\nTamaño de la muestra (\\(n_2\\)) = 10\nPuntajes: 82, 77, 85, 80, 79, 81, 83, 78, 82, 76\n\n\nLas hipótesis del contraste son:\n\nHipótesis nula (\\(H_0\\)): Las medias de las dos poblaciones son iguales (\\(\\mu_1=\\mu_2\\)).\nHipótesis alternativa (\\(H_1\\)): Las medias de las dos poblaciones son diferentes (\\(\\mu_1 \\neq \\mu_2\\)).\n\nConsideramos un nivel de significancia \\(\\alpha=0.05\\).\nPara el Grupo A, se tiene:\n\nTamaño de la muestra (\\(n_1= 12\\))\nMedia (\\(\\bar{\\mathbf{x}}_1 = 84.83\\))\nDesviación estándar (\\(s_1=5.33\\))\n\nPara el Grupo B:\n\nTamaño de la muestra (\\(n_2= 10\\))\nMedia (\\(\\bar{\\mathbf{x}}_2 = 80.3\\))\nDesviación estándar (\\(s_2=2.83\\))\n\nUsaremos la prueba t para muestras independientes. Calculamos la varianza combinada: \\[\nS^2_p=  \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}=\\frac{(12-1)5.33^2 + (10-1)2.83^2}{12+10-2} \\approx 19.288\n\\] El erro estándar combinados: \\[\nSE = \\sqrt{S^2_p \\left( \\frac{1}{n₁} + \\frac{1}{n₂} \\right)}= \\sqrt{19.288 \\left( \\frac{1}{12} + \\frac{1}{10} \\right)}\\approx 1.88\n\\] Para calcular el estadístico \\(t\\): \\[\nt = \\frac{\\bar{\\mathbf{x}}_1-\\bar{\\mathbf{x}}_2}{SE}=\\frac{84.83-80.3}{1.88}\\approx 2.41\n\\]\nPara un nivel de significancia \\(\\alpha= 0.05\\) y grados de libertad \\(df = n_1+n_2- 2 = 20\\), el valor crítico t de t de Student para una prueba de dos colas es aproximadamente \\(\\pm 2.086\\).\nEn nuestro caso, \\(|t| = 2.41\\), que es mayor que \\(2.086\\) y por tanto, eechazamos la hipótesis nula (\\(H_0\\)). Esto indica que hay evidencia suficiente para afirmar que existe una diferencia significativa en los puntajes de los exámenes de matemáticas entre los dos grupos de estudiantes.\n\n\n\nEn el caso en el que no se pueda asumir que las varianzas son iguales, el test cambia ligeramente. Es decir, cuando se quieren comparar las medias de dos muestras independientes con varianzas distintas, el estadístico del contraste bajo la hipótesis nula es\n\\[\nt = \\frac{\\bar{\\mathbf{x}}_1-\\bar{\\mathbf{x}}_2 - \\mu_1 - \\mu_2}{\\sqrt{\\frac{s_{1}^{2}}{n_1}+\\frac{s^{2}_2}{n_2}}} \\sim t_{q, \\alpha/2}\n\\] siendo \\[q = \\frac{\\left(\\frac{s_{1}^{2}}{n_1} + \\frac{s_{2}^{2}}{n_2}\\right)^{2}}{\\frac{\\frac{s_{1}^{2}}{n_1}}{n_1 -1} + \\frac{\\frac{s_{2}^{2}}{n_2}}{n_2 -1}} \\] que se calculan mediante la aproximación de Welch-Satterthwaite.\n\n\n3.7.7 Contraste de hipótesis para la diferencia de proporciones\nEl contraste de hipótesis para la diferencia de proporciones se utiliza para determinar si hay una diferencia significativa entre las proporciones de éxito en dos grupos independientes. Supongamos dos variables aleatorias \\(X\\) e \\(Y\\) que siguen una distribución binomial de parámetros \\(p_1\\) y \\(p_2\\) respectivamente.\nFormulamos las hipótesis:\n\nHipótesis nula (\\(H_0\\)): Las proporciones de las dos poblaciones son iguales (\\(p_1=p_2\\)).\nHipótesis alternativa (\\(H_1\\)): Las proporciones de las dos poblaciones son diferentes (\\(p_1 \\neq p_2\\)).\n\nConsideremos dos m.a.s. de tamaño \\(n_1\\) y \\(n_2\\), siendo \\(\\mathbf{x}\\) y \\(\\mathbf{y}\\) el número de observaciones que cumplen un criterio, de modo que: \\[\n\\hat{p}_1=\\frac{\\mathbf{x}}{n_1}, \\hat{p}_2=\\frac{\\mathbf{y}}{n_2}\n\\] son los estimadores de máxima verosimilitud de \\(p_1\\) y \\(p_2\\) respectivamente.\nConsideramos un nivel de significancia estadística \\(\\alpha\\). Típicamente \\(\\alpha=0.05\\).\nCalculamos el estadístico muestral, en este caso: \\[\nZ = \\frac{\\hat{p}_1-\\hat{p_2}}{SE}\n\\]\ndónde \\[\nSE=\\sqrt{\\hat{p}(1-\\hat{p})}\\left ( \\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}\\right )\n\\] Donde \\(\\hat{p}=\\frac{\\mathbf{x}+\\mathbf{y}}{n_1+n_2}\\).\nPara valores grandes de \\(n_1\\) y \\(n_2\\), la distribución de \\(Z\\) es Normal de media \\(0\\) y desviación típica \\(1\\).\nPara un nivel de significancia \\(\\alpha= 0.05\\), buscamos el valor crítico \\(Z\\) para la distribución Normal.\nComparamos el valor del estadístico \\(Z\\) calculado con el valor crítico:\n\nSi \\(|Z| &gt; Z_{1-\\alpha/2}\\), rechazamos \\(H_0\\).\nSi \\(|Z| \\leq Z_{1-\\alpha/2}\\), no rechazamos \\(H_0\\).\n\n\n\n\n\n\n\nEjemplo Práctico. Contraste de hipótesis igualdad de proporciones\n\n\n\n\n\nSupongamos que una empresa de marketing quiere evaluar la efectividad de dos campañas publicitarias diferentes (Campaña A y Campaña B) para atraer clientes. La empresa desea saber si hay una diferencia significativa en la proporción de clientes que responden positivamente a cada campaña.\n\nCampaña A:\n\nNúmero de personas que recibieron la campaña: \\(500\\)\nNúmero de personas que respondieron positivamente: \\(75\\)\n\nCampaña B:\n\nNúmero de personas que recibieron la campaña: \\(600\\)\nNúmero de personas que respondieron positivamente: \\(120\\)\n\n\nLas hipótesis son:\n\nHipótesis Nula (\\(H_0\\)): No hay diferencia en la proporción de éxito entre las dos campañas \\((p_1 = p_2)\\).\nHipótesis Alternativa (\\(H_1\\)): Hay una diferencia en la proporción de éxito entre las dos campañas \\((p_1 \\neq p_2)\\).\n\nCalculamos las proporciones muestrales:\n\\[\\hat{p}_1 = \\frac{75}{500} = 0.15, \\hat{p}_2 = \\frac{120}{600} = 0.20\\] calculamos la proporción combinada: \\[  \\hat{p} = \\frac{x_1 + x_2}{n_1 + n_2} = \\frac{75 + 120}{500 + 600} = \\frac{195}{1100} = 0.1773\n\\]\nCalcular el Error Estándar de la diferencia de proporciones \\[  SE = \\sqrt{\\hat{p} (1 - \\hat{p}) \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)} = \\sqrt{0.1733 \\times 0.8227 \\left( \\frac{1}{500} + \\frac{1}{600} \\right)} \\approx 0.0231\n\\]\nA continuación calculamos el estadístico de prueba: \\[z = \\frac{\\hat{p}_1 - \\hat{p}_2}{SE} = \\frac{0.15 - 0.20}{0.0231} \\approx -2.16\\] El \\(p-valor\\) asociado para una prueba bilateral es aproximadamente \\(P(|Z|\\geq 2.16)\\approx 0.031\\).\nDado que el \\(p-valor\\) es menor que el nivel de significancia típico (\\(\\alpha = 0.05\\)), rechazamos la hipótesis nula. Por tanto, hay evidencia suficiente para afirmar que existe una diferencia significativa en las proporciones de éxito entre la Campaña A y la Campaña B.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimación y contraste paramétrico</span>"
    ]
  },
  {
    "objectID": "nopara.html",
    "href": "nopara.html",
    "title": "4  Contraste no paramétrico",
    "section": "",
    "text": "4.1 Introducción a los contrastes no paramétricos\nEn el campo de la inferencia estadística, los contrastes no paramétricos se utilizan para comparar grupos o evaluar hipótesis cuando no se cumplen los supuestos necesarios para aplicar métodos paramétricos tradicionales como los vistos en el Capítulo 3. Los métodos no paramétricos no dependen de la suposición de que los datos sigan una distribución específica, como la Normal, lo que los hace especialmente útiles en situaciones donde los datos presentan sesgos, distribuciones desconocidas o tamaños de muestra pequeños.\nLos contrastes no paramétricos ofrecen una alternativa robusta y flexible para analizar datos en diversas circunstancias. Entre sus ventajas se incluyen la capacidad de manejar datos ordinales que no se ajustan a una escala continua y la resistencia a la influencia de valores atípicos. Esto permite obtener resultados fiables y válidos sin necesidad de transformaciones complicadas de los datos.\nAlgunos de los contrastes no paramétricos más conocidos incluyen la prueba de Mann-Whitney, utilizada para comparar medianas entre dos grupos independientes, y la prueba de Wilcoxon para muestras relacionadas, que evalúa diferencias en medianas para datos apareados. Otros ejemplos son la prueba de Kruskal-Wallis, que extiende el análisis a más de dos grupos independientes, y la prueba de Friedman, que se aplica a diseños con medidas repetidas.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Contraste no paramétrico</span>"
    ]
  },
  {
    "objectID": "nopara.html#prueba-chi-cuadrado-de-independencia",
    "href": "nopara.html#prueba-chi-cuadrado-de-independencia",
    "title": "4  Contraste no paramétrico",
    "section": "4.2 Prueba Chi-cuadrado de Independencia",
    "text": "4.2 Prueba Chi-cuadrado de Independencia\nLa prueba Chi-cuadrado de independencia se utiliza para determinar si hay una asociación significativa entre dos variables categóricas \\(X\\) con categorías \\(X_1,X_2,\\ldots,X_r\\) e \\(Y\\) con categorías \\(Y_1,Y2,\\ldots,Y_c\\). Esta prueba compara las frecuencias observadas en una tabla de contingencia con las frecuencias esperadas bajo la hipótesis de independencia.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(Y_1\\)\n\\(\\ldots\\)\n\\(Y_j\\)\n\\(\\ldots\\)\n\\(Y_c\\)\n\n\n\n\n\n\\(X_1\\)\n\\(n_{1,1}\\)\n\\(\\ldots\\)\n\\(n_{1,j}\\)\n\\(\\ldots\\)\n\\(n_{1,c}\\)\n\\(n_{1.}\\)\n\n\n\\(\\ldots\\)\n\n\n\n\n\n\\(\\ldots\\)\n\n\n\\(X_i\\)\n\\(n_{i,1}\\)\n\\(\\ldots\\)\n\\(n_{i,j}\\)\n\\(\\ldots\\)\n\\(n_{i,c}\\)\n\\(n_{i.}\\)\n\n\n\\(\\ldots\\)\n\n\n\n\n\n\\(\\ldots\\)\n\n\n\\(X_r\\)\n\\(n_{r,1}\\)\n\\(\\ldots\\)\n\\(n_{r,j}\\)\n\\(\\ldots\\)\n\\(n_{r,c}\\)\n\\(n_{r.}\\)\n\n\n\n\\(n_{.1}\\)\n\n\\(n_{.j}\\)\n\n\\(n_{.c}\\)\n\\(n_{..}\\)\n\n\n\nLa hipótesis nula \\(H_0\\) de esta prueba es que no hay asociación entre las variables, esto es, que las variables implicadas son independientes:\n\nHipótesis nula, \\(H_0\\): No hay asociación entre las variables (son independientes).\nHipótesis alternativa, \\(H_1\\): Hay una asociación entre las variables (son dependientes).\n\nLas frecuencias observadas Las frecuencias esperadas se calculan como sigue: \\[\n   E_{ij} = \\frac{(n_{i.} \\times n_{.j})}{N}\n   \\] donde \\(E_{ij}\\) es la frecuencia esperada en la celda \\((i,j)\\), \\(n_{i.}\\) es el total de la fila (\\(i\\)), (\\(n_{.j}\\)) es el total de la columna (\\(j\\)), y \\(n_{..}\\) es el total general.\nAhora, comparamos las frecuencias esperadas con las frecuencias observadas, definiendo con ellos el estadístico chi-cuadrado: \\[\n\\chi^2 = \\sum \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n\\]\ndonde (\\(O_{ij}\\)) es la frecuencia observada en la celda \\((i,j)\\).\nBajo la hipótesis nula, el estadístico de la prueba sigue una distribución Chi-cuadrado con \\((r-1)(c-1)\\) grados de libertad, donde (\\(r\\)) es el número de filas y (\\(c\\)) es el número de columnas. Podemos calcular el \\(p-valor\\) como: \\[\np-valor=P(\\chi^2_{(r-1)(c-1)}\\geq\\chi^2)\n\\]\nComo ocurría en los contrastes de hipótesis paramétricos, comparamos el \\(p-valor\\) con el nivel de significancia (\\(\\alpha\\)), generalmente \\(0.05\\). Si (\\(p-valor &lt; \\alpha\\)), se rechaza la hipótesis nula.\n\n\n\n\n\n\nEjemplo práctico. Prueba Chi-cuadrado de independencia.\n\n\n\n\n\nSupongamos que un investigador desea determinar si hay una asociación entre el tipo de dispositivo usado (Laptop, Tablet, Smartphone) y la satisfacción del usuario (Satisfecho, No satisfecho).\nDeterminamos las hipótesis del problema:\n\nHipótesis nula (\\(H_0\\)): No hay asociación entre el tipo de dispositivo y la satisfacción del usuario (son independientes).\nHipótesis alternativa (\\(H_1\\)): Hay una asociación entre el tipo de dispositivo y la satisfacción del usuario (son dependientes).\n\nRecolectamos datos de una muestra de \\(150\\) usuarios y construimos la siguiente tabla de contingencia:\n\n\n\n\nSatisfecho\nNo satisfecho\nTotal\n\n\n\n\nLaptop\n30\n10\n40\n\n\nTablet\n20\n20\n40\n\n\nSmartphone\n50\n20\n70\n\n\nTotal\n100\n50\n150\n\n\n\nVamos a utilizar R para realizar la prueba Chi-cuadrado de independencia.\n\n# Crear la tabla de contingencia\ntabla &lt;- matrix(c(30, 10, 20, 20, 50, 20), nrow = 3, byrow = TRUE)\nrownames(tabla) &lt;- c(\"Laptop\", \"Tablet\", \"Smartphone\")\ncolnames(tabla) &lt;- c(\"Satisfecho\", \"No Satisfecho\")\ntabla &lt;- as.table(tabla)\n\n# Mostrar la tabla de contingencia\nprint(tabla)\n\n           Satisfecho No Satisfecho\nLaptop             30            10\nTablet             20            20\nSmartphone         50            20\n\n# Realizar la prueba chi-cuadrado\nchi2_test &lt;- chisq.test(tabla)\n\n# Mostrar los resultados\nprint(chi2_test)\n\n\n    Pearson's Chi-squared test\n\ndata:  tabla\nX-squared = 6.9643, df = 2, p-value = 0.03074\n\n\nEl resultado de chisq.test en R incluye varios componentes clave:\n\nEstadístico Chi-cuadrado (\\(chi^2\\)): Este es el valor calculado del estadístico chi-cuadrado.\nGrados de libertad (\\(df\\)): Los grados de libertad de la prueba.\nValor p (\\(p-value\\)): Este es el \\(p-valor\\) asociado con el estadístico chi-cuadrado calculado.\n\nEn este ejemplo, el valor p es 0.031, que es menor que \\(0.05\\). Por lo tanto, rechazamos la hipótesis nula y concluimos que hay una asociación significativa entre el tipo de dispositivo y la satisfacción del usuario.\n\n\n\n\n\n\n\n\n\nEjemplo práctico. Prueba Chi-cuadrado en aprendizaje automático.\n\n\n\n\n\nVamos a realizar un ejemplo de la prueba Chi-cuadrado de independencia utilizando la matriz de confusión de un modelo de aprendizaje automático. Tal y como hemos visto, la prueba Chi-cuadrado de independencia se utiliza para determinar si hay una asociación significativa entre dos variables categóricas.\nSupongamos que tenemos un modelo de clasificación binaria que predice si un correo electrónico es spam o no. Construiremos estos modelos en la asignatura de Aprendizaje Automático I del Grado en Ciencia e Ingeniería de Datos. Tras entrenar y evaluar el modelo, obtenemos la siguiente matriz de confusión:\n\n\n\n\nPredicción: No Spam\nPredicción: Spam\n\n\n\n\nActual: No Spam\n50\n10\n\n\nActual: Spam\n5\n35\n\n\n\nEsta tabla de doble entrada cruza la predicción del modelo de aprendiaje automático con el verdadero valor en la muestra sobre la que dicho modelo ha sido entrenado. El objetivo de todo modelo de aprendizaje automático es conseguir una matriz de confusión diagonal. En ese caso, no hay errores en la predicción. Todo son éxitos y el modelo es perfecto.\nVamos a realizar la prueba Chi-cuadrado de independencia para ver si hay una asociación significativa entre las predicciones del modelo y las etiquetas reales. Si el modelo es útil, dicha asociación ha de existir. Si el modelo no es útil, entonces la prueba debería de decirnos que no podemos rechazar la hipótesis nula de independencia entre las dos variables (Predicción y Actual).\n\n# Cargar el paquete necesario\nlibrary(MASS)\n\n# Crear la matriz de confusión\nmatriz_confusion &lt;- matrix(c(50, 10, 5, 35), nrow = 2, byrow = TRUE)\ncolnames(matriz_confusion) &lt;- c(\"Predicción: No Spam\", \"Predicción: Spam\")\nrownames(matriz_confusion) &lt;- c(\"Actual: No Spam\", \"Actual: Spam\")\n\n# Mostrar la matriz de confusión\nprint(matriz_confusion)\n\n                Predicción: No Spam Predicción: Spam\nActual: No Spam                  50               10\nActual: Spam                      5               35\n\n# Realizar la prueba Chi-cuadrado de independencia\nprueba_chi_cuadrado &lt;- chisq.test(matriz_confusion)\n\n# Mostrar los resultados de la prueba\nprint(prueba_chi_cuadrado)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  matriz_confusion\nX-squared = 45.833, df = 1, p-value = 1.288e-11\n\n\nEl resultado de chisq.test() proporciona el valor de Chi-cuadrado, los grados de libertad y el valor p. En este caso, dado que el \\(p-valor\\) de la prueba es menor que el nivel de significancia (\\(0.05\\)), podemos rechazar la hipótesis nula de independencia y concluir que hay una asociación significativa entre las predicciones del modelo y las etiquetas reales. En otras palabras, el modelo de aprendizaje automático es útil para predecir si un correo electrónico es (o no) spam.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Contraste no paramétrico</span>"
    ]
  },
  {
    "objectID": "nopara.html#prueba-de-chi-cuadrado-de-bondad-de-ajuste",
    "href": "nopara.html#prueba-de-chi-cuadrado-de-bondad-de-ajuste",
    "title": "4  Contraste no paramétrico",
    "section": "4.3 Prueba de Chi-cuadrado de bondad de ajuste",
    "text": "4.3 Prueba de Chi-cuadrado de bondad de ajuste\nEsta prueba se utiliza para determinar si una distribución de frecuencias observadas sigue una distribución teórica esperada.\n\nHipótesis nula (\\(H_0\\)): Las frecuencias observadas siguen la distribución esperada.\nHipótesis alternativa (\\(H_1\\)): Las frecuencias observadas no siguen la distribución esperada.\n\nEl estadístico del contraste, como en el caso anterior se calcula como sigue: \\[ \\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}\\] donde (\\(O_i\\)) son las frecuencias observadas y (\\(E_i\\)) son las frecuencias esperadas según la distribución teórica.\nUtilizando la distribución chi-cuadrado con (\\(k-1\\)) grados de libertad, donde (\\(k\\)) es el número de categorías, podemos calcular el \\(p-valor\\). Comparamos el \\(p-valor\\) con el nivel de significancia. Si (\\(p-valor &lt; \\alpha\\)), se rechaza la hipótesis nula.\n\n\n\n\n\n\nEjemplo Práctico. Prueba Chi-cuadrado de bondad de ajuste.\n\n\n\n\n\nSupongamos que un investigador quiere determinar si los resultados de un dado son uniformemente distribuidos. El dado se lanza \\(60\\) veces y los resultados son los siguientes:\n\n1: 8 veces\n2: 10 veces\n3: 9 veces\n4: 11 veces\n5: 12 veces\n6: 10 veces\n\nQueremos comprobar si estos resultados siguen una distribución uniforme, es decir, cada número tiene la misma probabilidad de 1/6.\nFormulamos las hipótesis:\n\nHipótesis nula (H0): Los resultados del dado siguen una distribución uniforme.\nHipótesis alternativa (H1): Los resultados del dado no siguen una distribución uniforme.\n\nA continuación se recogen los datos:\n\n# Resultados observados\nobserved &lt;- c(8, 10, 9, 11, 12, 10)\n\n# Frecuencias esperadas si el dado es justo (distribución uniforme)\nexpected &lt;- rep(60 / 6, 6)\n\nY se realiza la prueba\n\n# Realizar la prueba chi-cuadrado\nchi2_test &lt;- chisq.test(observed, p = rep(1/6, 6))\n\n# Mostrar los resultados\nprint(chi2_test)\n\n\n    Chi-squared test for given probabilities\n\ndata:  observed\nX-squared = 1, df = 5, p-value = 0.9626\n\n\nEl resultado de chisq.test en R incluye varios componentes clave:\n\nEstadístico Chi-cuadrado  (\\(\\chi^2\\)): Este es el valor calculado del estadístico chi-cuadrado.\nGrados de libertad (\\(df\\)): Los grados de libertad de la prueba, que es \\(n - 1\\) donde \\(n\\) es el número de categorías.\nValor p (p-value): Este es el \\(p-valor\\) asociado con el estadístico chi-cuadrado calculado.\n\nEn este ejemplo, el \\(p-valor\\) es 0.963, que es mucho mayor que \\(0.05\\). Por lo tanto, no rechazamos la hipótesis nula y concluimos que no hay suficiente evidencia para decir que los resultados del dado no siguen una distribución uniforme.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Contraste no paramétrico</span>"
    ]
  },
  {
    "objectID": "nopara.html#pruebas-de-de-homogeneidad",
    "href": "nopara.html#pruebas-de-de-homogeneidad",
    "title": "4  Contraste no paramétrico",
    "section": "4.4 Pruebas de de homogeneidad",
    "text": "4.4 Pruebas de de homogeneidad\nLa prueba no paramétrica de homogeneidad se utiliza para determinar si dos o más muestras independientes provienen de la misma distribución o de distribuciones similares. Estas pruebas son útiles cuando no se cumplen los supuestos necesarios para las pruebas paramétricas, como la normalidad de los datos. Ejemplo de pruebas no paramétricas de Homogeneidad son:\n\nPrueba de Kolmogorov-Smirnov para dos muestras: Compara dos muestras para verificar si provienen de la misma distribución.\nPrueba de Mann-Whitney U (o Wilcoxon Rank-Sum Test): Compara dos muestras independientes para determinar si tienen la misma distribución.\nPrueba de Kruskal-Wallis: Extiende la prueba de Mann-Whitney U a más de dos muestras independientes.\n\n\n4.4.1 Prueba de Kolmogorov-Smirnov para dos muestras\nLa Prueba de Kolmogorov-Smirnov (K-S) para dos muestras es una prueba no paramétrica utilizada para determinar si dos muestras independientes provienen de la misma distribución. A diferencia de otras pruebas que se centran en comparar medias o varianzas, la prueba K-S compara las distribuciones acumuladas de dos muestras.\nLas hipótesis de la prueba son:\n\nHipótesis nula (\\(H_0\\)): Las dos muestras provienen de la misma distribución.\nHipótesis alternativa (\\(H_1\\)): Las dos muestras provienen de distribuciones diferentes.\n\nPara cada muestra, se construyen las funciones de distribución empírica (EDF). La función de distribución empírica \\(F_n(x)\\) es una función escalonada que aumenta en \\(\\frac{1}{n}\\) en cada punto de datos, donde \\(n\\) es el tamaño de la muestra. Para más detalles desplegar aquí:\n\n\n\n\n\n\nFunción de Distribución Empírica\n\n\n\n\n\nLa función de distribución empírica (EDF, por sus siglas en inglés) es una función de distribución de probabilidad utilizada para estimar la distribución subyacente de un conjunto de datos observados. Es una herramienta no paramétrica que proporciona una estimación de la función de distribución acumulada de una muestra de datos.\nDada una muestra de datos \\((X_1, X_2, \\ldots, X_n)\\), la función de distribución empírica \\(F_n(x)\\) se define como: \\[\nF_n(x) = \\frac{1}{n} \\sum_{i=1}^{n} I(X_i \\leq x)\n\\] donde \\(I(X_i \\leq x)\\) es una función indicadora que toma el valor \\(1\\) si \\(X_i \\leq x\\) y \\(0\\) en caso contrario.\nEn otras palabras, \\(F_n(x)\\) es la proporción de valores en la muestra que son menores o iguales a \\(x\\).\nPropiedades de la Función de Distribución Empírica\n\nEscalonada: La EDF es una función escalonada que incrementa en pasos de \\(1/n\\) en cada punto de datos.\nNo decreciente: La EDF nunca disminuye a medida que \\(x\\) aumenta.\nLímites: La EDF varía entre \\(0\\) y \\(1\\). Específicamente, \\(F_n(x) = 0\\) para \\(x\\) menor que el valor mínimo de la muestra y \\(F_n(x) = 1\\) para \\(x\\) mayor que el valor máximo de la muestra.\n\n\n\n\nCalculamos el Estadístico \\(D\\) de la prueba K-S es la máxima diferencia absoluta entre las dos funciones de distribución empírica: \\[\n     D = \\sup_x |F_{n_1}(x) - F_{n_2}(x)|\n\\]\nDonde, \\(F_{n_1}(x)\\) y \\(F_{n_2}(x)\\) son las funciones de distribución empírica de las dos muestras.\nEl \\(p-valor\\) se calcula para determinar la significancia de la diferencia observada. Se utiliza la distribución asintótica del estadístico D para calcular el valor p. El valor p se determina utilizando la distribución del estadístico \\(D\\) bajo la hipótesis nula de que ambas muestras provienen de la misma distribución. El cálculo exacto del \\(p-valor\\) para la prueba de K-S no es trivial y generalmente se realiza mediante métodos numéricos o tablas pre-calculadas. Sin embargo, se puede aproximar utilizando la distribución asintótica del estadístico \\(D\\).\nPara muestras grandes, el valor p se puede aproximar usando la fórmula: \\[\np \\approx Q_{KS}(\\sqrt{n} D)\n\\] donde:\n\n\\(n = \\frac{n_1 \\cdot n_2}{n_1 + n_2}\\) es el número efectivo de muestras.\n\\(D\\) es el valor del estadístico K-S.\n\\(Q_{KS}\\) es una función que representa la cola superior de la distribución de Kolmogorov-Smirnov.\n\nLa función \\(Q_{KS}\\) para grandes valores de \\(n\\) se puede aproximar usando la siguiente fórmula: \\[\nQ_{KS}(\\lambda) = 2 \\sum_{k=1}^{\\infty} (-1)^{k-1} e^{-2k^2 \\lambda^2}\n\\]\ndonde \\(\\lambda = \\sqrt{n} D\\).\nPara valores prácticos, esta sumatoria converge rápidamente y a menudo solo se necesita calcular unos pocos términos.\nComo en otros contrastes, si el \\(p-valor\\) calculado es menor que un grado de significancia estadística previamente fijado \\(\\alpha\\), entonces rechazamos la hipótesis nula en favor de la alternativa.\n\n\n\n\n\n\nEjemplo práctico. Prueba K-S\n\n\n\n\n\nSupongamos que queremos comparar dos muestras para determinar si provienen de la misma distribución.\n\nMuestra 1: 1, 2, 3, 4, 5\nMuestra 2: 2, 3, 4, 5, 6\n\nFormulamos las hipótesis\n\nH0: Las dos muestras provienen de la misma distribución.\nH1: Las dos muestras provienen de distribuciones diferentes.\n\nCalculamos las funciones de distribución empírica \\(F_{n_1}(x)\\) y \\(F_{n_2}(x)\\) para las dos muestras.\n\n\n\n\\(x\\)\n\\(F_{n_1}(x)\\)\n\\(F_{n_2}(x)\\)\n\n\n\n\n1\n0.2\n0.0\n\n\n2\n0.4\n0.2\n\n\n3\n0.6\n0.4\n\n\n4\n0.8\n0.6\n\n\n5\n1.0\n0.8\n\n\n6\n1.0\n1.0\n\n\n\nCalculamos la máxima diferencia absoluta entre \\(F_{n_1}(x)\\) y \\(F_{n_2}(x)\\): \\[\nD = \\max(|0.2 - 0.0|, |0.4 - 0.2|, |0.6 - 0.4|, |0.8 - 0.6|, |1.0 - 0.8|, |1.0 - 1.0|) = 0.2\n\\]\nEl valor p se determina utilizando la distribución asintótica del estadístico \\(D\\). Esto generalmente se hace usando tablas de referencia o software estadístico.\nImplementación en R\nPodemos realizar esta prueba en R utilizando la función ks.test:\n\n# Datos de las dos muestras\nmuestra1 &lt;- c(1, 2, 3, 4, 5)\nmuestra2 &lt;- c(2, 3, 4, 5, 6)\n\n# Realizar la prueba de Kolmogorov-Smirnov\nks_test &lt;- ks.test(muestra1, muestra2)\n\n# Mostrar los resultados\nprint(ks_test)\n\n\n    Exact two-sample Kolmogorov-Smirnov test\n\ndata:  muestra1 and muestra2\nD = 0.2, p-value = 1\nalternative hypothesis: two-sided\n\n\nEl resultado de ks.test incluye varios componentes clave:\n\nEstadístico D: Este es el valor calculado del estadístico K-S.\nValor p (p-value): Este es el valor p asociado con el estadístico calculado.\nDescripción de las muestras: Indica las muestras comparadas.\n\nEn este ejemplo, el valor p es 1, que es mayor que \\(0.05\\). Por lo tanto, no rechazamos la hipótesis nula y concluimos que no hay suficiente evidencia para decir que las dos muestras provienen de distribuciones diferentes.\n\n\n\n\n\n4.4.2 Prueba de Mann-Whitney U\nLa prueba de Mann-Whitney U, también conocida como prueba de Wilcoxon para muestras independientes, es una prueba no paramétrica que se utiliza para comparar dos muestras independientes para determinar si provienen de la misma distribución. Es una alternativa a la prueba t de Student cuando no se cumplen los supuestos de normalidad. En lugar de trabajar con los valores originales, la prueba utiliza los rangos de los datos.\nLas hipótesis que vamos a contrastar son:\n\nHipótesis Nula (\\(H_0\\)): Las dos muestras provienen de la misma distribución.\nHipótesis Alternativa (\\(H_1\\)): Las dos muestras no provienen de la misma distribución.\n\nEn primer lugar se combinan los datos de ambas muestras y se ordenan los valores de menor a mayor. A continuación se asignan rangos a estos valores, manejando adecuadamente los empates (otorgando a los valores iguales el rango promedio).\nSe calcula el estadístico del contraste tal y como sigue: \\[  U_1 = n_1n_2+ \\frac{n_1 (n_1 + 1)}{2} -R_1\n\\] \\[\nU_2 =U_1 = n_1n_2+ \\frac{n_2 (n_2 + 1)}{2} -R_2\n\\] donde:\n\n\\(n_1\\) y \\(n_2\\) son los tamaños de las dos muestras.\n\\(R_1\\) y \\(R_2\\) son las sumas de los rangos de las muestras \\(1\\) y \\(2\\), respectivamente.\n\nEl estadístico \\(U\\) final es: \\[\nU=min(U_1,U_2)\n\\]\nEl \\(p-valor\\) se determina comparando el estadístico \\(U\\) con una distribución de referencia para U (tabla de Mann-Whitney) o mediante aproximación normal para grandes tamaños de muestra. En ese caso: \\[\nZ=\\frac{U-E(U)}{\\sqrt{Var(U)}} \\approx N(0,1)\n\\] siendo \\[\nE(U)=n_1n_2+\\frac{n_1(n_1+1)}{2}-E(R_1)\n\\] \\[\nE(U)= n_1n_2+\\frac{n_1(n_1+1)}{2}-\\frac{n_1(n_1+n_2+1)}{2}=\\frac{n_1n_2}{2}\n\\] y \\[\nVar(U)=Var(R_1)=\\frac{n_1n_2(n_1+n_2+1)}{12}\n\\] Y podemos calcular el \\(p-valor\\) como hemos hecho en métodos anteriores: \\[\np-valor=P(Z&gt;z)\n\\] Siendo \\(z\\) el valor del estadístico calculado en las muestras.\nComparamos el \\(p-valor\\) p con el nivel de significancia (\\(\\alpha\\)), generalmente \\(0.05\\). Si (\\(p-valor &lt; \\alpha\\)), se rechaza la hipótesis nula.\n\n\n\n\n\n\nEjemplo práctico. Prueba Mann-Whitney\n\n\n\n\n\nSupongamos que queremos comparar los tiempos de entrega (en días) de dos proveedores distintos:\n\nProveedor A: \\(2, 3, 5, 6, 8\\)\nProveedor B: \\(1, 4, 4, 7, 9\\)\n\nEn primer lugar combinar y ordenamos las muestra\nValores combinados y ordenados: \\(1, 2, 3, 4, 4, 5, 6, 7, 8, 9\\)\nAsignación de rangos:\n\n1: rango 1\n2: rango 2\n3: rango 3\n4: rango 4.5 (promedio de rangos 4 y 5)\n4: rango 4.5\n5: rango 6\n6: rango 7\n7: rango 8\n8: rango 9\n9: rango 10\n\nRangos para cada muestra:\n\nProveedor A: \\(2, 3, 6, 7, 9\\) (sumados dan \\(R_1 = 27\\))\nProveedor B: \\(1, 4.5, 4.5, 8, 10\\) (sumados dan \\(R_2 = 28\\))\n\nPasamos a calcular el estadístico \\(U\\)\nTamaños de muestra:\n\n\\(n_1 = 5\\)\n\\(n_2 = 5\\)\n\nCálculo de \\(U_1\\) y \\(U_2\\):\n\\[\nU_1 = n_1n_2+ \\frac{n_1 (n_1 + 1)}{2} -R_1  =5\\cdot 5+ \\frac{5 \\cdot 6}{2} -27 = 23\n\\]\n\\[\nU_2 =n_1n_2+ \\frac{n_2 (n_2 + 1)}{2} -R_2  =5\\cdot 5+  \\frac{5 \\cdot 6}{2}-28 = 12\n\\]\nEl estadístico U es el menor de \\(U_1\\) y \\(U_2\\): \\(U = 12\\).\nPara determinar el valor p, utilizamos una tabla de referencia para U o una aproximación normal si las muestras son grandes. En este caso, consultamos una tabla para \\(n_1 = 5\\) y \\(n_2 = 5\\). Si no se dispone de la tabla, se puede utilizar software estadístico para calcular el \\(p-valor\\) como sigue:\n\n# Datos de ejemplo\nproveedorA &lt;- c(2, 3, 5, 6, 8)\nproveedorB &lt;- c(1, 4, 4, 7, 9)\n\n# Realizar la prueba de Mann-Whitney U\ntest &lt;- wilcox.test(proveedorA, proveedorB, alternative = \"two.sided\")\n\nWarning in wilcox.test.default(proveedorA, proveedorB, alternative =\n\"two.sided\"): cannot compute exact p-value with ties\n\n# Mostrar los resultados\nprint(test)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  proveedorA and proveedorB\nW = 12, p-value = 1\nalternative hypothesis: true location shift is not equal to 0\n\n\nLos resultados muestra:\n\nEstadístico W: El valor calculado del estadístico U.\nValor p (p-value): El valor p asociado con el estadístico.\nHipótesis alternativa: La prueba es de dos lados, lo que significa que estamos probando si las distribuciones son diferentes en cualquier dirección.\n\nEn este ejemplo, el \\(p-valor\\) es 1, que es mayor que \\(0.05\\), lo que indica que no hay suficiente evidencia para rechazar la hipótesis nula. Por lo tanto, no podemos concluir que las dos muestras provienen de diferentes distribuciones.\n\n\n\n\n\n4.4.3 Prueba de Kruskal-Wallis\nLa prueba de Kruskal-Wallis es una prueba no paramétrica utilizada para comparar tres o más muestras independientes para determinar si provienen de la misma distribución. Es una extensión de la prueba de Mann-Whitney U a más de dos grupos y una alternativa robusta a la ANOVA que veremos en el Capítulo 5 cuando no se cumplen los supuestos de normalidad y homogeneidad de varianzas.\nDado que es una prueba no paramétrica, no requiere que los datos provengan de una distribución normal. Al igual que la prueba de Mann-Whitney, la prueba de Kruskal-Wallis trabaja con los rangos de los datos en lugar de los valores originales.\nLas hipótesis son:\n\nHipótesis Nula (\\(H_0\\)): Todas las muestras provienen de la misma distribución.\nHipótesis Alternativa (\\(H_1\\)): Al menos una de las muestras proviene de una distribución diferente.\n\nA continuación, se combinan los datos de todas las muestras y se ordenan los valores de menor a mayor. Se asignan rangos a estos valores, manejando adecuadamente los empates (otorgando a los valores iguales el rango promedio).\nCalculamos el estadístico H utilizando la siguiente fórmula: \\[\n     H = \\frac{12}{N(N+1)} \\sum_{i=1}^{k} \\frac{R_i^2}{n_i} - 3(N+1)\n\\] donde:\n\n\\(N\\) es el tamaño total de la muestra (suma de los tamaños de las \\(k\\) muestras).\n\\(R_i\\) es la suma de los rangos de la \\(i\\)-ésima muestra.\n\\(n_i\\) es el tamaño de la \\(i\\)-ésima muestra.\n\\(k\\) es el número de muestras.\n\nPara tamaños de muestra relativamente grandes el estadístico \\(H\\) sigue una distriniución \\(\\chi^2\\) con \\(k-1\\) grados de libertdad. El \\(p-valor\\) se determina, por tanto como \\[\np-valor=P(H&gt;h)\n\\] donde h es el valor calculado para un caso particular.\n\n\n\n\n\n\nEjemplo práctico. Prueba Kruskal-Wallis\n\n\n\n\n\nSupongamos que queremos comparar los tiempos de espera (en días) de tres proveedores diferentes:\n\nProveedor A: \\(2, 3, 5, 6, 8\\)\nProveedor B: \\(1, 4, 4, 7, 9\\)\nProveedor C: \\(3, 4, 6, 8, 10\\)\n\nValores combinados y ordenados: \\(1, 2, 3, 3, 4, 4, 4, 5, 6, 6, 7, 8, 8, 9, 10\\)\nAsignación de rangos:\n\n1: rango 1\n2: rango 2\n3: rango 3.5 (promedio de rangos 3 y 4)\n3: rango 3.5\n4: rango 6 (promedio de rangos 5, 6 y 7)\n4: rango 6\n4: rango 6\n5: rango 8\n6: rango 9.5 (promedio de rangos 9 y 10)\n6: rango 9.5\n7: rango 11\n8: rango 12.5 (promedio de rangos 12 y 13)\n8: rango 12.5\n9: rango 14\n10: rango 15\n\nRangos para cada muestra:\n\nProveedor A: \\(2, 3.5, 8, 9.5, 12.5\\) (sumados dan \\(R_1 = 35.5\\))\nProveedor B: \\(1, 6, 6, 11, 14\\) (sumados dan \\(R_2 = 38\\))\nProveedor C: \\(3.5, 6, 9.5, 12.5, 15\\) (sumados dan \\(R_3 = 46.5\\))\n\nCalculamos el Estadístico \\(H\\)\nTamaños de muestra:\n\n\\(n_1 = 5\\)\n\\(n_2 = 5\\)\n\\(n_3 = 5\\)\n\nTamaño total de la muestra: \\(N = 15\\)\n\\[\nH = \\frac{12}{N(N+1)} \\sum_{i=1}^{k} \\frac{R_i^2}{n_i} - 3(N+1)\n\\]\n\\[\nH = \\frac{12}{15 \\cdot 16} \\left( \\frac{35.5^2}{5} + \\frac{38^2}{5} + \\frac{46.5^2}{5} \\right) - 3 \\cdot 16 = 0.665\n\\]\nPara determinar el \\(p-valor\\), utilizamos una tabla de distribución chi-cuadrado con \\(k-1 = 3-1 = 2\\) grados de libertad.\n\n# Datos de ejemplo\nproveedorA &lt;- c(2, 3, 5, 6, 8)\nproveedorB &lt;- c(1, 4, 4, 7, 9)\nproveedorC &lt;- c(3, 4, 6, 8, 10)\n\n# Realizar la prueba de Kruskal-Wallis\ntest &lt;- kruskal.test(list(proveedorA, proveedorB, proveedorC))\n\n# Mostrar los resultados\nprint(test)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  list(proveedorA, proveedorB, proveedorC)\nKruskal-Wallis chi-squared = 0.67342, df = 2, p-value = 0.7141\n\n\nde donde obtenemos\n\nKruskal-Wallis chi-squared: El valor calculado del estadístico \\(H\\).\ndf: Los grados de libertad.\nValor p (p-value): El \\(p-valor\\) asociado con el estadístico \\(H\\).\n\nEn este ejemplo, el \\(p-valor\\) es 0.714, que es mayor que \\(0.05\\), lo que indica que no hay suficiente evidencia para rechazar la hipótesis nula. Por lo tanto, no podemos concluir que las tres muestras provienen de diferentes distribuciones; podrían provenir de la misma distribución.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Contraste no paramétrico</span>"
    ]
  },
  {
    "objectID": "nopara.html#prueba-sobre-muestras-pareadas",
    "href": "nopara.html#prueba-sobre-muestras-pareadas",
    "title": "4  Contraste no paramétrico",
    "section": "4.5 Prueba sobre muestras pareadas",
    "text": "4.5 Prueba sobre muestras pareadas\n\n4.5.1 Prueba del signo\nLa prueba del signo es una prueba no paramétrica utilizada para comparar dos muestras relacionadas o emparejadas. Se emplea cuando se tienen dos conjuntos de datos dependientes y se desea determinar si hay una diferencia significativa en sus medianas. Es particularmente útil cuando los supuestos de normalidad necesarios para pruebas paramétricas como la prueba t para muestras pareadas no se cumplen.\nSupongamos que tenemos dos muestras relacionadas \\(X\\) e \\(Y\\) de tamaño \\(n\\):\nFormular las hipótesis:\n\nHipótesis Nula (\\(H_0\\)): Las medianas de las dos muestras son iguales.\nHipótesis Alternativa (\\(H_1\\)): Las medianas de las dos muestras son diferentes.\n\nCalcular las diferencias: Para cada par \\((X_i, Y_i)\\), calcular la diferencia \\(D_i = X_i - Y_i\\).\nContar los signos: - Contar cuántas diferencias son positivas (\\(D_i &gt; 0\\)). - Contar cuántas diferencias son negativas (\\(D_i &lt; 0\\)). - Ignorar las diferencias que son cero (\\(D_i = 0\\)).\nEstadístico de prueba:\n\nDenotar el número de diferencias positivas por \\(S_+\\).\nDenotar el número de diferencias negativas por \\(S_-\\).\n\nEl estadístico de la prueba del signo es el menor entre \\(S_+\\) y \\(S_-\\):\\[\n   s=min(S_+,S_-)\n   \\]\nDeterminar el valor crítico: Consultar una tabla de la distribución binomial o la tabla de la prueba del signo para obtener el valor crítico correspondiente al nivel de significancia \\(\\alpha\\) y el tamaño de la muestra efectiva \\(n\\) (número de pares no nulos). Podemos calcular el valor crítico como sigue: \\[\np-valor=P(S\\leq s)\n\\] donde \\(S\\) es binomial \\(n\\), \\(p=0.5\\).\nDecisión:\n\nRechazar \\(H_0\\) si el estadístico de la prueba es menor o igual al valor crítico.\nNo rechazar \\(H_0\\) si el estadístico de la prueba es mayor que el valor crítico.\n\n\n\n\n\n\n\nEjemplo práctico. Prueba del signo\n\n\n\n\n\nSupongamos que un investigador quiere comparar los tiempos de reacción antes y después de un tratamiento en 10 sujetos. Los datos son los siguientes:\n\n\n\nSujeto\nAntes (X)\nDespués (Y)\n\n\n\n\n1\n15\n10\n\n\n2\n20\n18\n\n\n3\n25\n20\n\n\n4\n30\n30\n\n\n5\n18\n15\n\n\n6\n22\n19\n\n\n7\n26\n21\n\n\n8\n28\n26\n\n\n9\n24\n22\n\n\n10\n20\n20\n\n\n\nFormular las hipótesis:\n\n\\(H_0\\): No hay diferencia en los tiempos de reacción antes y después del tratamiento.\n\\(H_1\\): Hay una diferencia en los tiempos de reacción antes y después del tratamiento.\n\nCalcular las diferencias:\n\n\n\nSujeto\nDiferencia (D = X - Y)\nSigno\n\n\n\n\n1\n5\n+\n\n\n2\n2\n+\n\n\n3\n5\n+\n\n\n4\n0\n0\n\n\n5\n3\n+\n\n\n6\n3\n+\n\n\n7\n5\n+\n\n\n8\n2\n+\n\n\n9\n2\n+\n\n\n10\n0\n0\n\n\n\nContar los signos:\n\n\\(S_+ = 8\\)\n\\(S_- = 0\\)\nPares nulos (\\(D = 0\\)): 2\n\nEstadístico de prueba: El estadístico de la prueba del signo es el menor entre \\(S_+\\) y \\(S_-\\), que es 0 en este caso.\nDeterminar el valor crítico: Para un nivel de significancia \\(\\alpha = 0.05\\) y \\(n = 8\\) (solo los pares no nulos se consideran), consultamos la tabla de la prueba del signo y encontramos que el valor crítico es 1.\nDecisión: Como el estadístico de la prueba (0) es menor que el valor crítico (1), rechazamos la hipótesis nula \\(H_0\\). Hay suficiente evidencia para concluir que hay una diferencia significativa en los tiempos de reacción antes y después del tratamiento.\n\n\n\n\n\n4.5.2 Prueba de rangos de signos de Wilcoxon\nPara comparar muestras pareadas la opción no paramétrica más común es la prueba de Wilcoxon de rangos de signos (Wilcoxon signed-rank test). La prueba de Wilcoxon de rangos de signos es una alternativa no paramétrica a la prueba t de muestras pareadas. Se utiliza cuando los datos no cumplen con los supuestos de normalidad necesarios para la prueba t. En lugar de comparar medias, esta prueba compara las medianas de las diferencias entre las dos muestras pareadas. Es una prueba ideal para muestras pequeñas y para datos ordinales o de razón/intervalo cuando la normalidad no puede ser asumida.\nProcedimiento:\n\nCalcular las diferencias entre cada par de observaciones.\nOrdenar las diferencias absolutas y asignarles rangos, ignorando las diferencias que sean cero.\nAsignar signos a los rangos de acuerdo con el signo de las diferencias originales.\nCalcular la suma de los rangos positivos y la suma de los rangos negativos.\nDeterminar el estadístico de prueba: El estadístico de Wilcoxon es el menor de las dos sumas de rangos.\nComparar el estadístico de prueba con los valores críticos de la tabla de Wilcoxon para determinar la significancia estadística.\n\nLa prueba de Wilcoxon de signos y rangos es robusta y fácil de implementar, lo que la convierte en una herramienta valiosa para el análisis de muestras pareadas en situaciones donde los supuestos de normalidad no se cumplen.\n\n\n\n\n\n\nEjemplo práctico. Prueba sobre datos emparejados\n\n\n\n\n\nSupongamos que tenemos dos conjuntos de datos emparejados que representan las puntuaciones antes y después de una intervención:\n\n# Datos de ejemplo\nantes &lt;- c(10, 20, 30, 40, 50)\ndespues &lt;- c(12, 18, 33, 35, 55)\n\n# Realizar la prueba de Wilcoxon de signos y rangos\nresultado &lt;- wilcox.test(antes, despues, paired = TRUE)\n\n# Mostrar el resultado\nprint(resultado)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  antes and despues\nV = 6, p-value = 0.7855\nalternative hypothesis: true location shift is not equal to 0\n\n\nEl estadístico V es 6 y el \\(p-valor\\) es 0.785. En este caso, dado que el valor p es mayor que un nivel de significancia común (como \\(0.05\\)), no se rechaza la hipótesis nula de que no hay una diferencia significativa entre las puntuaciones antes y después de la intervención.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Contraste no paramétrico</span>"
    ]
  },
  {
    "objectID": "conclusiones.html",
    "href": "conclusiones.html",
    "title": "6  Conclusiones",
    "section": "",
    "text": "6.1 Resumen de los aprendizajes\nA lo largo del libro, hemos abordado diversos temas que forman la columna vertebral del análisis de regresión:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Conclusiones</span>"
    ]
  },
  {
    "objectID": "conclusiones.html#resumen-de-los-aprendizajes",
    "href": "conclusiones.html#resumen-de-los-aprendizajes",
    "title": "6  Conclusiones",
    "section": "",
    "text": "Modelos de Regresión Lineal Simple y Múltiple:\nComprendimos cómo los modelos lineales permiten describir la relación entre una variable dependiente y una o más variables independientes. Exploramos técnicas para estimar parámetros, interpretar coeficientes y diagnosticar la validez del modelo.\nMétodos de selección de variables y Regularización:\nAprendimos a identificar las variables más relevantes mediante técnicas de selección como el stepwise, así como métodos de regularización como Ridge, Lasso y Elastic Net, que ayudan a mejorar la generalización del modelo.\nModelos no lineales y transformación de variables:\nIntrodujimos métodos para capturar relaciones no lineales entre variables, incluyendo la regresión polinomial, los splines y la ingeniería de características para mejorar el rendimiento predictivo.\nModelos de Regresión generalizada:\nAmpliamos nuestro conocimiento hacia modelos que permiten trabajar con diferentes tipos de variables dependientes, como la regresión logística para variables binarias y la regresión de Poisson para datos de conteo.\nOtros modelos avanzados:\nFinalmente, exploramos técnicas avanzadas como los Modelos Aditivos Generalizados (GAMs), que proporcionan una manera flexible de capturar relaciones no lineales complejas sin perder interpretabilidad.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Conclusiones</span>"
    ]
  },
  {
    "objectID": "conclusiones.html#reflexiones-finales",
    "href": "conclusiones.html#reflexiones-finales",
    "title": "6  Conclusiones",
    "section": "6.2 Reflexiones finales",
    "text": "6.2 Reflexiones finales\nEl aprendizaje de los Modelos de Regresión va más allá de la simple aplicación de fórmulas o técnicas estadísticas. A través de este curso, hemos desarrollado un enfoque crítico para analizar datos, identificar patrones y tomar decisiones basadas en evidencia. Hemos comprendido la importancia de realizar un diagnóstico adecuado del modelo, asegurando que los supuestos estadísticos se cumplan y que los resultados sean fiables y reproducibles.\nLa interpretabilidad y la validación del modelo son pilares fundamentales en el análisis de regresión. Entender cómo y por qué un modelo llega a ciertas conclusiones es tan importante como la precisión de sus predicciones. En un mundo donde los datos juegan un papel cada vez más crucial, la habilidad para interpretar resultados y comunicar hallazgos de manera clara y concisa es esencial para cualquier profesional de la ciencia de datos.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Conclusiones</span>"
    ]
  },
  {
    "objectID": "conclusiones.html#mirando-hacia-adelante",
    "href": "conclusiones.html#mirando-hacia-adelante",
    "title": "6  Conclusiones",
    "section": "6.3 Mirando hacia adelante",
    "text": "6.3 Mirando hacia adelante\nCon el conocimiento y las habilidades adquiridas en este curso, los estudiantes están mejor preparados para profundizar en el vasto campo de la ciencia de datos. Los modelos de regresión seguirán evolucionando con el avance de la tecnología y la creciente disponibilidad de datos. Por lo tanto, es crucial que los futuros profesionales mantengan una mentalidad de aprendizaje continuo y estén abiertos a adoptar nuevas metodologías y herramientas.\nLos modelos de regresión son solo el comienzo de un camino más amplio en el análisis predictivo y el aprendizaje automático. En los próximos cursos, como Aprendizaje Automático I y Aprendizaje Automático II, aplicarás muchas de las técnicas y conceptos que hemos aprendido, avanzando hacia modelos más complejos como los árboles de decisión, las redes neuronales y los modelos de ensamblaje.\nEn conclusión, esperamos que este libro haya proporcionado una comprensión profunda y práctica de los Modelos de Regresión, y que inspire a los estudiantes a aplicar estos conocimientos con confianza y creatividad en sus proyectos futuros. La capacidad de analizar datos de manera crítica y tomar decisiones basadas en evidencias es una habilidad poderosa y transformadora, que sin duda abrirá numerosas oportunidades en el ámbito profesional.\n¡Buena suerte en tu camino en la ciencia de datos!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Conclusiones</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Bibliografía",
    "section": "",
    "text": "Coxe, Stefany, Stephen G West, and Leona S Aiken. 2009. “The\nAnalysis of Count Data: A Gentle Introduction to Poisson Regression and\nIts Alternatives.” Journal of Personality Assessment 91\n(2): 121–36.\n\n\nDraper, NR. 1998. Applied Regression Analysis. McGraw-Hill.\nInc.\n\n\nHastie, Trevor J. 2017. “Generalized Additive Models.” In\nStatistical Models in s, 249–307. Routledge.\n\n\nHosmer Jr, David W, Stanley Lemeshow, and Rodney X Sturdivant. 2013.\nApplied Logistic Regression. John Wiley & Sons.\n\n\nIsabel, Alberto Fernández, and Isaac Martı́n De Diego. 2020. Ciencia\nde Datos Para La Ciberseguridad. Ra-Ma Editorial.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al.\n2013. An Introduction to Statistical Learning. Vol. 112.\nSpringer.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, and William Li.\n2005. Applied Linear Statistical Models. McGraw-hill.\n\n\nLambert, Diane. 1992. “Zero-Inflated Poisson Regression, with an\nApplication to Defects in Manufacturing.” Technometrics\n34 (1): 1–14.\n\n\nMarquardt, Donald W, and Ronald D Snee. 1975. “Ridge Regression in\nPractice.” The American Statistician 29 (1): 3–20.\n\n\nNelder, John Ashworth, and Robert WM Wedderburn. 1972.\n“Generalized Linear Models.” Journal of the Royal\nStatistical Society Series A: Statistics in Society 135 (3):\n370–84.\n\n\nRanstam, Jonas, and Jonathan A Cook. 2018. “LASSO\nRegression.” Journal of British Surgery 105 (10):\n1348–48.\n\n\nWeisberg, S. 2005. “Applied Linear Regression.” Wiley.\n\n\nZou, Hui, and Trevor Hastie. 2005. “Regularization and Variable\nSelection via the Elastic Net.” Journal of the Royal\nStatistical Society Series B: Statistical Methodology 67 (2):\n301–20.",
    "crumbs": [
      "Bibliografía"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modelos Estadísticos para la Predicción",
    "section": "",
    "text": "Prefacio\nLos modelos estadísticos han emergido como herramientas fundamentales en la era de la información, donde la capacidad de analizar y predecir comportamientos a partir de datos se ha convertido en una habilidad esencial. En este contexto, los modelos para la predicción juegan un papel crucial al permitirnos describir y cuantificar las relaciones entre variables, así como anticipar resultados futuros. Este libro está diseñado para proporcionar una comprensión profunda y práctica de estas técnicas, basándose en el contenido de la asignatura impartida en el Grado en Matemáticas.\nA lo largo de los capítulos, encontrarás una combinación de teoría rigurosa y aplicaciones prácticas. Se abordarán temas como la regresión lineal simple y múltiple, métodos de selección de variables y regularización, ingeniería de características y modelos generalizados, entre otros. Además, todos los conceptos se ilustrarán con ejemplos en R, permitiéndote aplicar lo aprendido a conjuntos de datos reales.\nEl objetivo de este libro es doble: por un lado, proporcionar herramientas avanzadas para analizar relaciones sujetas a incertidumbre y, por otro, capacitarte para elegir el método más apropiado para resolver problemas de predicción o explicación, analizando la naturaleza de las variables y sus posibles interacciones. Al finalizar, habrás desarrollado una comprensión sólida de los modelos estadísticos y estarás preparado para enfrentar desafíos en el análisis predictivo con confianza y creatividad.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "tema1.html",
    "href": "tema1.html",
    "title": "2  El modelo de regresión lineal simple",
    "section": "",
    "text": "2.1 Exploración inicial: visualización y cuantificación de la relación\nLa regresión lineal constituye uno de los pilares fundamentales de la modelización estadística. Es, a menudo, el primer y más importante modelo predictivo que se aprende, no solo por su simplicidad e interpretabilidad, sino porque los conceptos que exploraremos aquí son la base sobre la que se construyen técnicas mucho más avanzadas, como el modelo de regresión lineal múltiple, los modelos lineales generalizados (GLM) o incluso conceptos utilizados en algoritmos de machine learning (Draper 1998).\nEn este capítulo, daremos el primer y más crucial paso en nuestro viaje por el modelado predictivo: el estudio del modelo de regresión lineal simple. Para ello, seguiremos el ciclo de vida completo de un proyecto de modelado: comenzaremos con la exploración visual y cuantitativa de los datos, formalizaremos después nuestras observaciones mediante el lenguaje matemático del modelo y sus supuestos, aprenderemos a estimar sus parámetros, realizaremos inferencias sobre ellos y, finalmente, diagnosticaremos la validez de nuestro modelo.\nLa comprensión profunda que desarrollaremos aquí es esencial, ya que los principios de estimación, inferencia y diagnóstico que aprenderemos son directamente escalables al modelo de regresión lineal múltiple, que exploraremos en el siguiente capítulo.\nAntes de sumergirnos en la teoría de la regresión, debemos hacer lo que todo buen analista hace primero: observar y cuantificar la relación en los datos. Este paso exploratorio es fundamental para formular hipótesis y justificar la elección de un modelo lineal.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema1.html#preguntas",
    "href": "tema1.html#preguntas",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.1 Preguntas",
    "text": "1.1 Preguntas\nTu objetivo principal durante el EDA es adquirir una comprensión profunda de los datos que se están analizando. La forma más sencilla de hacerlo es utilizar preguntas como herramientas para guiar la investigación. Cuando planteas una pregunta, ésta centra tu atención en una parte específica del conjunto de datos y te ayuda a decidir qué gráficos, modelos o transformaciones realizar.\nEDA es un proceso creativo y como tal, la clave para llevarlo a cabo consiste en el planteamiento de preguntas de calidad. ¿Qué preguntas son las correctas? La respuesta es que depende del conjunto de datos con el que se trabaje.\n\n\n\n\n\n\nJohn Tukey\n\n\n\nMucho mejor una respuesta aproximada a la pregunta correcta, que a menudo es vaga, que una respuesta exacta a la pregunta incorrecta, que siempre se puede precisar.\n\n\nAl inicio del análisis, puede resultar todo un desafío formular preguntas reveladoras, ya que aún no se conoce completamente la información contenida en el conjunto de datos. Si estás involucrado en un proceso de inferencia estadística, en muchas ocasiones, esas preguntas vendrán formuladas por un experto del dominio o por un superior con conocimientos estadísticos. Cada nueva pregunta que se plantee te llevará a explorar un nuevo aspecto de tus datos, aumentando así las posibilidades de hacer descubrimientos importantes.\n\n\n\n\n\n\nPara recordar\n\n\n\nDurante la preparación y limpieza de los datos acumulamos pistas sobre los modelos de aprendizaje más adecuados que podrán ser aplicados en etapas posteriores.\n\n\nAlgunas de las preguntas que, generalmente, deberían de abordarse durante el EDA son:\n\n¿Cuál es el tamaño de la base de datos? Es decir:\n\n¿Cuántas observaciones hay?\n¿Cuántas variables/características están medidas?\n¿Disponemos de capacidad de cómputo en nuestra máquina para procesar la base de datos o necesitamos más recursos?\n¿Existen valores faltantes?\n\n¿Qué tipo variables aparecen en la base de datos?\n\n¿Qué variables son discretas?\n¿Cuáles son continuas?\n¿Qué categorías tienen las variables?\n¿Hay variables tipo texto?\n\nVariable objetivo: ¿Existe una variable de “respuesta”?\n\n¿Binaria o multiclase?\n\n¿Es posible identificar variables irrelevantes?. Estudiar variables relevantes requiere, habitualmente, métodos estadísticos.\n¿Es posible identificar la distribución que siguen las variables?\nCalcular estadísticos resumen (media, desviación típica, frecuencia,…) de todas las variables de interés. Estudiaremos las propiedades de estos estimadores en el próximo capítulo.\nDetección y tratamiento de valores atípicos.\n\n¿Son errores de media?\n¿Podemos eliminarlos?\n\n¿Existe correlación entre variables?\n\n\n\n\n\n\n\nPara recordar\n\n\n\nUna correcta preparación y limpieza de datos implica, sin duda, un ahorro de tiempo en etapas posteriores del proyecto.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#entender-el-negocio",
    "href": "tema1.html#entender-el-negocio",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.2 Entender el negocio",
    "text": "1.2 Entender el negocio\nLa comprensión del problema que estamos abordando representa una de las primeras etapas en cualquier proyecto de ciencia de datos. En la mayoría de los casos, esta tarea se realiza en estrecha colaboración con expertos en el dominio correspondiente, quienes a menudo son las personas que han solicitado (y a menudo financian) el análisis de datos. Es importante recordar que cualquier estudio que involucre ciencia de datos requiere un conocimiento profundo del dominio, el cual debe ser compartido con el científico de datos. Por lo tanto, el profesional de la ciencia de datos debe poseer un conocimiento suficiente para enfrentar con confianza los diversos desafíos que puedan surgir. Esta comprensión inicial permite establecer los objetivos del proyecto y procesar los datos de manera correcta para obtener información valiosa. A través de esta información, se busca derivar conocimientos aplicables. Este conocimiento puede ser aprendido y almacenado para su uso futuro, lo que lleva a la sabiduría, según la jerarquía de conocimiento presentada en la Figura Figura 1.1.\n\n\n\n\n\n\nFigura 1.1: Jerarquía de Conocimiento\n\n\n\n\n\n\n\n\n\nClaude Lévi-Strauss\n\n\n\n“El científico no es una persona que da las respuestas correctas, sino una persona que hace las preguntas correctas.”",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#un-primer-vistazo-a-los-datos",
    "href": "tema1.html#un-primer-vistazo-a-los-datos",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.3 Un primer vistazo a los datos",
    "text": "1.3 Un primer vistazo a los datos\nEn este capítulo vamos a trabajar con los datos de Bank Marketing del repositorio UCI. En primer lugar debemos comprender el problema. ¿Qué sabes del marketing bancario? En el caso que nos ocupa, los datos están relacionados con campañas de marketing directo (llamadas telefónicas) de una entidad bancaria portuguesa. El objetivo final, que abordaremos en cursos posteriores, es predecir si el cliente suscribirá un depósito a plazo (variable objetivo). En este curso nos conformamos con adquirir conocimiento de los datos, planteando algunas hipótesis de interés.\nLas variables que debemos estudiar son:\nVariables de entrada:\n\nDatos del cliente bancario:\n\n\nedad (variable numérica)\nempleo : tipo de empleo (variable categórica con las siguientes categorías: “admin.”, “desconocido”, “desempleado”, “directivo”, “empleada del hogar”, “empresario”, “estudiante”, “obrero”, “autónomo”, “jubilado”, “técnico”, “servicios”)\nestado civil : estado civil (variable categórica con categorías: “casado”, “divorciado”, “soltero”; nota: “divorciado” significa divorciado o viudo)\neducación (variable categórica con categorías: “desconocida”, “secundaria”, “primaria”, “terciaria”)\nimpago: ¿tiene un crédito impagado? (variable binaria con dos posibles valores: “sí”, “no”)\nsaldo: saldo medio anual, en euros (variable numérica)\nvivienda: ¿tiene préstamo para vivienda? (variable binaria: “sí”, “no”)\npréstamo: ¿tiene préstamo personal? (variable binaria: “sí”, “no”)\n# relacionado con el último contacto de la campaña actual:\ncontacto: tipo de comunicación del contacto (variable categórica: “desconocido”, “teléfono”, “móvil”)\ndía: día del mes del último contacto (variable numérica)\nmes: mes del año del último contacto (variable categórica: “ene”, “feb”, “mar”, …, “nov”, “dic”)\nduración: duración del último contacto, en segundos (variable numérica)\n\n\nOtros atributos\n\n\ncampaña: número de contactos realizados durante esta campaña y para este cliente (variable numérica, incluye el último contacto)\npdays: número de días transcurridos desde que el cliente fue contactado por última vez en una campaña anterior (variable numérica, -1 significa que el cliente no fue contactado previamente)\nprevious: número de contactos realizados antes de esta campaña y para este cliente (variable numérica)\npoutcome: resultado de la campaña de marketing anterior (variable categórica: “desconocido”, “otro”, “fracaso”, “éxito”)\n\n\nVariable de salida (objetivo deseado):\n\n17 - y: ¿ha suscrito el cliente un depósito a plazo? (variable binaria: “sí”, “no”)\n\n\n\n\n\n\nPara recordar\n\n\n\nA veces (muchas veces) la descripción que encontramos en una primera etapa no coincide al completo con los datos que luego nos entrega el cliente. Los datos suelen ser más complejos que la teoría detrás de los datos.\nEn otras ocasiones no se dispone de la descripción de las variables. En ese caso, ¡hay que hacer lo imposible por conseguirla! Si no conocemos el significado de una variable, difícilmente podremos interpretar los resultados asociados a ella.\n\n\nLeemos los datos con R.\n\nlibrary(tidyverse)\nbank = read.csv('https://raw.githubusercontent.com/rafiag/DTI2020/main/data/bank.csv')\ndim(bank)\n\n[1] 11162    17\n\nbank=as.tibble(bank)\nbank\n\n# A tibble: 11,162 × 17\n     age job       marital education default balance housing loan  contact   day\n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;int&gt;\n 1    59 admin.    married secondary no         2343 yes     no    unknown     5\n 2    56 admin.    married secondary no           45 no      no    unknown     5\n 3    41 technici… married secondary no         1270 yes     no    unknown     5\n 4    55 services  married secondary no         2476 yes     no    unknown     5\n 5    54 admin.    married tertiary  no          184 no      no    unknown     5\n 6    42 manageme… single  tertiary  no            0 yes     yes   unknown     5\n 7    56 manageme… married tertiary  no          830 yes     yes   unknown     6\n 8    60 retired   divorc… secondary no          545 yes     no    unknown     6\n 9    37 technici… married secondary no            1 yes     no    unknown     6\n10    28 services  single  secondary no         5090 yes     no    unknown     6\n# ℹ 11,152 more rows\n# ℹ 7 more variables: month &lt;chr&gt;, duration &lt;int&gt;, campaign &lt;int&gt;, pdays &lt;int&gt;,\n#   previous &lt;int&gt;, poutcome &lt;chr&gt;, deposit &lt;chr&gt;\n\n\nDisponemos de más de 10000 observaciones y un total de 17 variables.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#tipo-de-variables",
    "href": "tema1.html#tipo-de-variables",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.4 Tipo de variables",
    "text": "1.4 Tipo de variables\nPara averiguar qué tipo de variables manejamos, ejecutar:\n\nstr(bank)\n\ntibble [11,162 × 17] (S3: tbl_df/tbl/data.frame)\n $ age      : int [1:11162] 59 56 41 55 54 42 56 60 37 28 ...\n $ job      : chr [1:11162] \"admin.\" \"admin.\" \"technician\" \"services\" ...\n $ marital  : chr [1:11162] \"married\" \"married\" \"married\" \"married\" ...\n $ education: chr [1:11162] \"secondary\" \"secondary\" \"secondary\" \"secondary\" ...\n $ default  : chr [1:11162] \"no\" \"no\" \"no\" \"no\" ...\n $ balance  : int [1:11162] 2343 45 1270 2476 184 0 830 545 1 5090 ...\n $ housing  : chr [1:11162] \"yes\" \"no\" \"yes\" \"yes\" ...\n $ loan     : chr [1:11162] \"no\" \"no\" \"no\" \"no\" ...\n $ contact  : chr [1:11162] \"unknown\" \"unknown\" \"unknown\" \"unknown\" ...\n $ day      : int [1:11162] 5 5 5 5 5 5 6 6 6 6 ...\n $ month    : chr [1:11162] \"may\" \"may\" \"may\" \"may\" ...\n $ duration : int [1:11162] 1042 1467 1389 579 673 562 1201 1030 608 1297 ...\n $ campaign : int [1:11162] 1 1 1 1 2 2 1 1 1 3 ...\n $ pdays    : int [1:11162] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...\n $ previous : int [1:11162] 0 0 0 0 0 0 0 0 0 0 ...\n $ poutcome : chr [1:11162] \"unknown\" \"unknown\" \"unknown\" \"unknown\" ...\n $ deposit  : chr [1:11162] \"yes\" \"yes\" \"yes\" \"yes\" ...\n\n\nCreamos las particiones sobre los datos y trabajamos sobre la partición de entrenamiento. Este paso cobrará significado en asignaturas posteriores, especialmente en las enfocadas en el aprendizaje automático (Machine Learning). Sin entrar en más detalles, podemos decir que la idea fundamental es estudiar las hipótesis en un subconjunto de la muestra disponible y evaluar la veracidad (o no) de dichas hipótesis en muestras diferentes.\n\n# Parciticionamos los datos\nset.seed(2138)\nn=dim(bank)[1]\nindices=seq(1:n)\nindices.train=sample(indices,size=n*.5,replace=FALSE)\nindices.test=sample(indices[-indices.train],size=n*.25,replace=FALSE)\nindices.valid=indices[-c(indices.train,indices.test)]\n\nbank.train=bank[indices.train,]\nbank.test=bank[indices.test,]\nbank.valid=bank[indices.valid,]\n\n\n\n\n\n\n\nAtrévete\n\n\n\n¿Te has hecho (ya) alguna pregunta sobre los datos? Si es así, no esperes más, busca la respuesta!\n\n\nPor ejemplo, ¿qué te parecen estas preguntas que nosotros proponemos?\n¿Qué día del año se producen más depósitos por parte de los estudiantes?\n\n\nClick para ver el código\nbank.train %&gt;% \n  filter(deposit==\"yes\") %&gt;% \n  count(month, day) %&gt;% \n  top_n(1,n)\n\n\n# A tibble: 1 × 3\n  month   day     n\n  &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n1 apr      30    79\n\n\n¿En qué mes del año realizan más depósitos los estudiantes?\n\n\nClick para ver el código\nbank.train %&gt;% \n  filter(deposit==\"yes\" & job==\"student\") %&gt;% \n  count(month) %&gt;% \n  top_n(1,n)\n\n\n# A tibble: 1 × 2\n  month     n\n  &lt;chr&gt; &lt;int&gt;\n1 apr      21\n\n\n¿Qué trabajo está asociado con el mayor porcentaje de depósitos?\n\n\nClick para ver el código\nbank.train %&gt;%\n  group_by(job) %&gt;%\n  mutate(d = n()) %&gt;%\n  group_by(job, deposit) %&gt;%\n  summarise(Perc = n()/first(d), .groups = \"drop\") %&gt;%\n  pivot_wider(\n    id_cols = job,\n    names_from = deposit,\n    values_from = Perc\n  ) %&gt;%\n  top_n(1)\n\n\n# A tibble: 1 × 3\n  job        no   yes\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 student 0.218 0.782\n\n\n\n\n\n\n\n\nRepaso\n\n\n\ndplyr es un paquete en R diseñado para facilitar la manipulación y transformación de datos de manera eficiente y estructurada. Fue desarrollado por Hadley Wickham y se ha convertido en una de las herramientas más populares en la ciencia de datos y análisis de datos.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#variable-objetivo",
    "href": "tema1.html#variable-objetivo",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.5 Variable objetivo",
    "text": "1.5 Variable objetivo\nEn el ámbito del Aprendizaje Automático, en problemas de clasificación (Aprendizaje Supervisado) existe una variable de interés fundamental, es la variable respuesta o variable objetivo. En el próximo cuatrimestre se trata con detalle este tipo de problemas. En el caso que nos ocupa dicha variable es la característica: “deposit”. Vamos a estudiar la información que nos proporciona dicha variable.\n\nlibrary(ggplot2)\ntable(bank.train$deposit)\n\n\n  no  yes \n2908 2673 \n\nggplot(data=bank.train,aes(x=deposit,fill=deposit)) +\n  geom_bar(aes(y=(..count..)/sum(..count..))) +\n  scale_y_continuous(labels=scales::percent) +\n  theme(legend.position=\"none\") +\n  ylab(\"Frecuencia relativa\") +\n  xlab(\"Variable respuesta: deposit\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#visualizar-distribuciones",
    "href": "tema1.html#visualizar-distribuciones",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.6 Visualizar distribuciones",
    "text": "1.6 Visualizar distribuciones\nLa forma de visualizar la distribución de una variable dependerá de si la variable es categórica o continua. Una variable es categórica si sólo puede tomar uno de un pequeño conjunto de valores. En R, las variables categóricas suelen guardarse como factores o vectores de caracteres. Para examinar la distribución de una variable categórica, utiliza un gráfico de barras:\n\nggplot(data = bank.train) +\n  geom_bar(mapping = aes(x = contact))\n\n\n\n\n\n\n\n\nPuedes obtener los valores exactos en cada categoría como sigue:\n\nbank.train%&gt;% \n  count(contact)\n\n# A tibble: 3 × 2\n  contact       n\n  &lt;chr&gt;     &lt;int&gt;\n1 cellular   4025\n2 telephone   383\n3 unknown    1173\n\n\nUna variable es continua si puede tomar cualquiera de un conjunto infinito de valores ordenados. Para examinar la distribución de una variable continua, utiliza un histograma:\n\nggplot(data = bank.train) +\n  geom_histogram(mapping = aes(x = age), binwidth = 5)\n\n\n\n\n\n\n\n\nUn histograma divide el eje \\(x\\) en intervalos equidistantes y, a continuación, utiliza la altura de una barra para mostrar el número de observaciones que se encuentran en cada intervalo. En el gráfico anterior, la primera barra muestra unas \\(100\\) observaciones (realmente son \\(119\\)) tienen un valor de edad por debajo de \\(22.5\\) años. Puede establecer la anchura de los intervalos en un histograma con el argumento binwidth, que se mide en las unidades de la variable \\(x\\).\n\n\n\n\n\n\nPara recordar\n\n\n\nSiempre se deben explorar una variedad de anchos de intervalo cuando trabajamos con histogramas, ya que diferentes anchos de intervalo pueden revelar diferentes patrones.\n\n\nPodemos representar funciones de densidad de probabilidad.\n\nggplot(bank.train, aes(x = age)) +\ngeom_density() +\nggtitle('KDE de edad en datos bank')\n\n\n\n\n\n\n\n\nOtro gráfico muy utilizado para variables cuantitativas univariantes es el boxplot, también llamado box-and-whisker plot (diagrama de caja y bigotes). Es especialmente útil para detectar posibles datos atípicos en los valores de una variable, siempre que su distribución sea parecida a una distribución Normal. El gráfico muestra:\n\nUna caja cuyos límites son el primer y el tercel cuartil de la distribución de valores.\nUna línea central, que marca la mediana.\nLos bigotes, que por defecto (en R) se extienden hasta 1.5 veces el valor del rango intercuartílico (IQR) por encima y por debajo de la caja.\nPuntos individuales, que quedan más allá del límite de los bigotes, marcan posibles datos atípicos.\n\nEn distribuciones muy asimétricas o con muchos valores extremos, muy diferentes a una distribución Normal, aparecerán demasiados puntos más allá de los bigotes y no se podrán apreciar fácilmente los atípicos (demasiados puntos considerados como tales). En ese caso, es conveniente intentar una transformación de la variable antes de representar el boxplot.\n\nggplot(bank.train, aes(x=deposit, y=age, color=deposit)) +\n  geom_boxplot()",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#transformación-de-variables",
    "href": "tema1.html#transformación-de-variables",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.7 Transformación de variables",
    "text": "1.7 Transformación de variables\n\n1.7.1 Transformación de variables cuantitativas\nEn algunos métodos de inferencia estadística y aprendizaje automático será necesario contar con variables que cumplan requisitos de normalidad. Por ejemplo, si tomamos la transformación \\(log\\) sobre la variable edad obtenemos una distribución multimodal que, probablemente, corresponda a la combinación de dos (o más) normales.\n\nggplot(data = bank.train) +\n  geom_histogram(mapping = aes(x = log(age)), binwidth = .1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPara recordar\n\n\n\nLos modelos de aprendizaje serán tan buenos como lo sean las variables de entrada de dichos algoritmos.\n\n\n\n1.7.1.1 Transformaciones para igualar dispersión\nCon frecuencia, el objetivo de la transformación de variables cuantitativas es obtener una variable cuya distribución de valores sea:\n\nMás simétrica y con menor dispersión que la original.\nMás semejante a una distribución normal (e.g. para algunos modelos lineales).\nRestringida en un intervalo de valores (e.g. \\([0,1]\\) ).\n\nLa forma más sencilla de detectar que alguna de nuestras variables necesita ser transformada es representar un gráfico que muestre la distribución de valores de la variable. Por ejemplo, un histograma o un diagrama de densidad de probabilidad (o ambos).\nEl uso de los logaritmos tiene su propia recomendación en preparación de datos (Fox y Weisberg 2018):\n\n\n\n\n\n\nJohn Fox\n\n\n\n“Si la variable es estrictamente positiva, no tiene un límite superior para sus valores, y su rango abarca dos o más órdenes de magnitud (potencias de \\(10\\)), entonces la transformación logarítmica suele ser útil. A la inversa, cuando la variable tiene un rango de valores pequeño (menor de un orden de magnitud), el logaritmo o cualquier otra transformación simple no ayudará mucho.”\n\n\nLa versión general de esta transformación son las transformaciones de escala-potencia (scaled-power transformations), también denominadas transformaciones de Box-Cox.\n\\[x(\\lambda)= \\begin{cases} \\frac{x^\\lambda-1}{\\lambda},& \\text{cuando } \\lambda \\neq 0,\\\\ log_e(x), & \\text{cuando } \\lambda = 0 \\end{cases}\\]\nLa función car::symbox(...) permite probar varias combinaciones típicas del parámetro \\(\\lambda\\) , para comprobar con cuál de ellas obtenemos una distribución más simétrica de valores.\n\nlibrary(car)\n\nbank.train %&gt;% symbox(~ age, data = .)\n\n\n\n\n\n\n\n\n\n\n1.7.1.2 Transformaciones para igualar dispersión\nTambién es bastante común aplicar transformaciones en datos cuantitativos para igualar las escalas de representación de las variables. En muchos modelos, si una de nuestras variables tiene una escala mucho mayor que las demás, sus valores tienden a predominar en los resultados, enmascarando la influencia del resto de variables en el modelo.\nPor este motivo, en muchos modelos es importante garantizar que todas las variables se representan en escalas comparables, de forma que ninguna predomine sobre el resto. Conviene aclarar un poco algunos términos que se suelen emplear de forma indistinta:\n\nReescalado o cambio de escala: Consiste en sumar o restar una constante a un vector, y luego multiplicar o dividir por una constante. Por ejemplo, para transformar la unidad de medida de una variable (grados Farenheit → grados Celsius).\nNormalización: Consiste en dividir por la norma de un vector, por ejemplo para hacer su distancia euclídea igual a \\(1\\).\nEstandarización: Consiste en restar a un vector una medida de localización o nivel (e.g. media, mediana) y dividir por una medida de escala (dispersión). Por ejemplo, si restamos la media y dividimos por la desviación típica hacemos que la distribución tenga media \\(0\\) y desviación típica \\(1\\).\n\nAlgunas aternativas comunes son:\n\\[\nEstandarización \\rightarrow Y=\\frac{X-\\overline{x}}{s_x}\n\\]\n\\[\nEscalado \\space min-max \\rightarrow Y=\\frac{X-min_x}{max_x-min_x}\n\\]\nEn R, la función scale() se puede utilizar para realizar estas operaciones de estandarización. Automáticamente, puede actuar sobre las columnas de un data.frame, aplicando la misma operación a todas ellas (siempre que todas sean cuantitativas).\n\n\n\n1.7.2 Transformación de variables cualitativas\nA diferencia de las variables cuantitativas, que representan cantidades numéricas, las variables cualitativas, también conocidas como variables categóricas, se utilizan para describir características o cualidades que no tienen un valor numérico intrínseco. Las variables cualitativas son esenciales en la investigación y el análisis de datos, ya que a menudo se utilizan para clasificar, segmentar y comprender información sobre grupos, categorías o características. Algunas técnicas comunes para analizar variables cualitativas incluyen la creación de tablas de frecuencia para contar la ocurrencia de cada categoría y el uso de gráficos como gráficos de barras o diagramas de sectores para visualizar la distribución de categorías. Estos análisis pueden proporcionar información valiosa sobre patrones, tendencias y relaciones en los datos cualitativos, lo que puede ser fundamental para tomar decisiones informadas en una amplia gama de campos, desde marketing hasta investigación social y más.\nLas variables cualitativas se dividen en dos categorías principales:\n\n\n\n\n\n\nVariables Cualitativas Nominales\n\n\n\n\n\nLas variables nominales representan categorías o etiquetas que no tienen un orden inherente. Ejemplos comunes incluyen el género (masculino, femenino, otro), el estado civil (soltero, casado, divorciado) o los colores (rojo, azul, verde). No se pueden realizar operaciones matemáticas en variables nominales, como sumar o restar.\n\n\n\n\n\n\n\n\n\nVariables Cualitativas Ordinales\n\n\n\n\n\nLas variables ordinales representan categorías con un orden natural o jerarquía, pero la distancia entre las categorías no es necesariamente uniforme ni conocida. Ejemplos incluyen la calificación de satisfacción del cliente (muy insatisfecho, insatisfecho, neutral, satisfecho, muy satisfecho) o el nivel de educación (primaria, secundaria, universitaria). Aunque se pueden establecer comparaciones de orden (por ejemplo, “mayor que” o “menor que”), no es apropiado realizar operaciones matemáticas en variables ordinales.\n\n\n\nEn R, las variables categóricas se denominan factores (factors) y sus categorías niveles (levels). Es importante procesarlos adecuadamente para que los modelos aprovechen la información que contienen estas variables. Por otro lado, si se codifica incorrectamente esta información los modelos pueden estar realizando operaciones absurdas aunque nos devuelvan resultados aparentemente válidos.\n\n\n\n\n\n\nR\n\n\n\nPor defecto, R transforma columnas tipo string en factores al leer los datos de un archivo. Además, por defecto, R ordena los niveles de los factores alfabéticamente, según sus etiquetas. Debemos tener cuidado con esto, puesto que en muchos análisis es muy importante saber qué nivel se está tomando como referencia, de entre los valores posibles de un factor, para comparar con los restantes. En ciertos modelos, la elección como referencia de uno de los valores del factor (típicamente el primero que aparece en la lista de niveles) cambia por completo los resultados, así como la interpretación de los mismos.\n\n\nEn variables ordinales se debe respetar estrictamente el orden preestablecido de los niveles. Por ejemplo, una ordenación (“regular” &lt; “bueno” &lt; “malo”) es inaceptable. Para establecer una ordenación explícita entre los niveles hay que especificarla manualmente si no coincide con la alfabética, y además configurar el argumento ordered = TRUE en la función factor():\n\nsatisfaccion &lt;- rep(c(\"malo\", \"bueno\", \"regular\"), c(3,3,3))\nsatisfaccion &lt;- factor(satisfaccion, ordered = TRUE, levels = c(\"malo\", \"regular\", \"bueno\"))\nsatisfaccion\n\n[1] malo    malo    malo    bueno   bueno   bueno   regular regular regular\nLevels: malo &lt; regular &lt; bueno\n\n\nPara comprobar qué nivel se toma como referencia en cada uno de los factores de una base de datos usamos la funión levels():\n\nlevels(bank.train$marital)\n\nNULL\n\n\nY esto, ¿es correcto? Veamos la distribución de las observaciones en las categorías de la variable marital:\n\nggplot(data = bank.train) +\n  geom_bar(mapping = aes(x = marital))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecomendación\n\n\n\nHabitualmente será más recomendable elegir como categoría de referencia para variables categóricas aquella categoría con mayor número de observaciones.\n\n\nPor tanto, en este caso particular deberíamos modificar la categoría de referencia como sigue:\n\nreorder_marital = factor(bank.train$marital, levels=(c(' married', ' single', ' divorced')))\nlevels(reorder_marital)\n\n[1] \" married\"  \" single\"   \" divorced\"\n\n\nNótese que la nueva variable aquí creada, reorder_marital, no ha sido incluida (aún) en el tibble bank. Para ello:\n\nbank.train$marital = reorder_marital\n\n\n1.7.2.1 Conversión de variables cuantitativas a variables categóricas\nLa conversión de variables cuantitativas a variables categóricas es un proceso importante en EDA que implica transformar datos numéricos en categorías. Esto se realiza con el propósito de simplificar el análisis, resaltar patrones específicos y facilitar la interpretación de los resultados. A continuación, se destacan algunas situaciones comunes en las que se realiza esta conversión y cómo se lleva a cabo:\n\nAgrupación de datos numéricos: En ocasiones, es útil agrupar datos numéricos en intervalos o categorías para resaltar tendencias generales. Por ejemplo, en un estudio de edades de una población, en lugar de analizar cada edad individual, se pueden crear grupos como “menos de 18 años”, “18-30 años”, “31-45 años” y así sucesivamente.\nCreación de variables binarias: A menudo, se convierten variables numéricas en variables binarias (\\(1\\) o \\(0\\)) para simplificar el análisis. Por ejemplo, en un estudio de satisfacción del cliente, se puede crear una variable binaria donde “\\(1\\)” indica clientes satisfechos y “\\(0\\)” indica clientes insatisfechos.\nCategorización de variables continuas: Las variables continuas, como ingresos o puntuaciones, se pueden convertir en categorías para segmentar la población. Esto puede ser útil en análisis demográficos o de segmentación de mercado.\nSimplificación de modelos: Algunos modelos de ML pueden beneficiarse de la conversión de variables cuantitativas a categóricas para mejorar la interpretación y la eficacia del modelo.\n\n\n\n\n\n\n\nPara recordar\n\n\n\nEl proceso de conversión de variables cuantitativas a categóricas generalmente implica definir criterios o reglas claras para agrupar los valores numéricos en categorías significativas. Estos criterios pueden basarse en conocimiento previo del dominio, EDA o consideraciones específicas del problema. En esta etapa te vendrá genial contar con la ayuda de un experto en el dominio de aplicación, y puedes llevar a cabo cambios catastróficos en caso de no contar con esa ayuda.\n\n\nEs importante tener en cuenta que la conversión de variables cuantitativas a categóricas debe realizarse de manera cuidadosa y considerar el impacto en el análisis. La elección de cómo categorizar los datos debe estar respaldada por una comprensión sólida del problema y los objetivos del estudio. Además, se debe documentar claramente el proceso de conversión para que otros puedan replicarlo y comprender las categorías resultantes.\nA modo de ejemplo, vamos a categorizar la varible age en la base de datos bank. Para ello elegimos (elegimos!!!) las siguientes agrupaciones en la variable edad: (0,40],(40,60],(60,100].\n\n bank.train &lt;- within(bank.train, {   \n  age.cat &lt;- NA # need to initialize variable\n  age.cat[age &lt;= 40] &lt;- \"Low\"\n  age.cat[age &gt; 40 & age &lt;= 60] &lt;- \"Middle\"\n  age.cat[age &gt; 60] &lt;- \"High\"\n   } )\n\nbank.train$age.cat &lt;- factor(bank.train$age.cat, levels = c(\"Low\", \"Middle\", \"High\"))\nsummary(bank.train$age.cat)\n\n   Low Middle   High \n  3116   2151    314",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#valores-comunes-y-atípicos",
    "href": "tema1.html#valores-comunes-y-atípicos",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.8 Valores comunes y atípicos",
    "text": "1.8 Valores comunes y atípicos\nLos gráficos de barras relacionados con variables cualitativas nos han ayudado a identificar los valores más frecuentes o las categorías más repetidas en esas variables. Estos gráficos reflejan la frecuencia de cada categoría, es decir, el número de veces que aparece en el conjunto de datos. A veces ese número se representa en porcentaje respecto al número total de observaciones, proporcionando una visión relativa de la prevalencia en cada categoría. La moda es la categoría que aparece con mayor frecuencia en el conjunto de datos. Es especialmente útil para identificar la categoría más común y es aplicable a variables categóricas.\nEn el caso de las variables cuantitativas, el histograma de frecuencias se convierte en una herramienta gráfica sumamente útil para alcanzar este mismo objetivo.\n\n1.8.1 Estadísticos resumen\nA continuación te explicamos algunas medidas que resumen el comportamiento de una variable aleatoria cuantitativa:\n\n\n\n\n\n\nMedia\n\n\n\n\n\nLa media aritmética es el promedio de todos los valores de la variable. Se calcula sumando todos los valores y dividiendo por el número de observaciones. La media proporciona una indicación de la tendencia central de los datos.\n\n\n\n\n\n\n\n\n\nMediana\n\n\n\n\n\nLa mediana es el valor central en un conjunto de datos ordenados en forma ascendente o descendente. Divide el conjunto de datos en dos mitades iguales. La mediana es menos sensible a valores extremos que la media y es especialmente útil cuando los datos no siguen una distribución (aproximadamente) normal.\n\n\n\n\n\n\n\n\n\nModa\n\n\n\n\n\nLa moda es el valor que ocurre con mayor frecuencia en un conjunto de datos. Puede haber una o más modas en un conjunto de datos, y esta medida es especialmente útil para variables discretas.\n\n\n\n\n\n\n\n\n\nRango\n\n\n\n\n\nEl rango es la diferencia entre el valor máximo y el valor mínimo en un conjunto de datos. Proporciona una indicación de la dispersión o variabilidad de los datos.\n\n\n\n\n\n\n\n\n\nDesviación Estándar\n\n\n\n\n\nLa desviación estándar mide la dispersión de los datos con respecto a la media, y tiene sus mismas unidades de medida. Valores más altos indican mayor variabilidad. Es especialmente útil cuando se asume una distribución normal.\n\n\n\n\n\n\n\n\n\nCuartiles y Percentiles\n\n\n\n\n\nLos cuartiles dividen un conjunto de datos en cuatro partes iguales, mientras que los percentiles dividen los datos en cien partes iguales. Los cuartiles y percentiles son útiles para identificar valores atípicos y comprender la distribución de los datos.\n\n\n\n\n\n\n\n\n\nCoeficiente de Variación\n\n\n\n\n\nEl coeficiente de variación es una medida de la variabilidad relativa de los datos y se calcula como la desviación estándar dividida por la media. Se expresa como un porcentaje y es útil para comparar la variabilidad entre diferentes conjuntos de datos.\n\n\n\nEn R, podemos obtener algunos estadísticos resumen mediante la opción summary.\n\nsummary(bank.train$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.00   32.00   39.00   41.24   49.00   95.00 \n\n\nCuriosamente, R no tiene una función estándar incorporada para calcular la moda. Así que creamos una función de usuario para calcular la moda de un conjunto de datos en R. Esta función toma el vector como entrada y da el valor de la moda como salida.\n\n# Create the function.\nsummary_moda &lt;- function(v) {\n   uniqv &lt;- unique(v)\n   uniqv[which.max(tabulate(match(v, uniqv)))]\n}\n\nsummary_moda(bank.train$age)\n\n[1] 31\n\n\n\n\n1.8.2 Valores atípicos\nLos valores atípicos (outliers en inglés) son observaciones inusuales, puntos de datos que no parecen encajar en el patrón o el rango de la variable estudiada. A veces, los valores atípicos son errores de introducción de datos; otras veces, sugieren nuevos datos científicos importantes.\nCuando es posible, es una buena práctica llevar a cabo el análisis con y sin los valores atípicos. Si se determina que su influencia en los resultados es insignificante y no se puede identificar su origen, puede ser razonable reemplazarlos con valores faltantes y continuar con el análisis. Sin embargo, si estos valores atípicos tienen un impacto sustancial en los resultados, no se deben eliminar sin una justificación adecuada. En este caso, será necesario investigar la causa subyacente (por ejemplo, un error en la entrada de datos) y documentar su exclusión en el informe correspondiente.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#valores-faltantes",
    "href": "tema1.html#valores-faltantes",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.9 Valores faltantes",
    "text": "1.9 Valores faltantes\nLos valores faltantes (missing), también conocidos como valores nulos o valores ausentes, son observaciones o datos que no están disponibles o que no han sido registrados para una o más variables en un conjunto de datos. Estos valores pueden surgir por diversas razones, como errores de entrada de datos, respuestas incompletas en una encuesta, fallos en la medición o simplemente porque cierta información no está disponible en un momento dado.\n\n\n\n\n\n\nPara recordar\n\n\n\nLa presencia de valores faltantes en un conjunto de datos es un problema común en el análisis de datos y puede tener un impacto significativo en la calidad de los resultados. Es importante abordar adecuadamente los valores faltantes, ya que pueden sesgar los análisis y conducir a conclusiones incorrectas si no se manejan correctamente.\n\n\nAlgunas de las estrategias comunes para tratar los valores faltantes incluyen:\n\nEliminación de filas o columnas: Si la cantidad de valores faltantes es pequeña en comparación con el tamaño total del conjunto de datos, una opción es eliminar las filas o columnas que contengan valores faltantes. Sin embargo, esta estrategia puede llevar a la pérdida de información importante.\nImputación de valores: Esta estrategia implica estimar o llenar los valores faltantes con valores calculados a partir de otros datos disponibles. Esto puede hacerse utilizando técnicas como la imputación media (rellenar con la media de la variable), imputación mediana (rellenar con la mediana), imputación de vecinos más cercanos o técnicas más avanzadas como regresión u otras técnicas de modelado.\nMarcadores especiales: En algunos casos, es útil asignar un valor específico (como “N/A” o “-999”) para indicar que un valor está ausente. Esto puede ser útil cuando se desea mantener un registro explícito de los valores faltantes sin eliminarlos o imputarlos. Es importante que, en este caso, el valor asignado no tenga otro significado. Por ejemplo, asignamos “-999” como marcador de valor faltante y sin embargo, es un valor plausible dentro del rango de valores de la variable.\nMétodos basados en modelos: Utilizar modelos estadísticos o de aprendizaje automático para predecir los valores faltantes en función de otras variables disponibles. Esto puede ser especialmente eficaz cuando los datos faltantes siguen un patrón que puede ser capturado por el modelo.\n\nLa elección de la estrategia adecuada para tratar los valores faltantes depende del contexto del análisis, la cantidad de datos faltantes y la naturaleza de los datos. Es fundamental abordar este problema de manera cuidadosa y transparente, documentando cualquier procedimiento de imputación o tratamiento de valores faltantes utilizado en el análisis para garantizar la integridad y la validez de los resultados.\n\n\n\n\n\n\nPeligro\n\n\n\nSustituir valores faltantes por otros obtenidos con técnicas y métodos estadísticos o de aprendizaje automático siempre es un riesgo, pues implica “inventar” datos allá donde no los hay.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#correlación-entre-variables",
    "href": "tema1.html#correlación-entre-variables",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.10 Correlación entre variables",
    "text": "1.10 Correlación entre variables\nExisten varios métodos y técnicas para estudiar la correlación entre variables, lo que ayuda a comprender las relaciones entre las diferentes características en un conjunto de datos. En próximos cursos estudiarás que es de especial interés estudiar las relaciones entre la variable objetivo y las variables explicativas.\nPuedes desplegar los paneles siguientes para averiguar alguno de los métodos más comunes.\n\n\n\n\n\n\nMatriz de correlación\n\n\n\n\n\nLa matriz de correlación es una tabla que muestra las correlaciones entre todas las combinaciones de variables en un conjunto de datos. Los valores de correlación varían entre \\(-1\\) y \\(1\\), donde \\(-1\\) indica una correlación negativa perfecta, \\(1\\) indica una correlación positiva perfecta y \\(0\\) indica la ausencia de correlación. Este método es especialmente útil para identificar relaciones lineales entre variables numéricas.\n\n\n\n\n\n\n\n\n\nGráficos de dispersión\n\n\n\n\n\nLos gráficos de dispersión muestran la relación entre dos variables numéricas mediante puntos en un plano cartesiano. Estos gráficos permiten visualizar patrones de dispersión y tendencias entre las variables. Si los puntos se agrupan en una forma lineal, indica una posible correlación lineal.\n\n\n\n\n\n\n\n\n\nMapas de calor\n\n\n\n\n\nLos mapas de calor son representaciones visuales de la matriz de correlación en forma de un gráfico de colores. Permiten identificar rápidamente las relaciones fuertes o débiles entre variables y son útiles para resaltar patrones en grandes conjuntos de datos.\n\n\n\n\n\n\n\n\n\nCoeficiente de correlación de Pearson\n\n\n\n\n\nEste coeficiente mide la correlación lineal entre dos variables numéricas. Varía entre \\(-1\\) y \\(+1\\), donde valores cercanos a \\(-1\\) o \\(+1\\) indican una correlación fuerte, mientras que valores cercanos a \\(0\\) indican una correlación débil o nula.\n\n\n\n\n\n\n\n\n\nCoeficiente de correlación de Spearman\n\n\n\n\n\nEste coeficiente evalúa la correlación monotónica entre dos variables, lo que significa que puede detectar relaciones no lineales. Es útil cuando las variables no siguen una distribución normal.\n\n\n\n\n\n\n\n\n\nCoeficiente de correlación de Kendall\n\n\n\n\n\nSimilar al coeficiente de Spearman, evalúa la correlación entre variables, pero se centra en la concordancia de los rangos de datos, lo que lo hace útil para datos no paramétricos y muestras pequeñas.\n\n\n\n\n\n\n\n\n\nPruebas estadísticas\n\n\n\n\n\nLas pruebas estadísticas, como la prueba t de Student o la ANOVA, pueden utilizarse para evaluar si existe una diferencia significativa en los promedios de una variable entre diferentes categorías de otra variable. Si la diferencia es significativa, puede indicar una correlación entre las variables.\n\n\n\nVamos a estudiar la relación existente entre la variable objetivo deposit y la variable duration de la base de datos bank.\n\nggplot(bank.train, aes(x = log(duration), colour = deposit)) +\n  geom_density(lwd=2, linetype=1)\n\n\n\n\n\n\n\n\nPuede observarse una relación. Valores altos de la variable duración parecen estar relacionados con observaciones con deposit igual a ‘yes’.\n\ndf = bank.train %&gt;% \n      select(duration,deposit)%&gt;%\n      mutate(log.duration=log(duration))\n\n# Resumen para los casos de depósito\nsummary(df %&gt;% filter(deposit==\"yes\") %&gt;% .$log.duration)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.079   5.497   6.073   6.046   6.593   8.087 \n\n# Resumen para los casos de no depósito\nsummary(df %&gt;% filter(deposit==\"no\") %&gt;% .$log.duration)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.6931  4.5433  5.0999  5.0308  5.6276  7.5022 \n\n\nGráficamente, podemos comparar los boxplots.\n\nggplot(df, aes(deposit, log.duration)) +\n        geom_boxplot()\n\n\n\n\n\n\n\n\nPodemos determinar la importancia de relación. Por ejemplo, podemos realizar un test de la T para igualdad de medias. Estudiaremos estos conceptos en el ?sec-para.\n\nt.test(log.duration ~ deposit, data = df)\n\n\n    Welch Two Sample t-test\n\ndata:  log.duration by deposit\nt = -45.828, df = 5464.5, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group no and group yes is not equal to 0\n95 percent confidence interval:\n -1.0583835 -0.9715488\nsample estimates:\n mean in group no mean in group yes \n         5.030821          6.045787 \n\n\n\n\n\n\n\n\nEjercicio\n\n\n\nComprenderás este resultado a lo largo del curso. De momento, puedes preguntar al profesor. Dejamos como ejercicio para el alumno la interpretación del resultado del test.\n\n\nEs posible estudiar la relación entre dos variables categóricas de manera gráfica.\n\nggplot(data = bank.train, aes(x = housing, fill = deposit)) +\n    geom_bar()\n\n\n\n\n\n\n\n\nParece haber una relación, estando asociados las observaciones de personas con casa propia a un mayor porcentaje de `no’ en la variable respuesta. Podemos obtener la tabla de contingencia:\n\ndata1=table(bank.train$housing, bank.train$deposit)\n\n\ndimnames(data1) &lt;- list(housing = c(\"no\", \"yes\"),\n                        deposit = c(\"no\", \"yes\"))\ndata1\n\n       deposit\nhousing   no  yes\n    no  1246 1688\n    yes 1662  985\n\n\nY el contraste correspondiente para la hipótesis nula de no existencia de relación. Estudiaremos estos conceptos en el ?sec-para.\n\nchisq.test(bank.train$housing, bank.train$deposit)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  bank.train$housing and bank.train$deposit\nX-squared = 229.44, df = 1, p-value &lt; 2.2e-16\n\n\n\n\n\n\n\n\nEjercicio\n\n\n\nDejamos como ejercicio para el alumno la interpretación del resultado del test.\n\n\n\n\n\n\nFox, John, y Sanford Weisberg. 2018. An R companion to applied regression. Sage publications.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#modelización-estadística",
    "href": "tema1.html#modelización-estadística",
    "title": "2  El modelo de regresión lineal simple",
    "section": "",
    "text": "2.1.1 Contextualización del problema\nEl primer paso en la modelización estadística es definir el problema que se busca analizar. Esto incluye identificar el contexto, establecer objetivos claros y determinar las variables involucradas.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\nProblema: Investigar si existe una relación positiva entre el tiempo dedicado al estudio semanal y el promedio de calificaciones de los estudiantes universitarios.\n\nVariables:\n\nVariable explicativa (independiente): Tiempo de estudio semanal (en horas).\n\nVariable respuesta (dependiente): Promedio de calificaciones al final del semestre (en una escala de 0 a 10).\n\n\nObjetivo: Determinar si los estudiantes que dedican más horas al estudio semanalmente obtienen mejores calificaciones en promedio.\n\nEmplearemos datos simulados para este ejemplo:\n\n# Establecer la semilla para reproducibilidad\nset.seed(123)\n\n# Número de estudiantes\nn &lt;- 100\n\n# Generar tiempo de estudio (en horas) como una variable independiente\ntiempo_estudio &lt;- round(runif(n, min = 5, max = 40), 1)\n\n# Relación lineal entre tiempo de estudio y calificaciones (con ruido)\nbeta_0 &lt;- 5  # Intercepto\nbeta_1 &lt;- 0.1  # Pendiente (efecto del tiempo de estudio)\nsigma &lt;- 0.5  # Varianza del ruido\n\n# Esta será la relación que deseamos \"descubrir\".\ncalificaciones &lt;- round(beta_0 + beta_1 * tiempo_estudio + rnorm(n, mean = 0, sd = sigma), 2)\n\n# Crear un data frame con los datos generados\ndatos &lt;- data.frame(Tiempo_Estudio = tiempo_estudio, Calificaciones = calificaciones)\n\n# Visualizar los primeros registros\nhead(datos)\n\n  Tiempo_Estudio Calificaciones\n1           15.1           6.64\n2           32.6           8.25\n3           19.3           6.91\n4           35.9           9.27\n5           37.9           8.68\n6            6.6           6.42\n\n\n\n\n\n\n\n2.1.2 Inspección gráfica e identificación de tendencias\nAntes de ajustar un modelo, es esencial realizar una inspección visual de los datos. Los gráficos de dispersión son una herramienta útil para observar tendencias, relaciones o patrones entre las variables.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nEn el ejemplo, representamos el tiempo de estudio frente a las calificaciones. De este modo podemos analizar si los puntos siguen un patrón lineal o si muestran comportamientos más complejos.\n\n# Graficar los datos de estudio generados anteriormente\nplot(datos$Tiempo_Estudio, datos$Calificaciones,\n     main = \"Relación entre Tiempo de Estudio y Calificaciones\",\n     xlab = \"Tiempo de Estudio (horas/semana)\",\n     ylab = \"Calificaciones (promedio)\",\n     pch = 19, col = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.1.3 Propuesta y ajuste del modelo\nCon base en las observaciones previas, se propone un modelo estadístico que relacione las variables. En el caso de relaciones lineales, la ecuación típica es:\n\\[\n\\text{Respuesta} = \\beta_0 + \\beta_1 (\\text{Variable explicativa}) + \\varepsilon,\n\\]\ndonde \\(\\beta_0\\) es la constante (o intercepto), \\(\\beta_1\\) es la pendiente, y \\(\\varepsilon\\) es el término de error aleatorio.\nFíjate que la expresión anterior concuerda con el siguiente principio fundamental del análisis de datos:\n\\[DATOS = MODELO + ERROR\\]\n\nLos datos representan la realidad (procesos de negocios, clientes, productos, actividades, fenómenos físicos, etc.) que se quiere comprender, predecir o mejorar.\nEl modelo es una representación simplificada de la realidad que proponemos para describirla e interpretarla más fácilmente.\nEl error refleja la diferencia entre nuestra representación simplificada de la realidad (el modelo) y los datos que relamente describen esa realidad de forma precisa.\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\nPropuesta del modelo:\n\\[\n\\text{Calificaciones} = \\beta_0 + \\beta_1 (\\text{Tiempo de Estudio}).\n\\]\nAjuste: Calcular los valores de $ _0 $ y $ _1 $ mediante el método de mínimos cuadrados. Este método busca minimizar la suma de los errores cuadrados entre los valores observados y los predichos por el modelo.\n\n\n# Ajustar un modelo de regresión lineal\nmodelo &lt;- lm(Calificaciones ~ Tiempo_Estudio, data = datos)\nsummary(modelo)\n\n\nCall:\nlm(formula = Calificaciones ~ Tiempo_Estudio, data = datos)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.11465 -0.30262 -0.00942  0.29509  1.10533 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     5.00118    0.11977   41.76   &lt;2e-16 ***\nTiempo_Estudio  0.09875    0.00488   20.23   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4842 on 98 degrees of freedom\nMultiple R-squared:  0.8069,    Adjusted R-squared:  0.8049 \nF-statistic: 409.5 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n# Graficar los datos de estudio y su recta de regresión lineal\nplot(datos$Tiempo_Estudio, datos$Calificaciones,\n     main = \"Relación entre Tiempo de Estudio y Calificaciones\",\n     xlab = \"Tiempo de Estudio (horas/semana)\",\n     ylab = \"Calificaciones (promedio)\",\n     pch = 19, col = \"blue\")\nabline(lm(Calificaciones ~ Tiempo_Estudio, data = datos), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nA la vista de los estimadores de los parámetros del modelo, tenemos la siguiente interpretación:\n\nSe produce un incremento de la calificiación de \\(0.1\\) (aproximadamente) por cada incremento de \\(1\\) hora en el tiempo de estudio. O bien, por cada \\(10\\) horas de estudio, se incrementa la nota en \\(1\\) punto.\nEn ocasiones es complicado interpretar el valor de \\(\\beta_0\\). En este caso, corresponde a la calificación de los estudiantes que no dedican ningún tiempo de estudio. En este caso sería \\(5\\) (aproximadamente).\n\n\n\n\n\n\n2.1.4 Revisión y diagnóstico del modelo\nUna vez ajustado el modelo, es crucial evaluar su calidad. Esto implica analizar si los supuestos del modelo se cumplen, como la linealidad, la homocedasticidad y la independencia de los errores.\nDeberemos realizar las siguientes tareas:\n\nComparar los valores predichos por el modelo con los datos observados para verificar su ajuste.\nExaminar los residuos (errores) para identificar posibles problemas, como tendencias no capturadas o varianzas no constantes.\n\n\n\n2.1.5 Reajuste del modelo\nSi el diagnóstico revela deficiencias en el modelo ajustado, se plantean modificaciones. Estas pueden incluir:\n\nTransformaciones de variables para mejorar la linealidad.\nIntroducción de términos adicionales, como variables cuadráticas o interacciones.\nCambios a modelos no lineales si el comportamiento de los datos lo requiere.\n\n\n\n\n\n\n\nEstudios de Galton sobre estatura\n\n\n\nLos estudios de Sir Francis Galton sobre la estatura son un ejemplo clásico en estadística y forman parte de la historia de la regresión lineal. Galton, un polímata británico del siglo XIX, investigó la herencia biológica y publicó en 1889 su libro Natural Inheritance, donde analizó datos sobre la relación entre las estaturas de padres e hijos. Estos estudios no solo sentaron las bases para la regresión lineal, sino que también ayudaron a formalizar conceptos clave en estadística, haciendo de Galton una figura central en su desarrollo.\nContexto y propósito\nGalton estaba interesado en cómo las características físicas, como la estatura, se transmiten de padres a hijos. Su objetivo era cuantificar esta relación y establecer patrones de herencia. En particular, buscó responder si los hijos de padres altos tienden a ser más altos y si los de padres bajos tienden a ser más bajos.\nDatos recopilados\n\nGalton recopiló datos sobre las estaturas de 928 hijos y sus respectivos padres.\nLas medidas fueron expresadas en pulgadas (1 pulgada = 2.54 cm).\n\nEn sus análisis, utilizó el promedio de las estaturas de ambos padres, conocido como estatura media parental, para compararlo con la estatura de los hijos.\n\nPrincipales hallazgos\n\nRelación lineal entre padres e hijos:\nGalton observó que existe una relación positiva entre la estatura de los padres y la de los hijos. Los padres altos tienden a tener hijos altos, y los padres bajos tienden a tener hijos bajos. Esta relación puede modelarse con una línea recta, lo que inspiró la formulación de la regresión lineal.\nRegresión a la media:\n\nAunque los hijos de padres altos son, en promedio, más altos que el promedio general de la población, también tienden a ser menos altos que sus padres.\n\nDe manera similar, los hijos de padres bajos son más bajos que el promedio general, pero suelen ser menos bajos que sus padres.\n\nEste fenómeno, que Galton llamó “regresión a la media”, ocurre porque las características extremas tienden a suavizarse en la siguiente generación debido a la influencia de múltiples factores genéticos y ambientales.\n\nEcuación de la recta de regresión:\nGalton ajustó una recta para describir la relación entre la estatura media parental (\\(X\\)) y la estatura de los hijos (\\(Y\\)): \\[\nY = \\beta_0 + \\beta_1 X\n\\] Donde:\n\n\\(\\beta_0\\): Intercepto, representa la estatura promedio de los hijos cuando la estatura parental es promedio.\n\\(\\beta_1\\): Pendiente, indica cómo cambia la estatura de los hijos por cada unidad de cambio en la estatura media parental.\n\n\nImportancia en la Estadística\n\nRegresión lineal:\nEste estudio introdujo el concepto de recta de regresión, que describe cómo varía la media de una variable dependiente en función de una variable independiente.\nCorrelación:\nGalton también estudió el grado de relación entre variables, precursor del concepto de coeficiente de correlación desarrollado posteriormente por Karl Pearson, un discípulo suyo.\nRegresión a la media:\nEl término y la idea detrás de “regresión a la media” surgieron de estos estudios y son hoy fundamentales en estadística y genética.\n\nEjemplo Gráfico\nGalton representó sus datos en gráficos de dispersión, mostrando cómo los puntos (pares de estatura media parental y estatura de los hijos) se agrupan alrededor de la recta de regresión, ilustrando la tendencia general de la relación.\n\n# Instalar y cargar los paquetes necesarios\nlibrary(ggplot2)\nlibrary(HistData)\n\n# Cargar los datos de Galton\ndata(\"GaltonFamilies\")\ngalton_data &lt;- GaltonFamilies\n\n# Crear el modelo de regresión lineal\nmodelo &lt;- lm(childHeight ~ midparentHeight, data = galton_data)\n\n# Crear el gráfico con ggplot2\ngrafico &lt;- ggplot(galton_data, aes(x = midparentHeight, y = childHeight)) +\n  geom_point() +  # Añadir puntos de datos\n  geom_smooth(method = \"lm\", col = \"red\") +  # Añadir la recta de regresión\n  labs(title = \"Altura de Padres e Hijos (Datos de Galton)\",\n       x = \"Altura de los Padres\",\n       y = \"Altura de los Hijos\") +\n  annotate(\"text\", x = 67, y = 75, label = paste(\"y =\", round(coef(modelo)[1], 2), \"+\", round(coef(modelo)[2], 2), \"x\"), color = \"red\")\n\n# Mostrar el gráfico\nprint(grafico)\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema1.html#correlación",
    "href": "tema1.html#correlación",
    "title": "2  El modelo de regresión lineal simple",
    "section": "2.2 Correlación",
    "text": "2.2 Correlación\nLa correlación es una medida estadística que describe la relación entre dos variables. Permite evaluar si existe una asociación entre ellas y en qué grado los cambios en una variable están relacionados con los cambios en la otra. En el contexto de modelos de regresión, la correlación es un paso fundamental para explorar la fuerza y la dirección de la relación entre las variables explicativas y la variable respuesta.\n\n\n\n\n\n\nConcepto de correlación\n\n\n\n\n\nLa correlación responde a preguntas como:\n\n¿A valores altos de una variable le corresponden valores altos de la otra? (correlación positiva).\n¿A valores altos de una variable le corresponden valores bajos de la otra? (correlación negativa).\n¿No existe un patrón evidente de asociación? (ausencia de correlación).\n\n\n\n\n\n2.2.1 Covarianza\nLa covarianza es una medida que cuantifica cómo varían conjuntamente dos variables. Su fórmula es:\n\\[\n\\text{Cov}(X, Y) = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{n}\n\\]\n\nSi la covarianza es positiva, indica una tendencia a que ambas variables aumenten juntas (relación positiva).\nSi es negativa, una variable tiende a aumentar mientras la otra disminuye (relación negativa).\n\nUna covarianza cercana a cero sugiere que no hay una relación lineal significativa.\n\nSin embargo, la covarianza tiene una limitación: su valor depende de las unidades de medida de las variables, lo que dificulta su interpretación directa.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Cargar los datos de Galton\ndata(\"GaltonFamilies\")\ngalton_data &lt;- GaltonFamilies\n\n# Seleccionar las variables de interés\nmidparent_height &lt;- galton_data$midparentHeight\nchild_height &lt;- galton_data$childHeight\n\n# Calcular la covarianza\ncovarianza &lt;- round(cov(midparent_height, child_height),3)\n\n# Mostrar el resultado\nprint(paste(\"La covarianza entre la altura de los padres y la altura de los hijos es:\", covarianza))\n\n[1] \"La covarianza entre la altura de los padres y la altura de los hijos es: 2.07\"\n\n\n\n\n\n\n\n2.2.2 Coeficiente de correlación lineal\nPara superar la limitación anterior, se utiliza el coeficiente de correlación lineal de Pearson (\\(r\\)), que estandariza la covarianza dividiéndola por las desviaciones típicas de las variables:\n\\[\nr(X, Y) = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\cdot \\sigma_Y}\n\\]\nEl coeficiente de correlación es adimensional y toma valores entre \\(-1\\) y \\(1\\), facilitando su interpretación sin importar las unidades de las variables.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Cargar los datos de Galton\ndata(\"GaltonFamilies\")\ngalton_data &lt;- GaltonFamilies\n\n# Seleccionar las variables de interés\nmidparent_height &lt;- galton_data$midparentHeight\nchild_height &lt;- galton_data$childHeight\n\n# Calcular la covarianza\ncorrelacion_pearson &lt;- round(cor(midparent_height, child_height),2)\n\n# Mostrar el resultado\nprint(paste(\"El coeficiente de correlación de Pearson entre la altura de los padres y la altura de los hijos es:\", correlacion_pearson))\n\n[1] \"El coeficiente de correlación de Pearson entre la altura de los padres y la altura de los hijos es: 0.32\"\n\n\n\n\n\nInterpretación del Coeficiente de Correlación\n\nMagnitud:\n\nValores cercanos a \\(|1|\\) indican una relación lineal fuerte.\n\nValores cercanos a \\(0\\) sugieren una relación lineal débil o inexistente.\n\nSigno:\n\nPositivo: Ambas variables tienden a moverse en la misma dirección.\n\nNegativo: Las variables tienden a moverse en direcciones opuestas.\n\n\nRelación lineal y otras relaciones\nEs importante destacar que una correlación de \\(r = 0\\) no implica necesariamente que no haya relación entre las variables. Puede haber una relación no lineal que el coeficiente de correlación lineal no detecta.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nPor ejemplo, en un gráfico de dispersión en forma de parábola, el coeficiente de correlación lineal podría ser cercano a 0, a pesar de que existe una relación cuadrática clara entre las variables.\n\n# Generar datos de ejemplo\nset.seed(0)\nx &lt;- seq(-10, 10, length.out = 100)\ny &lt;- x^2 + rnorm(100, mean = 0, sd = 10)\n\n# Calcular el coeficiente de correlación de Pearson\ncorrelacion_pearson &lt;- cor(x, y)\n\n# Crear el gráfico de dispersión\nplot(x, y, main = \"Gráfico de Dispersión con Relación Cuadrática\",\n     xlab = \"X\", ylab = \"Y\", pch = 19)\nabline(h = 0, v = 0, col = \"gray\", lty = 2)\n\n# Mostrar el coeficiente de correlación en el gráfico\ntext(0, 80, paste(\"Correlación de Pearson:\", round(correlacion_pearson, 2)), col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.3 Aplicación práctica\nLa correlación es una herramienta inicial esencial en la exploración de datos:\n\nIdentifica variables explicativas potenciales para modelos de regresión.\n\nAyuda a entender la estructura de los datos y a verificar si una relación lineal es razonable.\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nEn un análisis del tiempo de estudio semanal y las calificaciones, se puede calcular el coeficiente de correlación \\(r\\) para determinar si los estudiantes que estudian más tienden a obtener mejores resultados.\n\n# Generar datos de ejemplo\n# Calcular el coeficiente de correlación de Pearson\ncoef_correlacion &lt;- round(cor(datos$Tiempo_Estudio, datos$Calificaciones),3)\n\n# Mostrar el coeficiente de correlación\ncat(\"El coeficiente de correlación entre el tiempo de estudio y las calificaciones es:\", coef_correlacion, \"\\n\")\n\nEl coeficiente de correlación entre el tiempo de estudio y las calificaciones es: 0.898",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema1.html#regresión-lineal-simple",
    "href": "tema1.html#regresión-lineal-simple",
    "title": "2  Modelo de regresión lineal: simple y múltiple",
    "section": "2.3 Regresión lineal simple",
    "text": "2.3 Regresión lineal simple\nLa regresión lineal simple es una de las herramientas más fundamentales y ampliamente utilizadas en el análisis estadístico. Su objetivo principal es modelar la relación entre dos variables: una variable explicativa (independiente) y una variable respuesta (dependiente). Este modelo permite no solo describir cómo se relacionan estas dos variables, sino también realizar predicciones basadas en dicha relación.\nEl concepto básico de la regresión lineal simple es ajustar una recta que minimice las discrepancias entre los valores observados y los predichos por el modelo. La ecuación general de este modelo es:\n\\[\nY = \\beta_0 + \\beta_1 X + \\varepsilon,\n\\] donde:\n\n\\(Y\\) es la variable dependiente o respuesta.\n\n\\(X\\) es la variable independiente o explicativa.\n\nLa ecuación recibe el nombre de recta de regresión.\n\nLos coeficiente \\(\\beta_0\\) y \\(\\beta_1\\) reciben el nombre de coeficientes del modelo de regresión.\n\n\\(\\beta_0\\) es el intercepto, que representa el valor de \\(Y\\) cuando \\(X = 0\\).\n\n\\(\\beta_1\\) es la pendiente, que indica el cambio esperado en \\(Y\\) por cada unidad de cambio en \\(X\\).\n\n\n\\(\\varepsilon\\) es un término de error que captura la variabilidad no explicada por el modelo.\n\nPara cada valor fijo de \\(X\\), \\(Y\\) es una variable aleatoria, es decir, \\(Y\\) sigue una distribución de probabilidad para cada valor fijo de \\(X\\), con media o valor esperado: \\[\nE[Y|X]=\\beta_0+\\beta_1X,\n\\] y con varianza: \\[\nVar(Y|X)=Var(\\beta_0+\\beta_1X+\\varepsilon)=Var(\\varepsilon)=\\sigma^2.\n\\]\n\n\n\n\n\n\nPropiedades Clave\n\n\n\n\n\nPara que las inferencias del modelo sean válidas, se requieren ciertos supuestos sobre los datos y el término de error \\(\\varepsilon\\):\n\nLinealidad: La relación entre \\(X\\) y \\(Y\\) es lineal. Es decir, la esperanza de \\(Y\\) es una función lineal de \\(X\\).\n\nIndependencia: Los términos de error \\(\\varepsilon\\) son independientes entre sí. Los errores para observaciones distintas son incorrelados, es decir \\(\\varepsilon_i\\) y \\(\\varepsilon_j\\) son no correlados y, por tanto, también lo son \\(Y_i\\) e \\(Y_j\\).\nHomocedasticidad: La varianza de los términos de error \\(\\varepsilon\\) es constante para todos los valores de \\(X\\). Es decir, la varianza de \\(Y\\) es constante, no depende \\(X\\).\n\n\nCuando el objetivo no es sólo estimar la recta, sino inferir con ella, entonces se asume una hipótesis más: la normalidad de la variable respuesta, o lo que es lo mismo, del error aleatorio:\n\nNormalidad: Los términos de error \\(\\varepsilon\\) siguen una distribución normal con media cero y varianza constante:\n\\[\n    \\varepsilon_i \\overset{\\mathrm{iid}}{\\sim} N(0, \\sigma^2) , \\hspace{0.5cm} i=1,\\ldots,n\n    \\]\n\nEstos supuestos son esenciales para garantizar la validez de las estimaciones y conclusiones derivadas del modelo.\n\n\n\nEn resumen, el modelo de regresión lineal simple, implica que las respuestas \\(Y_i\\) proceden de una distribución de probabilidad con esperanza: \\[\nE[Y_i|X_i]=\\beta_0+\\beta_1X_i \\hspace{.5cm} i=1,\\dots,n,\n\\] y con varianza \\(\\sigma^2\\), para todas las \\(Y\\). Además, cualesquiera dos respuestas \\(Y_i\\) e \\(Y_j\\) son incorreladas.\nLa regresión lineal simple es fundamental en estadística y ciencia de datos porque proporciona un marco intuitivo y matemáticamente riguroso para analizar relaciones. Algunas aplicaciones comunes incluyen:\n\nPredecir valores futuros, como ingresos o gastos, en función de un predictor.\n\nEvaluar el impacto de una variable en otra, como el efecto de la inversión publicitaria en las ventas.\n\nExplorar relaciones lineales entre variables en estudios científicos o sociales.\n\nEn esta sección, exploraremos los fundamentos de la regresión lineal simple, desde su formulación teórica hasta su implementación práctica. Veremos cómo ajustar este modelo, interpretar sus parámetros, y evaluar su adecuación utilizando herramientas estadísticas y gráficas. Además, se presentarán ejemplos prácticos para ilustrar su aplicación en situaciones reales.\n\n2.3.1 Estimación de los parámetros del modelo\nLos parámetros del modelo (\\(\\beta_0\\) y \\(\\beta_1\\)) se estiman utilizando el método de mínimos cuadrados. Llamaremos \\(\\hat{\\beta_0}\\) y \\(\\hat{\\beta_1}\\) a los estimadores de los parámetros.\nLlamaremos valor predicho (\\(\\hat{Y}\\)):\n\\[\n  \\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i,\n  \\] al valor estimado de \\(Y\\) para un valor dado de \\(X\\).\nY nos referimos al residuo (\\(e_i\\)):\n\\[\n  e_i = Y_i - \\hat{Y}_i,\n  \\] para medir la discrepancia entre el valor observado \\(Y_i\\) y el valor predicho \\(\\hat{Y}_i\\).\nEl método de los mínimos cuadrados busca minimizar la suma de los errores cuadráticos (residuos) entre los valores observados de \\(Y\\) y los valores predichos (\\(\\hat{Y}\\)):\n\\[\n\\text{SSE} = \\sum_{i=1}^n e_i^2 =\\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2\n= \\sum_{i=1}^{n} \\left(Y_i - (\\hat{\\beta_0} + \\hat{\\beta_1}\nX_i)\\right)^2.\n\\]\n\n\n\n\n\n\nMinimización de SSE\n\n\n\n\n\nLa obtención de los estimadores de mínimos cuadrados para la regresión lineal simple se basa en minimizar la suma de los cuadrados de los residuos (\\(SSE\\)). Aquí está el proceso paso a paso:\nPara minimizar \\(SSE\\), derivamos parcialmente con respecto a \\(\\beta_0\\) y \\(\\beta_1\\) y resolvemos el sistema de ecuaciones.\n\nPrimera derivada con respecto a \\(\\beta_0\\):\n\n\\[\n    \\frac{\\partial SSE}{\\partial \\beta_0} = -2\\sum_{i=1}^n\n    \\left(Y_i - (\\beta_0 + \\beta_1 X_i)\\right).\n  \\]\nIgualando a cero: \n\\[\n    \\sum_{i=1}^n \\left(Y_i - \\beta_0 - \\beta_1\n    X_i\\right) = 0.\n  \\]\nReordenando: \n\\[\n    n\\beta_0 + \\beta_1 \\sum_{i=1}^n X_i = \\sum_{i=1}^n Y_i. \\tag{1}\n  \\]\n\nPrimera derivada con respecto a \\(\\beta_1\\):\n\n\\[\n    \\frac{\\partial SSE}{\\partial \\beta_1} = -2\\sum_{i=1}^n X_i\n    \\left(Y_i - (\\beta_0 + \\beta_1 X_i)\\right).\n    \\]\nIgualando a cero: \n\\[\n    \\sum_{i=1}^n X_i \\left(Y_i - \\beta_0 -\n    \\beta_1 X_i\\right) = 0.\n   \\]\nReordenando: \n\\[\n    \\beta_0 \\sum_{i=1}^n X_i + \\beta_1 \\sum_{i=1}^n X_i^2 = \\sum_{i=1}^n X_i Y_i. \\tag{2}\n   \\]\nResolución del Sistema de Ecuaciones\nEl sistema está dado por las ecuaciones (1) y (2):\n\n\\(n\\beta_0 + \\beta_1 \\sum_{i=1}^n X_i = \\sum_{i=1}^n Y_i.\\)\n\n\\(\\beta_0 \\sum_{i=1}^n X_i + \\beta_1 \\sum_{i=1}^n X_i^2 = \\sum_{i=1}^n X_i Y_i.\\)\n\nResolviendo para \\(\\beta_0\\) y \\(\\beta_1\\):\n\nDe la primera ecuación, despejamos \\(\\beta_0\\):\n\\[\n\\beta_0 = \\frac{\\sum_{i=1}^n Y_i - \\beta_1 \\sum_{i=1}^n X_i}{n}. \\tag{3}\n\\]\nSustituimos \\(\\beta_0\\) en la segunda ecuación:\n\\[\n\\frac{\\sum_{i=1}^n Y_i - \\beta_1 \\sum_{i=1}^n X_i}{n} \\sum_{i=1}^n X_i + \\beta_1 \\sum_{i=1}^n X_i^2 = \\sum_{i=1}^n X_i Y_i.\n\\]\nSimplificando:\n\\[\n\\beta_1 \\left(\\sum_{i=1}^n X_i^2 - \\frac{(\\sum_{i=1}^n X_i)^2}{n}\\right) = \\sum_{i=1}^n X_i Y_i - \\frac{\\sum_{i=1}^n X_i \\sum_{i=1}^n Y_i}{n}.\n\\]\nExpresamos \\(\\beta_1\\):\n\\[\n\\beta_1 = \\frac{\\sum_{i=1}^n X_i Y_i - \\frac{\\sum_{i=1}^n X_i \\sum_{i=1}^n Y_i}{n}}{\\sum_{i=1}^n X_i^2 - \\frac{(\\sum_{i=1}^n X_i)^2}{n}}.\n\\] Esta es la fórmula para \\(\\beta_1\\), que puede reescribirse como:\n\\[\n\\beta_1 = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)},\n\\] donde \\(\\text{Cov}(X, Y)\\) y \\(\\text{Var}(X)\\) son la covarianza y la varianza muestral de \\(X\\) y \\(Y\\).\nFinalmente, sustituimos \\(\\beta_1\\) en la ecuación (3) para obtener \\(\\beta_0\\):\n\\[\n\\beta_0 = \\bar{Y} - \\beta_1 \\bar{X},\n\\] donde \\(\\bar{X}\\) y \\(\\bar{Y}\\) son las medias de \\(X\\) y \\(Y\\).\n\n\n\n\nLas fórmulas para los estimadores de mínimos cuadrados son:\n\\[\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)} = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2},\n\\] \\[\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{X},\n\\] donde \\(\\bar{X}\\) y \\(\\bar{Y}\\) son las medias de \\(X\\) y \\(Y\\), respectivamente.\n\n\n\n\n\n\nEjercicio\n\n\n\n\n\nAsumiendo la normalidad de la variable respuesta: \\[\n    \\varepsilon_i \\overset{\\mathrm{iid}}{\\sim} N(0, \\sigma^2) \\Leftrightarrow Y_i \\overset{\\mathrm{iid}}{\\sim} N(\\beta_0+\\beta_1X_i,\\sigma^2), \\hspace{0.5cm} i=1,\\ldots,n.\n    \\]\nSe tiene entonces la verosimilud para \\(\\beta=(\\beta_0,\\beta_1)\\),\n\\[\nL(\\beta; Y)=exp \\left ( -\\frac{\\sum_{i=1}^{n}(Y_i-\\beta_0-\\beta_1X_i)^2}{2\\sigma^2} \\right).\n\\]\nEn este ejercicio, pedimos buscar los valores de \\(\\beta\\) tales que maximizan la verosimilud. Esto es, los estimadores máximo verosimiles. Para ello derivamos e igualamos a cero. Se pide demostrar que los estimadores máximo verosimiles coinciden con los obtenidos por mínimos cuadrados.\n\n\n\n\n\n2.3.2 Propiedades de los estimadores\nEstudiemos algunas de las propiedades de los estimadores de mı́nimos cuadrados y del modelo de regresión ajustado.\n\nSean: \\[\nS_{xx}=\\sum_{i=1}^n (X_i-\\bar{X})^2\n\\] y \\[\nS_{xy}=\\sum_{i=1}^n (X_i-\\bar{X})(Y_i-\\bar{Y})\n\\] Entonces: \\[\n\\hat{\\beta_1}=\\frac{S_{xy}}{S_{xx}}\n\\]\nLos estimadores de \\(\\beta_0\\) y \\(\\beta_1\\) son insesgados, es decir: \\[\nE[\\hat{\\beta_0}]=\\beta_0, \\space E[\\hat{\\beta_1}]=\\beta_1\n\\]\nLas varianzas de los estimadores de \\(\\beta_0\\) y \\(\\beta_1\\) son: \\[\nVar(\\hat{\\beta_1})=\\frac{\\sigma^2}{S_{xx}}\n\\] donde \\(\\sigma^2\\) es la varianza del error \\(\\varepsilon\\). En la práctica, como \\(\\sigma^2\\) no se conoce, se estima con la varianza residual (\\(\\hat{\\sigma}^2\\)): \\[\n\\hat{\\sigma}^2=\\frac{SSE}{n-2},\n\\] siendo \\(SSE=\\sum_{i=1}^{n}(Y_i-\\hat{Y_i})^2\\), la suma de los cuadrados de los residos y \\(n-2\\) los grados de libertad (por estimar dos parámetros). De modo que:\n\n\\[\nVar(\\hat{\\beta_1}) \\approx \\frac{\\hat{\\sigma}^2}{S_{xx}}\n\\]\nDe igual modo tenemos: \\[\nVar(\\hat{\\beta_0}) \\approx \\hat{\\sigma}^2 \\left ( \\frac{1}{n}+\\frac{\\bar{X}^2}{S_{xx}}\\right )\n\\]\n\nTeorema de Gaus-Markov: Para el modelo de regresión con \\(E[\\varepsilon] = 0\\), \\(Var(\\varepsilon) = \\sigma^2\\) y los errores incorrelados, se tiene que los estimadores \\(\\hat{\\beta_0}\\) y \\(\\hat{\\beta_1}\\) son insesgados y de mı́nima varianza. Demostración en (Kutner et al. 2005).\n\n\n\n\n\n\n\nPropiedades adicionales para las predicciones y para los residuos\n\n\n\n\n\n\nLa suma de los residuos es cero: \\[\n  \\sum_{i=1}^n e_i=\\sum_{i=1}^n(Y_i-\\hat{Y_i})=0\n  \\]\nLa suma de los valores observados es igual a la suma de los valores ajustados: \\[\n  \\sum_{i=1}^n Y_i=\\sum_{i=1}^n \\hat{Y_i}\n  \\]\nLa suma de los residuos ponderados por los regresores es cero: \\[\n  \\sum_{i=1}^n X_ie_i=0\n  \\]\nLa suma de los residuos ponderados por las predicciones es cero: \\[\n  \\sum_{i=1}^n \\hat{Y_i}e_i=0\n  \\]\nLa recta de regresión contiene el punto \\((\\bar{X},\\bar{Y})\\):\n\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nPara los datos de calificaciones y tiempo de estudio, estos son los estimadores de los parámetros del modelo de regresión:\n\n# Ajustar el modelo de regresión lineal\nmodelo &lt;- lm(calificaciones ~ tiempo_estudio)\n\n# Obtener estimadores de los parámetros y errores estándar\nestimadores &lt;- coef(summary(modelo))  # Incluye estimadores y errores estándar\n\n# Imprimir los resultados\ncat(\"Estimadores para los parámetros del modelo:\\n\")\n\nEstimadores para los parámetros del modelo:\n\nprint(estimadores)\n\n                 Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept)    5.00117598 0.11976840 41.75706 3.102826e-64\ntiempo_estudio 0.09874923 0.00488005 20.23529 9.033965e-37\n\n\n\n\n\nUna vez obtenida la recta de regresión, surgen una serie de preguntas interesantes:\n\n¿Cómo de bien describe la ecuación de regresión los datos observados?\n¿Podemos emplear este modelo para predecir nuevas observaciones?\n¿Se cumplen todas las suposiciones del modelo?\n\nTodas estas cuestiones serán analizadas antes de adoptar el modelo como válido. Una herramienta muy importante en la validación del modelo será el estudio de los residuos. Lo veremos más adelante.\n\n\n2.3.3 Inferencia sobre los parámetros del modelo\nEn regresión, con frecuencia interesa realizar contraste de hipótesis o construir intervalos de confianza para los parámetros del modelo.\nPara hacer inferencias consideramos la distribución en el muestreo de los estimadores de dichos parámetros. Este procedimiento requiere que tengamos en cuenta la suposición de normalidad sobre los errores \\(\\varepsilon_i\\) , es decir: \\[\n    \\varepsilon_i \\overset{\\mathrm{iid}}{\\sim} N(0, \\sigma^2)\n\\] Entonces: \\[\nY_i\\sim N(\\beta_0+\\beta_1X,\\sigma^2),\n\\] y se puede demostrar que: \\[\n\\hat{\\beta_1} \\sim N\\left (\\beta_1,\\frac{\\sigma^2}{S_{xx}}\\right),\n\\] \\[\n\\hat{\\beta_0} \\sim N\\left ( \\beta_0, \\sigma^2 \\left ( \\frac{1}{n} + \\frac{\\bar{X}^2}{S_{xx}} \\right) \\right )\n\\]\nPuesto que las varianzas de los parámetros1 dependen de \\(\\sigma^2\\) , cuando el modelo de regresión es adecuado, podemos estimarlas sustituyendo dicho valor por su estimador insesgado \\(s^2\\). En este caso, la distribución de los parámetros ya no será normal, al igual que ocurrı́a en la distribución del valor esperado \\(\\mu\\) en una población i.i.d., aparece la distribución t-Student: \\[\nt=\\frac{\\hat{\\beta_1}-\\beta_1}{SE({\\hat{\\beta_1}})}\\sim t_{n-2},\n\\] siendo \\(SE(\\hat{\\beta_1})=\\frac{s}{\\sqrt{S_{xx}}}\\), es una t-Student con \\(n-2\\) grados de ligertad.\n\n2.3.3.1 Intervalo de confianza\nAsí, su intervalo de confianza al \\(1-\\alpha\\%\\) calculado a partir de la distribución en el muestro es: \\[\nIC(\\beta_1;1-\\alpha)=\\hat{\\beta_1} \\pm t_{1-\\alpha/2,n-2}\\sqrt{\\frac{s^2}{S_{xx}}},\n\\] donde \\(t_{1-\\alpha/2,n-2}\\) es el cuantil \\(1-\\alpha/2\\) de una distribución \\(t\\) con \\(n-2\\) grados de libertad (los correspondientes a \\(s^2\\)).\n\n\n2.3.3.2 Contraste de hipótesis\nEn regresión, normalmente estamos interesado en realizar el siguiente contraste de hipótesis: \\[\nH_0:\\beta_1=0 \\text{ frente a } H_1: \\beta_1\\neq 0.\n\\] De este modo, si no podemos rechazar la hipótesis nula, podemos concluir que no existe prueba (en los datos) de una relación lineal significativa entre las variables estudiadas. Este tipo de contraste nos será muy útil cuando trabajemos con modelos con más de una variable para eliminar aquellas que no son importantes dentro del mismo.\nEn el caso del modelo de regresión lineal simple, no rechazar \\(H_0\\) implica que la mejor predicción para todas las observaciones es: \\(\\hat{Y_i}=Y_i\\), o bien que la relación entre las variables \\(X\\) e \\(Y\\) no es lineal y, entonces, demos corregir nuestro modelo.\nPor otro lado, rechazar la hipótesis nula en favor de la alternativa, significa que la variable \\(X\\) influye al explicar la variabilidad de la variable \\(Y\\). Quizás el modelo no sea el más adecuado, pero no podemos eliminar la variable del mismo.\nPara realizar el contraste, como \\(\\varepsilon_i\\) sin variables i.i.d. tales que \\(\\varepsilon_i\\sim N(0,\\sigma^2)\\), entonces:\n\\[\nY_i\\sim N(\\beta_0+\\beta_1X,\\sigma^2), \\text{ } \\hat{\\beta_1} \\sim N\\left (0,\\frac{\\sigma^2}{S_{xx}}\\right)\n\\]\nDe manera que si, como es lo habitual, \\(\\sigma^2\\) es desconocida, como \\(E[\\hat{\\sigma^2}]=\\sigma^2\\), se tiene que, bajo la hipótesis nula: \\[\nt=\\frac{\\hat{\\beta_1}}{SE({\\hat{\\beta_1}})}\\sim t_{n-2},\n\\] donde \\(SE({\\hat{\\beta_1}})=\\sqrt{\\frac{\\hat{\\sigma^2}}{S_{xx}}}\\), es el error estándar estimado del parámetro \\(\\hat{\\beta_1}\\). De este modo, la hipótesis nula será rechazada si \\(|t|&gt;t_{\\alpha/2,n-2}\\), siendo \\(\\alpha\\), el nivel de significación del contraste.\n\n\n\n\n\n\nPara recordar\n\n\n\nEn los programas estadı́sticos se suele proporcionar el p-valor del contraste. Puedes repasar el significado de p-valor proporcionado en la asignatura de Inferencia.\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nA continuación se presenta un resumen del modelo. Podemos ver los p-valores asociados a cada uno de los parámetros del modelo.\n\n# Ajustar el modelo de regresión lineal\nmodelo &lt;- lm(calificaciones ~ tiempo_estudio)\n\n# Obtener resumen del modelo\nsummary(modelo)\n\n\nCall:\nlm(formula = calificaciones ~ tiempo_estudio)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.11465 -0.30262 -0.00942  0.29509  1.10533 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     5.00118    0.11977   41.76   &lt;2e-16 ***\ntiempo_estudio  0.09875    0.00488   20.23   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4842 on 98 degrees of freedom\nMultiple R-squared:  0.8069,    Adjusted R-squared:  0.8049 \nF-statistic: 409.5 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\n2.3.4 Descomposición de la varianza: ANOVA\nLa descomposición de la varianza es un paso crucial para evaluar la calidad del ajuste del modelo de regresión lineal simple. A través de un análisis de varianza (ANOVA), se divide la variabilidad total de la variable respuesta (\\(Y\\)) en componentes atribuibles al modelo y al error, proporcionando un marco para evaluar si la relación entre \\(X\\) e \\(Y\\) es estadísticamente significativa. Un modelo es bueno si la variabilidad explicada es mucha, o lo que es lo mismo, si las diferencias entre los datos y las predicciones según el modelo son pequeñas.\n\n\n\n\n\n\nRepaso\n\n\n\nEs conveniente repasar el tema de Análisis de la Varianza estudiado en la asignatura de Inferencia.\n\n\n\n2.3.4.1 Variabilidad total\nLa variabilidad total de\\(Y\\) se mide mediante la Suma Total de los Cuadrados (SST):\n\\[ SST = \\sum_{i=1}^n (Y_i - \\bar{Y})^2,\n\\]\ndonde \\(\\bar{Y}\\) es la media de \\(Y\\). \\(SST\\) refleja la dispersión general de \\(Y\\) respecto a su media.\nDescomposición de la variabilidad\nEl modelo de regresión lineal permite descomponer\\(SST\\) en dos componentes principales:\n\\[ SST = SSR + SSE,\n\\]\ndonde:\n\n\\(SSR\\) (Suma de los Cuadrados del Modelo): Representa la variabilidad explicada por la regresión, es decir, la parte de \\(Y\\) que se puede predecir a partir de \\(X\\):\n\\[ SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2,\n\\] donde \\(\\hat{Y}_i\\) es el valor predicho por el modelo para el \\(i\\)-ésimo dato.\n\\(SSE\\) (Suma de los Cuadrados de los Errores): Como hemos visto al inicio de esta sección, representa la variabilidad no explicada por el modelo, es decir, la dispersión de los valores observados respecto a los valores predichos:\n\\[ SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2.\n\\]\n\n\n\n2.3.4.2 Tabla ANOVA\nEl análisis de varianza organiza la descomposición de la varianza en una tabla, donde cada componente se asocia con sus grados de libertad (\\(df\\)), suma de cuadrados (\\(SS\\)), media cuadrática (\\(MS\\)) y el estadístico \\(F\\):\n\n\n\nFuente\n\\(df\\)\n\\(SS\\)\n\\(MS = SS/df\\)\nEstadístico\\(F\\)\n\n\n\n\nRegresión\n1\n\\(SSR\\)\n\\(MSR = SSR/1\\)\n\\(F = MSR/MSE\\)\n\n\nError\n\\(n-2\\)\n\\(SSE\\)\n\\(MSE = SSE/(n-2)\\)\n\n\n\nTotal\n\\(n-1\\)\n\\(SST\\)\n\n\n\n\n\n\n\n2.3.4.3 Prueba de significancia global\nPara evaluar si \\(X\\) tiene un efecto significativo sobre \\(Y\\), se utiliza el estadístico \\(F\\):\n\\[\nF = \\frac{MSR}{MSE}.\n\\]\n\nBajo la hipótesis nula (\\(H_0: \\beta_1 = 0\\)), \\(F\\) sigue una distribución \\(F\\) con 1 y \\(n-2\\) grados de libertad.\nSi el valor \\(p\\) asociado al estadístico \\(F\\) es pequeño (\\(p &lt; \\alpha\\)), se rechaza \\(H_0\\), indicando que \\(X\\) tiene un efecto significativo sobre \\(Y\\).\n\nEl análisis ANOVA permite responder preguntas clave:\n\n¿Qué proporción de la variabilidad de \\(Y\\) es explicada por \\(X\\)?\n\n¿Es significativa esta relación desde el punto de vista estadístico?\n\nEn el contexto de la regresión lineal simple, esta herramienta no solo cuantifica el ajuste del modelo, sino que también valida su relevancia estadística.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nA continuación se presenta la tabla ANOVA correspondiente al modelo de horas de estudio y calificaciones, previamente entrenado.\n\n# Ajustar el modelo de regresión lineal\nmodelo &lt;- lm(calificaciones ~ tiempo_estudio)\n\n# Obtener y mostrar la tabla ANOVA\ntabla_anova &lt;- anova(modelo)\ncat(\"Tabla ANOVA:\\n\")\n\nTabla ANOVA:\n\nprint(tabla_anova)\n\nAnalysis of Variance Table\n\nResponse: calificaciones\n               Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ntiempo_estudio  1 96.016  96.016  409.47 &lt; 2.2e-16 ***\nResiduals      98 22.980   0.234                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\n2.3.5 Bondad del ajuste: coeficiente de determinación\nEl coeficiente de determinación (\\(R^2\\)) mide qué proporción de la variabilidad total en \\(Y\\) es explicada por \\(X\\) a través del modelo:\n\\[\nR^2 = 1 - \\frac{\\text{Suma de los Cuadrados de los Residuos (SSE)}}{\\text{Suma Total de los Cuadrados (SST)}}\n\\] \\[\n= \\frac{\\text{Suma de los Cuadrados del Modelo (SSR)}}{\\text{Suma Total de los Cuadrados (SST)}}.\n\\]\nDonde:\n\n\\(\\text{SST} = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\\): Variabilidad total en \\(Y\\).\n\n\\(\\text{SSR} = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2\\): Variabilidad explicada por el modelo.\n\n\\(\\text{SSE} = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\): Variabilidad no explicada.\n\nUn \\(R^2\\) cercano a 1 indica que el modelo ajusta bien los datos, mientras que un \\(R^2\\) cercano a 0 indica un ajuste pobre.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nEn el resumen del modelo anterior hemos obtenido el siguiente valor para el coeficiente de determinación \\(R^2=0.8069\\). ¿Cómo evaluas el ajuste obtenido en este modelo?\n\n\n\n\n\n\n\n\n\nObservaciones\n\n\n\n\n\n\n\\(R^2\\) debe ser interpretado con cautela, ya que resultará con frecuencia grande a pesar de que la relación entre \\(X\\) e \\(Y\\) no sea lineal.\nAsı́ por ejemplo, la magnitud de \\(R^2\\) depende del rango de variabilidad de la variable explicativa, \\(X\\). Siendo el modelo de regresión adecuado, la magnitud de \\(R^2\\) aumenta (o disminuye) cuando lo hace la dispersión de \\(X\\).\nAdemás \\(R^2\\) podrı́a ser un valor muy pequeño debido a que el rango de variación de \\(X\\) es demasiado pequeño e impide que se detecte la relación con \\(Y\\).\n\n\n\n\n\n\n2.3.6 Diagnóstico del modelo\nEl diagnóstico del modelo de regresión es un paso esencial para evaluar si los supuestos subyacentes se cumplen y garantizar la validez de las inferencias. El análisis de residuos proporciona información clave sobre la calidad del ajuste del modelo y la adecuación del modelo a los supuestos de la regresión lineal.\nEl análisis de residuos ayuda a verificar los siguientes supuestos del modelo:\n\nLinealidad: La relación entre \\(X\\) e \\(Y\\)es lineal.\nIndependencia: Los residuos son independientes entre sí.\nHomocedasticidad: Los residuos tienen varianza constante.\nNormalidad: Los residuos se distribuyen de manera aproximadamente normal.\n\nUna vez ajustado el modelo, hemos de detectar desviaciones de las hipótesis, es decir, proceder al diagnóstico del modelo. El análisis de los residuos nos permitirá identificar deficiencias en la verificación de estas hipótesis, ası́ como la detección de observaciones anómalas o influyentes.\nRecordemos que los residuos del modelo son las diferencias entre los valores observados y los valores predichos por el modelo:\n\\[\ne_i = Y_i - \\hat{Y}_i.\n\\] Los residuos representan la parte de la variabilidad en \\(Y\\) no explicada por el modelo.\nEn ocasiones, es preferible trabajar con los residuos estandarizados, que tienen media cero y varianza, aproximadamente, uno: \\[\nd_i = \\frac{e_i}{\\sqrt{MSE}},  \\hspace{0.5cm} i=1,\\ldots,n.\n\\] Otro tipo de residuos habitual son los llamados residuos estudentizados: \\[\nr_i = \\frac{e_i}{\\sqrt{MSE \\left ( 1-\\frac{1}{n}-\\frac{(X_i-\\bar{X})^2}{S_{xx}} \\right )}}, \\hspace{0.5cm} i=1,\\ldots,n.\n\\]\nEstos residuos son preferibles a los estandarizados cuando \\(n\\) es pequeño.\n\n2.3.6.1 Pasos en el diagnóstico\nEn primer lugar emplearemos gráficos de los residuos para visualizar cómo se comportan las discrepancias entre los valores observados y los predichos en relación con diversas variables o aspectos del modelo.\nA continuación se ejecutarán pruebas estadísticas (contrastes de hipótesis) para confirmar los hallazgos visuales.\nCorregir los problemas detectados será el último paso. Para ello, y dependiendo del problema, será necesario transformar la variable respuesta, considerar modificaciones del modelo, o el uso de modelos alternativos.\n\n\n\n\n\n\nAviso\n\n\n\nAlgunos científicos y analistas de datos buscan un flujo de trabajo reproducible y aplicable a cualquier conjunto de datos, con la esperanza de establecer un procedimiento estándar para el diagnóstico de modelos, a menudo representado mediante un diagrama de flujo. Sin embargo, esta aproximación puede ser arriesgada. Los datos son inherentemente complejos y variados, y aplicar una guía estricta sin considerar las particularidades de cada conjunto puede conducir a conclusiones erróneas. Aunque existen múltiples herramientas para el diagnóstico de modelos, la elección de una u otra dependerá en gran medida de las características específicas de los datos que estamos analizando. Además, en situaciones del mundo real, es fundamental contar con el asesoramiento de expertos en el dominio para evaluar si las decisiones estadísticas adoptadas tienen sentido en el contexto del problema. La flexibilidad y la interpretación contextual son esenciales para un análisis riguroso y significativo (Isabel y De Diego 2020).\n\n\n\n\n2.3.6.2 Gráficos de residuos\nLa utilidad de los gráficos radica en que ofrecen una manera intuitiva de evaluar si los supuestos clave del modelo de regresión se cumplen, ayudando a identificar problemas que podrían comprometer la validez de las conclusiones. Los gráficos de residuos no solo permiten diagnosticar problemas en el ajuste del modelo, sino también identificar áreas donde el modelo puede ser mejorado. Su uso sistemático en el análisis de regresión asegura un modelo más robusto, fiable y ajustado a los datos, mejorando la calidad de las conclusiones y predicciones.\nLas principales utilidades de estos gráficos son:\n\nEvaluación de la linealidad:\n\nUn gráfico de residuos frente a valores ajustados ayuda a identificar si la relación entre la variable explicativa y la variable respuesta es lineal. La ausencia de patrones sistemáticos indica que el supuesto de linealidad se cumple.\n\nDetección de heterocedasticidad:\n\nSe detectan problemas de heterocedasticidad, si en el gráfico de residuos frente a los valores ajustados se observa un patrón de “embudo” o una variación creciente/disminuyente, es decir, cuando se observa que la varianza de los residuos no es constante para todos los valores de \\(X\\).\n\nVerificación de normalidad:\n\nRecordemos que la hipótesis de normalidad de los residuos es esencial para realizar inferencias estadísticas fiables (intervalos de confianza, pruebas de hipótesis). Los histogramas de residuos o los gráficos QQ-plot permiten detectar desviaciones respecto a la distribución normal. Los residuos estandarizados y estudentizados también son útiles para detectar desviaciones de la normalidad, Si los errores se distribuyen según una normal, entonces aproximadamente el \\(68\\%\\) de los residuos estandarizados (estudentizados) quedarán entre \\(−1\\) y \\(+1\\), y el \\(95\\%\\) entre \\(−2\\) y \\(+2\\).\n\nDetección de observaciones atípicas e influyentes:\n\nLos gráficos de residuos ayudan a identificar puntos atípicos (outliers) o valores con alta influencia que podrían afectar de manera desproporcionada el modelo. Esto es especialmente útil para decidir si ajustar el modelo o realizar un análisis más detallado de estos puntos.\n\nFacilidad para diagnóstico rápido:\n\nLos gráficos de residuos son intuitivos y ofrecen una evaluación visual inmediata de posibles problemas, complementando las pruebas estadísticas formales.\nLos gráficos más comunes para llevar a cabo todos los análisis anteriores son:\n\nResiduos vs. Valores Ajustados: Este gráfico permite verificar la linealidad y la homocedasticidad. Los residuos deben distribuirse aleatoriamente alrededor del cero sin patrones discernibles. Cuando aparece alguna tendencia como una forma de embudo o un abombamiento, etc., podemos tener algún problema con la violación de la hipótesis de varianza constante para los residuos (heterocedasticidad). También pueden aparecer gráficos con cierta tendencia cuando la relación entre \\(X\\) e \\(Y\\) es no lineal. Aunque esto se podrı́a ver en un gráfico de dispersión de (\\(X\\) , \\(Y\\) ), suele ser más evidente en un gráfico con los residuos.\nHistograma de Residuos: Muestra si los residuos tienen una distribución aproximadamente normal.\nQQ-Plot (Gráfico Cuantil-Cuantil): Compara la distribución de los residuos con una distribución normal. Si los puntos se alinean con la diagonal, se cumple la normalidad. Se dibujan los residuos ordenados \\(e_{[i]}\\) frente a los cuantiles correspondientes de una normal estándar, \\(\\phi^{−1}_{[(i − 1)/n]}\\). Si es cierta la normalidad de los residuos, el gráfico resultante deberı́a corresponderse con una recta.\nResiduos vs. Variables Predictoras: Verifica si hay patrones entre los residuos y \\(X\\), lo que indicaría problemas de linealidad o varianza no constante.\n\n\n\n\n\n\n\nPara recordar\n\n\n\nEs análogo hacer el gráfico de los \\(e_i\\) frente a \\(\\hat{Y_i}\\) o \\(X_i\\), ya que los valores ajustados \\(\\hat{Y_i}\\) son función lineal de \\(X_i\\).\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nEstudiamos los residuos del ejemplo de calificaciones de estudiantes.\n\n# Ajustar el modelo de regresión lineal\nmodelo &lt;- lm(calificaciones ~ tiempo_estudio)\n\n# Obtener los residuos\nresiduos &lt;- resid(modelo)\n\n# Estudiar los residuos\n\n# 1. Gráfico de residuos vs. valores ajustados\nvalores_ajustados &lt;- fitted(modelo)\nplot(valores_ajustados, residuos,\n     main = \"Residuos vs Valores Ajustados\",\n     xlab = \"Valores Ajustados\",\n     ylab = \"Residuos\",\n     pch = 19, col = \"blue\")\nabline(h = 0, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n# 2. Histograma de los residuos\nhist(residuos,\n     main = \"Histograma de Residuos\",\n     xlab = \"Residuos\",\n     col = \"lightblue\", border = \"black\")\n\n\n\n\n\n\n\n# 3. QQ-Plot de residuos\nqqnorm(residuos, main = \"QQ-Plot de los Residuos\")\nqqline(residuos, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n# 4. Pruebas de normalidad de los residuos\nshapiro_test &lt;- shapiro.test(residuos)\ncat(\"Prueba de Shapiro-Wilk para normalidad de los residuos:\\n\")\n\nPrueba de Shapiro-Wilk para normalidad de los residuos:\n\nprint(shapiro_test)\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuos\nW = 0.99008, p-value = 0.671\n\n# 5. Gráfico de residuos vs. tiempo de estudio\nplot(tiempo_estudio, residuos,\n     main = \"Residuos vs Tiempo de Estudio\",\n     xlab = \"Tiempo de Estudio (horas/semana)\",\n     ylab = \"Residuos\",\n     pch = 19, col = \"purple\")\nabline(h = 0, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.6.3 Estadísticas para el diagnóstico\nLas pruebas estadísticas son útiles para confirmar los hallazgos visuales obtenidos al analizar los gráficos de los residuos. Las principales pruebas formales a considerar son:\n\nPrueba de normalidad (Shapiro-Wilk): Confirma si los residuos siguen una distribución normal. Un p-valor por debajo del grado de significatividad elegido (típicamente \\(0.05\\)), indica que no podemos rechazar la hipótesis nula de normalidad de los residuos.\nPrueba de autocorrelación (Durbin-Watson): Evalúa la independencia de los residuos.\nPrueba de homocedasticidad (Breusch-Pagan): Detecta si los residuos presentan varianza constante. Un p-valor pequeño (típicamente menor que \\(0.05\\)) sugiere heterocedasticidad.\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nEstudiamos los residuos del ejemplo de calificaciones de estudiantes.\n\n# Ajustar el modelo de regresión lineal\nmodelo &lt;- lm(calificaciones ~ tiempo_estudio)\n\n# Obtener los residuos\nresiduos &lt;- resid(modelo)\n\n# Estadísticas para diagnóstico\n\n# 1. Prueba de normalidad de los residuos (Shapiro-Wilk)\nshapiro_test &lt;- shapiro.test(residuos)\ncat(\"Prueba de Shapiro-Wilk para normalidad de los residuos:\\n\")\n\nPrueba de Shapiro-Wilk para normalidad de los residuos:\n\nprint(shapiro_test)\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuos\nW = 0.99008, p-value = 0.671\n\n# 2. Prueba de homocedasticidad (Breusch-Pagan)\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nbreusch_pagan &lt;- bptest(modelo)\ncat(\"\\nPrueba de Breusch-Pagan para homocedasticidad:\\n\")\n\n\nPrueba de Breusch-Pagan para homocedasticidad:\n\nprint(breusch_pagan)\n\n\n    studentized Breusch-Pagan test\n\ndata:  modelo\nBP = 0.019638, df = 1, p-value = 0.8886\n\n# 3. Estadística Durbin-Watson para autocorrelación de los residuos\ndurbin_watson &lt;- dwtest(modelo)\ncat(\"\\nPrueba de Durbin-Watson para autocorrelación de los residuos:\\n\")\n\n\nPrueba de Durbin-Watson para autocorrelación de los residuos:\n\nprint(durbin_watson)\n\n\n    Durbin-Watson test\n\ndata:  modelo\nDW = 2.0565, p-value = 0.6104\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\n\n\n\n\n\n2.3.7 Observaciones atípicas (outliers)\nEn el contexto de la regresión lineal simple, una observación atípica es un punto de datos que se aleja notablemente del patrón general observado entre la variable independiente (\\(X\\)) y la variable dependiente (\\(Y\\)). Estas observaciones pueden surgir por errores en la recopilación de datos, valores extremos naturales o situaciones especiales que no están representadas en el modelo.\nLas observaciones atípicas pueden tener un impacto significativo en el ajuste del modelo, afectando las estimaciones de los coeficientes de regresión, el análisis de residuos y la validez de las conclusiones. Las observaciones atípicas pueden distorsionar los coeficientes \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\), añadiendo sesgos a las estimaciones. Además, pueden conducir a diagnóticos incorrectos, al alterae la normalidad y homocedasticidad de los residuos, afectando las pruebas estadísticas. Por último, conducen a predicciones inexactas, puesto que los modelos ajustados en presencia de valores atípicos pueden no ser representativos del conjunto de datos general.\n\n2.3.7.1 Tipos de observaciones atípicas\n\nOutliers en \\(Y\\):\n\n\nSon puntos que tienen valores de \\(Y\\) mucho mayores o menores en comparación con lo predicho por el modelo.\n\nEstos puntos suelen detectarse como residuos grandes.\n\n\nOutliers en \\(X\\):\n\n\nSon puntos con valores extremos de \\(X\\) que no están bien representados en el rango principal de los datos.\n\nSi bien pueden no afectar directamente la regresión, pueden influir en los coeficientes del modelo si tienen alta “leverage” (potencial para influir en el ajuste).\n\n\nObservaciones Influyentes:\n\n\nSon puntos que, debido a su posición, tienen un impacto desproporcionado en el ajuste del modelo. Pueden ser outliers en \\(X\\), \\(Y\\), o ambos.\n\n\n\n2.3.7.2 Detección de observaciones atípicas\nUn gráfico de dispersión entre la variables respuesta y la variable explicativa permite identificar qué puntos se desvían claramente del patrón general. Además, en un gráfico de los residuos frente a los valores ajutados, los outliers en \\(Y\\) aparecen como residuos grandes.\nGeneralmente, residuos estudentizados con valores mayores a \\(|3|\\) indican outliers.\nEl leverage, o apalancamiento mide el grado en el que una observación individual influye en el ajuste del modelo. En términos simples, el leverage cuantifica cómo de “lejos” está un punto de datos de la media de la variable explicativa \\(X\\) y, por lo tanto, cuánto contribuye al ajuste del modelo.\nEl leverage se calcula a partir de la matriz de proyección \\(H\\) (también llamada matriz “hat”), que transforma los valores observados en valores ajustados. La diagonal de esta matriz (\\(h_{ii}\\)) mide el leverage para cada observación: \\[\nh_{ii}=\\frac{1}{n}+\\frac{(X_i-\\bar{X})^2}{\\sum_{j=1}^n (X_j-\\bar{X})^2}\n\\]\nEl leverage (\\(h_{ii}\\)) toma valores entre \\(0\\) y \\(1\\). Un leverage cercano a \\(0\\) indica que el punto está muy cerca del centro de los datos, mientras que un leverage alto sugiere que el punto está lejos. Valores de leverage altos (\\(h_{ii} &gt; \\frac{2p}{n}\\), donde \\(p\\) es el número de parámetros del modelo) son sospechosos.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nEstudiamos los valores de leverage del ejemplo de calificaciones de estudiantes.\n\n# Ajustar el modelo de regresión lineal\nmodelo &lt;- lm(calificaciones ~ tiempo_estudio)\n\n# Calcular leverage\nleverage &lt;- hatvalues(modelo)\n\n# Umbral para leverage alto\np &lt;- length(coef(modelo))  # Número de parámetros (incluyendo el intercepto)\nleverage_threshold &lt;- 2 * p / n\n\n# Identificar observaciones con leverage alto\nleverage_high &lt;- which(leverage &gt; leverage_threshold)\n\n# Resultados\ncat(\"Valores de leverage:\\n\")\n\nValores de leverage:\n\nprint(leverage)\n\n         1          2          3          4          5          6          7 \n0.01548204 0.02046914 0.01100581 0.02838063 0.03425201 0.03550447 0.01011261 \n         8          9         10         11         12         13         14 \n0.02920954 0.01034872 0.01021265 0.03617186 0.01024305 0.01397099 0.01066195 \n        15         16         17         18         19         20         21 \n0.02947303 0.03005673 0.01794903 0.03582737 0.01359185 0.03584680 0.02893121 \n        22         23         24         25         26         27         28 \n0.01463144 0.01249149 0.04058238 0.01301990 0.01549100 0.01024494 0.01114180 \n        29         30         31         32         33         34         35 \n0.01548204 0.02548264 0.03682806 0.03034319 0.01463144 0.02088565 0.03780742 \n        36         37         38         39         40         41         42 \n0.01005667 0.01832352 0.01984759 0.01409128 0.01887292 0.02573445 0.01088203 \n        43         44         45         46         47         48         49 \n0.01088203 0.01209977 0.02498512 0.02598829 0.01868408 0.01013361 0.01674089 \n        50         51         52         53         54         55         56 \n0.02600358 0.03550447 0.01038499 0.02131030 0.02755397 0.01047077 0.02066389 \n        57         58         59         60         61         62         63 \n0.02702395 0.01814065 0.02948991 0.01191912 0.01347920 0.03032595 0.01166337 \n        64         65         66         67         68         69         70 \n0.01625359 0.02240745 0.01030996 0.02218396 0.02218396 0.02088565 0.01042556 \n        71         72         73         74         75         76         77 \n0.01814065 0.01210532 0.01564137 0.04091460 0.01007286 0.01964859 0.01174659 \n        78         79         80         81         82         83         84 \n0.01158700 0.01269048 0.02863839 0.01812975 0.01359910 0.01082318 0.02046914 \n        85         86         87         88         89         90         91 \n0.02947303 0.01051278 0.03953410 0.02948991 0.02865490 0.02307628 0.02676199 \n        92         93         94         95         96         97         98 \n0.01301990 0.01301326 0.01313168 0.01396337 0.02194927 0.02006074 0.03032595 \n        99        100 \n0.01013361 0.01002084 \n\ncat(\"\\nUmbral para leverage alto:\", leverage_threshold, \"\\n\")\n\n\nUmbral para leverage alto: 0.04 \n\ncat(\"\\nObservaciones con leverage alto (si las hay):\\n\")\n\n\nObservaciones con leverage alto (si las hay):\n\nprint(leverage_high)\n\n24 74 \n24 74 \n\n# Gráfico de leverage\nplot(leverage, \n     main = \"Leverage de las Observaciones\",\n     xlab = \"Índice de Observación\",\n     ylab = \"Leverage\",\n     pch = 19, col = \"blue\", ylim=c(min(leverage)*.9,max(leverage)*1.05))\nabline(h = leverage_threshold, col = \"red\", lwd = 2, lty = 2)  # Línea del umbral\ntext(leverage_high, leverage[leverage_high], labels = leverage_high, pos = 3, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nLas medidas de influencia son herramientas que evalúan el impacto que tiene cada observación individual en el ajuste del modelo. Mientras que conceptos como los residuos y el leverage analizan diferentes aspectos de las observaciones, las medidas de influencia integran esta información para determinar qué tan decisiva es una observación en la estimación de los coeficientes y en las predicciones del modelo:\n\nDistancia de Cook: Combina leverage y residuos para medir el impacto de una observación en el ajuste del modelo. Valores mayores a \\(1\\) son indicativos de observaciones influyentes.\nDFBETAS: Evalúa cuánto cambia un coeficiente del modelo si se elimina una observación. Valores absolutos mayores a \\(2/\\sqrt{n}\\) sugieren influencia significativa.\nDFFITS: Mide el impacto de una observación en el valor ajustado. Valores absolutos mayores a \\(2\\sqrt{p/n}\\) sugieren influencia significativa.\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nEstudiamos los valores de leverage del ejemplo de calificaciones de estudiantes.\n\n# Ajustar el modelo de regresión lineal\nmodelo &lt;- lm(calificaciones ~ tiempo_estudio)\n\n# 1. Calcular Distancia de Cook\ncooks_distance &lt;- cooks.distance(modelo)\n\n# 2. Calcular DFBETAS\ndfbetas_values &lt;- dfbetas(modelo)\n\n# 3. Calcular DFFITS\ndffits_values &lt;- dffits(modelo)\n\n# Umbrales sugeridos\ncooks_threshold &lt;- 4 / n  # Umbral para Distancia de Cook\ndffits_threshold &lt;- 2 * sqrt(length(coef(modelo)) / n)  # Umbral para DFFITS\n\n# Resultados\ncat(\"Distancia de Cook (primeras 10 observaciones):\\n\")\n\nDistancia de Cook (primeras 10 observaciones):\n\nprint(head(cooks_distance, 10))\n\n           1            2            3            4            5            6 \n7.431009e-04 3.985310e-05 2.107588e-07 3.357548e-02 3.184641e-04 4.788575e-02 \n           7            8            9           10 \n1.210846e-02 7.376917e-03 1.793360e-04 4.056442e-04 \n\ncat(\"\\nObservaciones con Distancia de Cook alta (&gt; \", cooks_threshold, \"):\\n\")\n\n\nObservaciones con Distancia de Cook alta (&gt;  0.04 ):\n\nprint(which(cooks_distance &gt; cooks_threshold))\n\n 6 20 47 85 87 89 \n 6 20 47 85 87 89 \n\ncat(\"\\nDFBETAS (primeras 10 observaciones):\\n\")\n\n\nDFBETAS (primeras 10 observaciones):\n\nprint(head(dfbetas_values, 10))\n\n     (Intercept) tiempo_estudio\n1   0.0333531373  -0.0228338026\n2  -0.0032998199   0.0063523298\n3   0.0004275323  -0.0001952665\n4  -0.1294145160   0.2099533405\n5   0.0138396781  -0.0211294746\n6   0.3088623210  -0.2644845389\n7  -0.0478874296  -0.0165389211\n8  -0.0611970157   0.0982450431\n9   0.0043259263   0.0034593203\n10  0.0150833337  -0.0040906772\n\ncat(\"\\nDFFITS (primeras 10 observaciones):\\n\")\n\n\nDFFITS (primeras 10 observaciones):\n\nprint(head(dffits_values, 10))\n\n            1             2             3             4             5 \n 0.0383726006  0.0088823393  0.0006459231  0.2608877009 -0.0251106446 \n            6             7             8             9            10 \n 0.3120568093 -0.1567290069  0.1211475166  0.0188450608  0.0283488066 \n\ncat(\"\\nObservaciones con DFFITS alto (&gt; \", dffits_threshold, \"):\\n\")\n\n\nObservaciones con DFFITS alto (&gt;  0.2828427 ):\n\nprint(which(abs(dffits_values) &gt; dffits_threshold))\n\n 6 20 22 47 85 87 89 \n 6 20 22 47 85 87 89 \n\n# Graficar Distancia de Cook\nplot(cooks_distance,\n     main = \"Distancia de Cook\",\n     xlab = \"Índice de Observación\",\n     ylab = \"Distancia de Cook\",\n     pch = 19, col = \"blue\", ylim=c(min(cooks_distance)*.9,max(cooks_distance)*1.05))\nabline(h = cooks_threshold, col = \"red\", lwd = 2, lty = 2)\ntext(which(cooks_distance &gt; cooks_threshold), cooks_distance[cooks_distance &gt; cooks_threshold],\n     labels = which(cooks_distance &gt; cooks_threshold), pos = 3, col = \"red\")\n\n\n\n\n\n\n\n# Graficar DFFITS\nplot(dffits_values,\n     main = \"DFFITS\",\n     xlab = \"Índice de Observación\",\n     ylab = \"DFFITS\",\n     pch = 19, col = \"green\", ylim=c(min(dffits_values)*.85,max(dffits_values)*1.07))\nabline(h = c(dffits_threshold, -dffits_threshold), col = \"red\", lwd = 2, lty = 2)\ntext(which(abs(dffits_values) &gt; dffits_threshold), dffits_values[abs(dffits_values) &gt; dffits_threshold],\n     labels = which(abs(dffits_values) &gt; dffits_threshold), pos = 3, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.7.3 Tratamiento de observaciones atípicas\nHemos visto que las observaciones atípicas son inevitables en el análisis de datos y pueden surgir por diversas razones, como errores en la recopilación de datos, fenómenos extremos o situaciones únicas en el conjunto de datos. Aunque estas observaciones pueden proporcionar información valiosa, también tienen el potencial de distorsionar los resultados del análisis, afectando la validez de los modelos ajustados y las conclusiones derivadas.\nEl manejo adecuado de las observaciones atípicas es esencial para garantizar que el modelo represente de manera precisa y robusta el comportamiento general de los datos. Este proceso no implica simplemente eliminar valores problemáticos, sino evaluar cuidadosamente su naturaleza y decidir una estrategia adecuada para tratarlos.\nConsideramos cuatro enfoques principales para el manejo de observaciones atípicas:\n\nVerificación de datos:\n\nConfirmar si las observaciones atípicas son errores de registro o si representan casos válidos pero raros.\n\nTransformación de datos:\n\nAplicar transformaciones como logaritmos o raíces cuadradas para reducir la influencia de los valores atípicos.\n\nModelos robustos:\n\nConsiderar modelos de regresión robusta, que son menos sensibles a los valores atípicos.\n\nEliminación justificada:\n\nEn casos donde los valores atípicos son errores claros o no representativos, pueden eliminarse del análisis, siempre documentando esta decisión.\n\n\n\n\n\n2.3.8 Tratamiento de problemas\nSi el modelo de regresión lineal simple no es apropiado porque se incumplen algunas de las suposiciones, algunas opciones básicas son:\n\nAbandonar el modelo de regresión y desarrollar otro más apropiado, según las suposiciones incumplidas.\nEmplear alguna transformación de los datos de forma que el modelo de regresión sea válido para los datos transformados.\n\nTrataremos estas técnicas en próximos capítulos de este libro.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#regresión-lineal-simple-1",
    "href": "tema1.html#regresión-lineal-simple-1",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.4 Regresión Lineal Simple",
    "text": "1.4 Regresión Lineal Simple\nLa regresión lineal simple es una de las herramientas más fundamentales y ampliamente utilizadas en el análisis estadístico. Su objetivo principal es modelar la relación entre dos variables: una variable explicativa (independiente) y una variable respuesta (dependiente). Este modelo permite no solo describir cómo se relacionan estas dos variables, sino también realizar predicciones basadas en dicha relación.\nEl concepto básico de la regresión lineal simple es ajustar una recta que minimice las discrepancias entre los valores observados y los predichos por el modelo. La ecuación general de este modelo es:\n\\[\nY = \\beta_0 + \\beta_1 X + \\varepsilon,\n\\] donde:\n- \\(Y\\) es la variable dependiente o respuesta.\n- \\(X\\) es la variable independiente o explicativa.\n- \\(\\beta_0\\) es el intercepto, que representa el valor de \\(Y\\) cuando \\(X = 0\\).\n- \\(\\beta_1\\) es la pendiente, que indica el cambio esperado en \\(Y\\) por cada unidad de cambio en \\(X\\).\n- \\(\\varepsilon\\) es un término de error que captura la variabilidad no explicada por el modelo.\nLa regresión lineal simple es fundamental en estadística y ciencia de datos porque proporciona un marco intuitivo y matemáticamente riguroso para analizar relaciones. Algunas aplicaciones comunes incluyen:\n\nPredecir valores futuros, como ingresos o gastos, en función de un predictor.\n\nEvaluar el impacto de una variable en otra, como el efecto de la inversión publicitaria en las ventas.\n\nExplorar relaciones lineales entre variables en estudios científicos o sociales.\n\n\n\n\n\n\n\nPropiedades Clave\n\n\n\n\n\n1.5 Propiedades clave\nEl modelo de regresión lineal simple se basa en ciertos supuestos, como:\n\nUna relación lineal entre \\(X\\) y \\(Y\\).\n\nIndependencia de los términos de error.\n\nVarianza constante de los errores (homocedasticidad).\n\nNormalidad de los errores para inferencias estadísticas.\n\nEstos supuestos son esenciales para garantizar la validez de las estimaciones y conclusiones derivadas del modelo.\n\n\n\nEn esta sección, exploraremos los fundamentos de la regresión lineal simple, desde su formulación teórica hasta su implementación práctica. Veremos cómo ajustar este modelo, interpretar sus parámetros, y evaluar su adecuación utilizando herramientas estadísticas y gráficas. Además, se presentarán ejemplos prácticos para ilustrar su aplicación en situaciones reales.\n\n\n\n\n\n\nEjercicio\n\n\n\nDejamos como ejercicio para el alumno la interpretación del resultado.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#propiedades-clave",
    "href": "tema1.html#propiedades-clave",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.4 Propiedades clave",
    "text": "1.4 Propiedades clave\nEl modelo de regresión lineal simple se basa en ciertos supuestos, como:\n\nUna relación lineal entre \\(X\\) y \\(Y\\).\n\nIndependencia de los términos de error.\n\nVarianza constante de los errores (homocedasticidad).\n\nNormalidad de los errores para inferencias estadísticas.\n\nEstos supuestos son esenciales para garantizar la validez de las estimaciones y conclusiones derivadas del modelo.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#diagnóstico-del-modelo.-análisis-de-residuos",
    "href": "tema1.html#diagnóstico-del-modelo.-análisis-de-residuos",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.4 Diagnóstico del Modelo. Análisis de Residuos",
    "text": "1.4 Diagnóstico del Modelo. Análisis de Residuos\nEl diagnóstico del modelo de regresión es un paso esencial para evaluar si los supuestos subyacentes se cumplen y garantizar la validez de las inferencias. El análisis de residuos proporciona información clave sobre la calidad del ajuste del modelo y la adecuación de los datos a los supuestos de la regresión lineal.\nRecordemos que los residuos del modelo (\\(e_i\\)) son las diferencias entre los valores observados (\\(Y_i\\)) y los valores predichos (\\(\\hat{Y}_i\\)) por el modelo: \\[\ne_i = Y_i - \\hat{Y}_i.\n\\] Representan la parte de la variabilidad en \\(Y\\) no explicada por el modelo.\nEn ocasiones, es preferible trabakar con los residuos estandarizados, que tienen media cero y varianza aproximadamente unidad: \\[\nd_i = \\frac{e_i}{\\sqrt{MSE}},  \\hspace{0.5cm} i=1,\\ldots,n.\n\\] Otro tipo de residuos habitual es el de los llamados residuos estudentizados: \\[\nd_i = \\frac{e_i}{\\sqrt{MSE \\left ( 1-\\frac{1}{n}-\\frac{(X_i-\\bar{X})^2}{S_xx} \\right )}}, \\hspace{0.5cm} i=1,\\ldots,n.\n\\]\nEstos residuos son preferibles a los estandarizados cuando \\(n\\) es pequeño.\n\n\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, y William Li. 2005. Applied linear statistical models. McGraw-hill.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#regresión-lineal-múltiple",
    "href": "tema1.html#regresión-lineal-múltiple",
    "title": "2  Modelo de regresión lineal: simple y múltiple",
    "section": "2.4 Regresión lineal múltiple",
    "text": "2.4 Regresión lineal múltiple\nEl modelo de regresión lineal múltiple es una extensión natural del modelo de regresión lineal simple, diseñada para analizar la relación entre una variable respuesta (\\(Y\\)) y múltiples variables explicativas (\\(X_1, X_2, \\dots, X_p\\)). Este modelo permite capturar interacciones más complejas entre las variables y mejora la capacidad de explicar y predecir fenómenos en contextos donde una sola variable explicativa no es suficiente para describir la variabilidad en \\(Y\\).\nLa ecuación general del modelo de regresión lineal múltiple se expresa como: \\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\varepsilon,\n\\] donde:\n\n\\(\\beta_0\\): Intercepto, representa el valor esperado de \\(Y\\) cuando todas las variables explicativas son cero.\n\\(\\beta_1, \\beta_2, \\dots, \\beta_p\\): Coeficientes de regresión, que miden el efecto promedio de cada variable explicativa sobre \\(Y\\), manteniendo las demás constantes.\n\\(\\varepsilon\\): Término de error aleatorio, que captura la variabilidad en \\(Y\\) no explicada por las variables \\(X_1, X_2, \\dots, X_p\\).\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nGeneramos nuevos datos para regresión lineal múltiple.\n\n# Generar dos nuevas variables explicativas\n# Establecer la semilla para reproducibilidad\nset.seed(123)\n\n# Generar datos base\nn &lt;- 100\ntiempo_estudio &lt;- round(runif(n, min = 5, max = 40), 1)  # Tiempo de estudio (horas/semana)\nbeta_0_true &lt;- 5  # Intercepto verdadero\nbeta_1_true &lt;- 0.1  # Pendiente verdadera\nsigma &lt;- 0.5  # Desviación estándar del ruido\n\n# Generar calificaciones dependientes del tiempo de estudio con ruido\ncalificaciones &lt;- round(beta_0_true + beta_1_true * tiempo_estudio + rnorm(n, mean = 0, sd = sigma), 2)\n\n\ndistraccion &lt;- sample(0:1, size = n, replace = TRUE)  # Nivel de distracción (0 bajo, 1 alto)\ninteracciones_clase &lt;- sample(0:20, size = n, replace = TRUE)  # Interacciones en clase\n\n# Crear un data frame\ndatos &lt;- data.frame(\n  Tiempo_Estudio = tiempo_estudio,\n  Distraccion = distraccion,\n  Interacciones_Clase = interacciones_clase,\n  Calificaciones = calificaciones\n)\n\n# Mostrar los primeros registros\nhead(datos)\n\n  Tiempo_Estudio Distraccion Interacciones_Clase Calificaciones\n1           15.1           1                  14           6.64\n2           32.6           1                   2           8.25\n3           19.3           0                   8           6.91\n4           35.9           1                   6           9.27\n5           37.9           0                   8           8.68\n6            6.6           0                   3           6.42\n\n\n\n\n\nEl modelo de regresión lineal múltiple es fundamental para analizar fenómenos que dependen de múltiples factores, ya que permite explicar variabilidad compleja al identificar la contribución individual de cada variable explicativa. Al incluir diversas variables relevantes, este modelo mejora significativamente la precisión de las predicciones en comparación con modelos más simples. Además, proporciona una herramienta poderosa para controlar variables confusoras, evaluando el efecto de cada variable explicativa mientras ajusta por la influencia de las demás, lo que garantiza un análisis más robusto y detallado.\nEl modelo de regresión lineal múltiple se basa en los mismos supuestos fundamentales que el modelo simple: linealidad, independencia, homocedasticidad y normalidad de los errores. Sin embargo, la presencia de múltiples variables explicativas introduce nuevos desafíos, como la multicolinealidad, que deben ser diagnosticados y manejados para garantizar la fiabilidad del modelo.\nEn esta sección, exploraremos los fundamentos teóricos del modelo de regresión lineal múltiple, sus aplicaciones prácticas y las técnicas de diagnóstico necesarias para evaluar su calidad y validez. También abordaremos cómo interpretar los coeficientes y realizar predicciones útiles en contextos reales.\n\n2.4.1 Variables regresoras cualitativas\nEl modelo de regresión lineal múltiple no se limita al uso de variables cuantitativas, sino que también permite incorporar variables predictoras cualitativas. Por ejemplo, se pueden incluir características como el género (masculino o femenino) o el grado de satisfacción (nada satisfecho, poco satisfecho, satisfecho, muy satisfecho). Para representar estas variables en el modelo de regresión lineal, se utilizan variables binarias que indican a qué categoría pertenece cada individuo.\nPor ejemplo, consideremos un modelo de regresión lineal para predecir las calificaciones de los estudiantes, \\(Y\\) , en función del tiempo de estudio \\(X_1\\), las interacciones en clase, \\(X_2\\) y el grado de distracción \\(X_3\\).\nEn este caso definimos \\(X_3\\) como sigue:\n\\[\nX_3 =\n\\begin{cases}\n0 & \\text{si el grado de distracción es bajo} \\\\\n1 & \\text{si el grado de distracción es alto}\n\\end{cases}\n\\] En general, una variable cualitativa con \\(c\\) niveles, se puede codificar mediante \\(c − 1\\) variables binarias o variables indicadoras (también llamadas dummy).\nTrataremos ampliamente el tema de creación de variables en temas posteriores.\n\n\n2.4.2 Estimación de los coeficientes de regresión\nEs más conveniente trabajar con los modelos de regresión lineal múltiple si los expresamos en notación matricial. En ese caso, el modelo de regresión lineal múltiple se escribe como:\n\\[\n\\mathbf{Y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon},\n\\]\ndonde:\n\n\\(\\mathbf{Y}\\) es el vector de observaciones de la variable dependiente (respuesta), de dimensión \\(n \\times 1\\): \\[\n  \\mathbf{Y} =\n  \\begin{bmatrix}\n  Y_1 \\\\\n  Y_2 \\\\\n  \\vdots \\\\\n  Y_n\n  \\end{bmatrix}.\n  \\]\n\\(\\mathbf{X}\\) es la matriz de diseño de las variables explicativas (predictoras), de dimensión \\(n \\times (p+1)\\), donde \\(p\\) es el número de variables explicativas. Incluye una columna de unos para el término independiente (\\(\\beta_0\\)): \\[\n  \\mathbf{X} =\n  \\begin{bmatrix}\n  1 & X_{11} & X_{12} & \\cdots & X_{1p} \\\\\n  1 & X_{21} & X_{22} & \\cdots & X_{2p} \\\\\n  \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  1 & X_{n1} & X_{n2} & \\cdots & X_{np}\n  \\end{bmatrix}.\n  \\]\n\\(\\boldsymbol{\\beta}\\) es el vector de coeficientes de regresión, de dimensión \\((p+1) \\times 1\\): \\[\n  \\boldsymbol{\\beta} =\n  \\begin{bmatrix}\n  \\beta_0 \\\\\n  \\beta_1 \\\\\n  \\vdots \\\\\n  \\beta_p\n  \\end{bmatrix}.\n  \\]\n\\(\\boldsymbol{\\varepsilon}\\) es el vector de errores aleatorios, de dimensión \\(n \\times 1\\): \\[\n  \\boldsymbol{\\varepsilon} =\n  \\begin{bmatrix}\n  \\varepsilon_1 \\\\\n  \\varepsilon_2 \\\\\n  \\vdots \\\\\n  \\varepsilon_n\n  \\end{bmatrix}.\n  \\]\n\nEn esta formulación, el modelo expresa que cada observación de la variable dependiente es una combinación lineal de las variables predictoras, más un término de error aleatorio. La notación matricial es compacta y facilita la manipulación algebraica para obtener estimaciones y realizar inferencias.\nComo en el caso de la regresión lineal simple, asumimos que los errores \\(\\varepsilon\\) son i.i.d. según una normal:\n\\[\n\\boldsymbol{\\varepsilon} \\overset{i.i.d.}{\\sim} \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}),\n\\]\ndonde \\(\\mathbf{I}\\) denota la matriz identidad,\n\\[\n\\mathbf{I} =\n\\begin{pmatrix}\n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1\n\\end{pmatrix}.\n\\]\nEl criterio de mínimos cuadrados es el método más común para estimar los parámetros del modelo de regresión lineal múltiple. Este método busca minimizar la suma de los cuadrados de los residuos. Es decir, la suma de los cuadrados de las diferencias entre los valores observados de la variable dependiente y los valores predichos por el modelo.\nEl objetivo es estimar \\(\\boldsymbol{\\beta}\\), el conjunto de coeficientes de regresión.\nLa función de pérdida que se minimiza es la suma de los cuadrados de los residuos (\\(S(\\boldsymbol{\\beta})\\)): \\[\nS(\\boldsymbol{\\beta}) = \\sum_{i=1}^n (Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^2,\n\\] o en notación matricial: \\[\nS(\\boldsymbol{\\beta}) = (\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^\\top (\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta}),\n\\] donde \\((\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\) es el vector de residuos.\nPara encontrar el valor de \\(\\boldsymbol{\\beta}\\) que minimiza \\(S(\\boldsymbol{\\beta})\\), derivamos \\(S(\\boldsymbol{\\beta})\\) con respecto a \\(\\boldsymbol{\\beta}\\) y la igualamos a cero:\n\nExpandimos \\(S(\\boldsymbol{\\beta})\\): \\[\nS(\\boldsymbol{\\beta}) = \\mathbf{Y}^\\top \\mathbf{Y} - 2 \\mathbf{Y}^\\top \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\beta}^\\top \\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta}.\n\\]\nDerivamos con respecto a \\(\\boldsymbol{\\beta}\\): \\[\n\\frac{\\partial S(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = -2 \\mathbf{X}^\\top \\mathbf{Y} + 2 \\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta}.\n\\]\nIgualamos a cero: \\[\n-2 \\mathbf{X}^\\top \\mathbf{Y} + 2 \\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta} = 0.\n\\]\nResolviendo para \\(\\boldsymbol{\\beta}\\): \\[\n\\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta} = \\mathbf{X}^\\top \\mathbf{Y}.\n\\]\nFinalmente, el estimador de mínimos cuadrados es: \\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{Y}.\n\\]\n\nEl criterio de mínimos cuadrados ajusta un plano en el espacio multidimensional de las variables explicativas que mejor representa los datos en términos de minimizar los residuos al cuadrado. Este estimador es ampliamente usado por su simplicidad y eficacia bajo los supuestos de normalidad y homocedasticidad.\n\n\n\n\n\n\nPara recordar\n\n\n\nCuando se trata de un modelo de regresión con múltiples regresores, el análisis visual mediante diagramas de dispersión puede resultar engañoso. Construir gráficos que muestren la relación entre la variable respuesta y cada regresor de forma aislada no siempre proporciona una representación precisa de la realidad. Esto ocurre porque los efectos conjuntos de los regresores no se reflejan en estos gráficos individuales. Incluso en un caso ideal, donde existe una relación perfecta entre la variable respuesta y los regresores, sin ningún tipo de ruido o error, los diagramas de dispersión individuales pueden sugerir una relación débil o inexistente si no se tiene en cuenta el contexto multivariado.\nEn situaciones más comunes, donde los datos incluyen errores en los valores observados de la variable respuesta y los regresores interactúan de manera compleja, los gráficos univariantes tienden a complicar aún más la interpretación. Pueden surgir aparentes inconsistencias o patrones confusos, ya que no se está considerando la influencia simultánea de todos los regresores en la respuesta. Por esta razón, es fundamental adoptar enfoques que analicen las relaciones de manera conjunta, como los coeficientes estimados en un modelo de regresión múltiple, que capturan el impacto parcial de cada regresor ajustando por los efectos de los demás.\n\n\n\n2.4.2.1 Propiedades del estimador\nEstas son las propiedades del estimador de mínimos cuadrados:\n\nLinealidad: El estimador es una combinación lineal de los valores observados (\\(\\mathbf{Y}\\)).\nInsesgado: En ausencia de errores correlacionados y bajo supuestos estándar, \\(E[\\hat{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\\).\nVarianza mínima: Entre los estimadores lineales insesgados, \\(\\hat{\\boldsymbol{\\beta}}\\) tiene la menor varianza posible (propiedad BLUE: Best Linear Unbiased Estimator).\n\n\n\n\n2.4.3 Varianza del error\nLa varianza de los errores (\\(\\sigma^2\\)) mide cuánto varían las observaciones reales (\\(Y_i\\)) con respecto a los valores predichos por el modelo (\\(\\hat{Y}_i\\)). En el modelo de regresión múltiple, esta varianza puede ser estimada utilizando la suma de los cuadrados de los residuos (SSE).\nLa estimación de \\(\\sigma^2\\), denotada como \\(\\hat{\\sigma}^2\\), se calcula como: \\[\n\\hat{\\sigma}^2 = \\frac{\\text{SSE}}{n - p - 1}, \\] donde:\n\n\\(\\text{SSE} = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 = \\mathbf{e}^\\top \\mathbf{e}\\) es la suma de los cuadrados de los residuos.\n\\(\\mathbf{e} = \\mathbf{Y} - \\hat{\\mathbf{Y}} = \\mathbf{Y} - \\mathbf{X} \\hat{\\boldsymbol{\\beta}}\\) es el vector de residuos.\n\nLa cantidad \\(n - p - 1\\) es conocida como los grados de libertad residuales, que reflejan el número de datos disponibles para estimar la varianza de los errores después de ajustar los parámetros del modelo.\nEn términos matriciales, \\(\\hat{\\sigma}^2\\) se expresa como: \\[\n\\hat{\\sigma}^2 = \\frac{1}{n - p - 1} (\\mathbf{Y} - \\mathbf{X} \\hat{\\boldsymbol{\\beta}})^\\top (\\mathbf{Y} - \\mathbf{X} \\hat{\\boldsymbol{\\beta}}),\n\\] o de forma más compacta: \\[\n\\hat{\\sigma}^2 = \\frac{1}{n - p - 1} \\mathbf{e}^\\top \\mathbf{e}.\n\\]\nLa varianza estimada \\(\\hat{\\sigma}^2\\) se encuentra en las mismas unidades al cuadrado que la variable dependiente (\\(\\mathbf{Y}\\)). Este valor es crucial para construir intervalos de confianza y realizar pruebas de hipótesis sobre los coeficientes del modelo (\\(\\boldsymbol{\\beta}\\)). La raíz cuadrada de \\(\\hat{\\sigma}^2\\) proporciona el error estándar de los residuos.\nEste valor permite cuantificar la variabilidad no explicada por el modelo ajustado y es un componente esencial en la evaluación de la calidad del ajuste.\n\n\n2.4.4 Inferencia sobre los parámetros del modelo\nTal como indicamos en el modelo de regresión simple, en el análisis de regresión lineal múltiple, la inferencia estadística permite evaluar la significancia de los parámetros estimados y del modelo completo. Estas evaluaciones son cruciales para determinar si el modelo ajustado es útil para explicar y predecir la variable respuesta.\n\n2.4.4.1 Intervalos de confianza para los coeficientes de regresión\nLos intervalos de confianza proporcionan un rango plausible para los valores verdaderos de los coeficientes de regresión (\\(\\beta_j\\)), permitiendo evaluar su significancia.\nPara construir intervalos de confianza para los coeficientes de la regresión, \\(\\beta_0, \\beta_1, \\ldots, \\beta_k\\), procedemos de forma análoga a como hacíamos en el caso de la regresión lineal simple. Usamos la distribución en el muestreo de los estimadores, \\(\\hat{\\beta}\\): \\[\n\\hat{\\beta} \\sim N_p(\\beta, \\sigma^2 (\\mathbf{X}^\\top \\mathbf{X})^{-1}).\n\\]\nEsto implica que cada coeficiente \\(\\hat{\\beta}_j\\) es normal con media \\(\\beta_j\\) y varianza \\(\\sigma^2 C_{jj}\\), con \\(C_{jj}\\) el elemento \\(j\\)-ésimo de la diagonal de la matriz \\((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\).\nDe este modo, los estadísticos: \\[\nt_j=\\frac{\\hat{\\beta_j}-\\beta_j}{\\sqrt{\\hat{\\sigma^2}C_{jj}}} \\hspace{0.5cm} j=0,1,\\ldots,p\n\\] se distribuye como una t-Student, con \\(n-p\\) grados de libertad.\nPo tanto, el intervalo de confianza para cada coeficiente \\(\\beta_j\\) se calcula como:\n\\[\n\\hat{\\beta}_j \\pm t_{n-p-1,\\alpha/2} \\cdot \\text{SE}(\\hat{\\beta}_j) \\hspace{0.5cm} j=0,1,\\ldots,p\n\\]\ndonde:\n\\[\n  \\text{SE}(\\hat{\\beta}_j) = \\sqrt{\\sigma^2 (\\mathbf{X}^\\top \\mathbf{X})^{-1}_{jj}}.\n  \\]\nes el error estándar del estimador \\(\\hat{\\beta}_j\\).\n\n\n2.4.4.2 Contraste global sobre lo significativo que es el modelo de regresión lineal múltiple\nEl contraste global evalúa si el modelo de regresión, como un todo, explica una proporción significativa de la variabilidad de la variable respuesta. Esto se realiza mediante un test F y una tabla ANOVA.\nLa tabla ANOVA para la regresión múltiple descompone la variabilidad total de la variable respuesta (\\(\\text{SCT}\\)) en dos componentes principales:\n\nSuma de Cuadrados del Modelo de Regresión (SSR): Variabilidad explicada por los regresores.\nSuma de Cuadrados de los Residuos (SSE): Variabilidad no explicada por el modelo.\n\nLa descomposición se expresa como: \\[\n\\text{SST} = \\text{SSE} + \\text{SSR}.\n\\]\nLos términos involucrados son:\n\n\\(\\text{SSR} = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2\\), donde \\(\\hat{Y}_i\\) son los valores predichos por el modelo y \\(\\bar{Y}\\) es la media de \\(Y\\).\n\\(\\text{SSE} = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\), la suma de los cuadrados de los residuos.\n\\(\\text{SST} = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\\), la suma total de los cuadrados.\n\nLa tabla ANOVA incluye los siguientes elementos:\n\n\n\n\n\n\n\n\n\n\nFuente de Variación\nSuma de Cuadrados\nGrados de Libertad\nCuadrados Medios\nEstadístico F\n\n\n\n\nModelo\n\\(\\text{SSR}\\)\n\\(p\\)\n\\(MSR=\\text{SSR}/p\\)\n\\(F = \\frac{\\text{SCM}/p}{\\text{SCR}/(n-p-1)}\\)\n\n\nResiduos\n\\(\\text{SSE}\\)\n\\(n-p-1\\)\n\\(MSE=\\text{SSE}/(n-p-1)\\)\n\n\n\nTotal\n\\(\\text{SST}\\)\n\\(n-1\\)\n\n\n\n\n\nLa hipótesis del test F global es:\n\n\\(H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0\\) (el modelo no es significativo),\n\\(H_1:\\) al menos uno de los \\(\\beta_j \\neq 0\\).\n\nEl estadístico F se calcula como: \\[\nF = \\frac{\\text{SSR}/p}{\\text{SSE}/(n-p-1)}=\\frac{MSR}{MSE}.\n\\] Se compara con un valor crítico de la distribución \\(F_{p, n-p-1}\\). Si \\(F\\) es grande o el valor \\(p\\) asociado es pequeño, se rechaza \\(H_0\\).\n\n\n2.4.4.3 Test global sobre el ajuste del modelo\nEl test global evalúa si la proporción de la variabilidad total explicada por el modelo (\\(R^2\\)) es significativa. Esto se relaciona con el test F global presentado en la sección anterior.\nEl coeficiente de determinación \\(R^2\\) se define como: \\[\nR^2 = \\frac{\\text{SSR}}{\\text{SST}}.\n\\] El test F analiza si este valor es significativamente distinto de cero, como se muestra en la tabla ANOVA.\n\n\n2.4.4.4 Test sobre coeficientes individuales\nEl objetivo aquí es determinar si cada variable explicativa tiene un efecto significativo en la variable respuesta, después de ajustar por las demás.\nPara cada coeficiente \\(\\beta_j\\):\n\n\\(H_0: \\beta_j = 0\\) (la variable \\(X_j\\) no contribuye al modelo),\n\\(H_1: \\beta_j \\neq 0\\) (la variable \\(X_j\\) tiene un efecto significativo).\n\nEl estadístico \\(t\\) para cada \\(\\beta_j\\) se calcula como: \\[\nt_j = \\frac{\\hat{\\beta}_j}{\\text{SE}(\\hat{\\beta}_j)},\n\\]\ndonde \\(\\text{SE}(\\hat{\\beta}_j)\\) es el error estándar del coeficiente. Este estadístico sigue una distribución \\(t_{n-p-1}\\).\nSe compara \\(|t_j|\\) con el valor crítico de \\(t\\) para un nivel de significancia \\(\\alpha\\), o se evalúa el valor \\(p\\). Si \\(|t_j|\\) es suficientemente grande (o \\(p\\) es pequeño), se rechaza \\(H_0\\), indicando que \\(X_j\\) contribuye significativamente al modelo.\nEl test sobre los coeficientes individuales permite identificar qué variables son más importantes en la explicación de la variabilidad de la variable respuesta, ajustando por las demás. Esto ayuda a interpretar y refinar el modelo.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nPara los datos de calificaciones y tiempo de estudio, estos son los estimadores de los parámetros del modelo de regresión:\n\n# Ajustar el modelo de regresión lineal\nmodelo_regresion &lt;- lm(Calificaciones ~ Tiempo_Estudio + Distraccion + Interacciones_Clase, data = datos)\n\n# Mostrar los resultados del modelo\nsummary(modelo_regresion)\n\n\nCall:\nlm(formula = Calificaciones ~ Tiempo_Estudio + Distraccion + \n    Interacciones_Clase, data = datos)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.05269 -0.30234 -0.00226  0.27892  1.15358 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         4.829436   0.148491  32.524   &lt;2e-16 ***\nTiempo_Estudio      0.097585   0.004902  19.906   &lt;2e-16 ***\nDistraccion         0.075166   0.098742   0.761   0.4484    \nInteracciones_Clase 0.015429   0.008310   1.857   0.0664 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4801 on 96 degrees of freedom\nMultiple R-squared:  0.8141,    Adjusted R-squared:  0.8083 \nF-statistic: 140.1 on 3 and 96 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\n\n\nDraper, NR. 1998. Applied regression analysis. McGraw-Hill. Inc.\n\n\nIsabel, Alberto Fernández, y Isaac Martı́n De Diego. 2020. Ciencia de datos para la ciberseguridad. Ra-Ma Editorial.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, y William Li. 2005. Applied linear statistical models. McGraw-hill.\n\n\nWeisberg, S. 2005. «Applied linear regression». Wiley.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema2.html",
    "href": "tema2.html",
    "title": "3  Métodos de selección de variables y problemas de regularización",
    "section": "",
    "text": "3.1 Proceso de construcción del modelo de regresión\nEn los modelos de regresión, especialmente cuando se trabaja con conjuntos de datos que incluyen un gran número de variables predictoras, es común enfrentarse al desafío de identificar qué variables son realmente relevantes para explicar la variable respuesta. La inclusión de demasiadas variables en un modelo puede llevar a problemas como el sobreajuste, pérdida de interpretabilidad y complejidad innecesaria, mientras que la exclusión de variables importantes puede resultar en modelos subóptimos.\nLa construcción de un modelo de regresión múltiple es un proceso sistemático que busca explicar la relación entre una variable respuesta (\\(Y\\)) y múltiples variables predictoras (\\(X_1, X_2, \\dots, X_k\\)). Este proceso consta de varias etapas clave (Kutner et al. 2005):\nEl objetivo principal de este tema es presentar las técnicas más relevantes para la selección de variables y regularización, entender sus fundamentos teóricos, y aplicarlas a casos prácticos. Esto no solo permitirá construir modelos más robustos y eficientes, sino que también ayudará a obtener insights más claros y útiles a partir de los datos.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Métodos de selección de variables y problemas de regularización</span>"
    ]
  },
  {
    "objectID": "tema2.html#métodos-de-selección-directa",
    "href": "tema2.html#métodos-de-selección-directa",
    "title": "3  Métodos de selección de variables y problemas de regularización",
    "section": "3.4 Métodos de selección directa",
    "text": "3.4 Métodos de selección directa\nLos métodos de selección directa son un enfoque fundamental en la búsqueda de un subconjunto óptimo de variables predictoras en modelos de regresión. Este enfoque evalúa de manera sistemática diferentes combinaciones de variables para identificar cuál de ellas proporciona el mejor ajuste al modelo en función de un criterio predefinido, como el coeficiente de determinación ajustado (\\(R^2\\) ajustado), el error cuadrático medio (ECM) o criterios de información como AIC o BIC.\nA diferencia de los métodos automáticos, los métodos de selección directa no dependen de un proceso iterativo de adición o eliminación de variables. En cambio, buscan exhaustivamente (o mediante aproximaciones computacionalmente más eficientes) entre todas las posibles combinaciones de variables, lo que garantiza un análisis completo de las interacciones y relevancias potenciales.\nEstos métodos son especialmente útiles cuando el número de predictores no es demasiado grande, ya que el esfuerzo computacional crece exponencialmente con el número de variables. Aunque el costo computacional puede ser elevado en datasets amplios, los métodos de selección directa proporcionan una referencia sólida y transparente para evaluar qué variables son fundamentales en el modelo.\nEn esta sección, analizaremos los métodos de selección directa más comunes, su implementación práctica y las métricas utilizadas para comparar modelos, destacando sus ventajas y limitaciones.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Métodos de selección de variables y problemas de regularización</span>"
    ]
  },
  {
    "objectID": "tema2.html#métodos-automáticos",
    "href": "tema2.html#métodos-automáticos",
    "title": "3  Métodos de selección de variables y problemas de regularización",
    "section": "3.5 Métodos automáticos",
    "text": "3.5 Métodos automáticos\nLos métodos automáticos de selección de variables son herramientas prácticas y eficientes diseñadas para identificar subconjuntos relevantes de predictores en un modelo de regresión. A diferencia de los métodos de selección directa, que exploran exhaustivamente todas las combinaciones posibles de variables, los métodos automáticos siguen un enfoque iterativo que simplifica el proceso de selección. Estos métodos son especialmente útiles en situaciones donde el número de predictores es elevado, ya que reducen significativamente el esfuerzo computacional.\nEl principio clave detrás de los métodos automáticos es el ajuste dinámico del conjunto de variables en función de criterios estadísticos, como \\(p\\)-valores, coeficientes de determinación ajustados (\\(R^2\\)) ajustado), o criterios de información como AIC y BIC. Entre las estrategias más comunes se encuentran:\n\nMétodo Forward (selección progresiva): Parte de un modelo vacío e incorpora variables de manera secuencial, añadiendo en cada paso la variable que mejora más el modelo.\nMétodo Backward (eliminación regresiva): Comienza con todas las variables en el modelo y elimina iterativamente aquellas que tienen menor impacto.\nMétodo Stepwise: Combina las estrategias forward y backward, permitiendo tanto la inclusión como la exclusión de variables en cada iteración.\n\nEstos métodos ofrecen una manera estructurada y ágil de seleccionar variables, aunque no garantizan encontrar el mejor modelo global debido a su naturaleza secuencial. A lo largo de esta sección, examinaremos cada uno de estos métodos, sus ventajas, limitaciones y aplicaciones en diferentes contextos de análisis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Métodos de selección de variables y problemas de regularización</span>"
    ]
  },
  {
    "objectID": "tema2.html#métodos-basados-en-regularización",
    "href": "tema2.html#métodos-basados-en-regularización",
    "title": "3  Métodos de selección de variables y problemas de regularización",
    "section": "3.6 Métodos basados en regularización",
    "text": "3.6 Métodos basados en regularización\nEn los modelos de regresión, especialmente cuando se trabaja con un gran número de variables predictoras o con datos multicolineales, los métodos tradicionales de selección de variables pueden resultar ineficaces o inestables. En estos casos, los métodos basados en regularización surgen como una alternativa poderosa que no solo selecciona variables, sino que también mejora la estabilidad y la precisión del modelo.\nLa regularización consiste en introducir una penalización en la función de ajuste del modelo, lo que tiene dos efectos principales: controlar el sobreajuste al reducir la complejidad del modelo y forzar la selección de un subconjunto más parsimonioso de predictores. Estas penalizaciones ajustan los coeficientes de las variables predictoras, favoreciendo soluciones más simples y robustas (James et al. 2013).\nEntre los métodos de regularización más destacados se encuentran:\n\nRidge Regression: Aplica una penalización proporcional al cuadrado de los coeficientes, lo que permite manejar problemas de multicolinealidad pero no conduce a la eliminación completa de variables.\nLasso (Least Absolute Shrinkage and Selection Operator): Introduce una penalización basada en el valor absoluto de los coeficientes, lo que no solo reduce su magnitud, sino que también puede anularlos completamente, realizando una selección automática de variables.\nElastic Net: Combina las penalizaciones de Ridge y Lasso, ofreciendo mayor flexibilidad en situaciones donde hay una gran correlación entre los predictores.\n\nEstos métodos son especialmente útiles en problemas donde el número de variables predictoras excede el número de observaciones, o cuando se desea un modelo más interpretable. En esta sección, exploraremos en detalle los fundamentos teóricos, la implementación práctica y las aplicaciones de cada uno de estos métodos, destacando sus ventajas en escenarios complejos y desafiantes.\n\n3.6.1 Ridge regression\nLa regresión Ridge introduce una penalización en la estimación de los coeficientes de regresión, lo que ayuda a reducir la varianza del modelo y mejora su capacidad predictiva en presencia de datos altamente correlacionados o con muchas variables (Marquardt y Snee 1975). El modelo de regresión Ridge es una extensión de la regresión lineal estándar. Dado un conjunto de datos con \\(n\\) observaciones y \\(p\\) predictores, expresamos el modelo de regresión lineal múltiple como:\n\\[\n\\mathbf{Y}= \\mathbf{X} \\beta + \\boldsymbol{\\varepsilon}\n\\]\ndonde:\n\n\\(\\mathbf{Y}\\) es el vector de respuesta de dimensión \\(n \\times 1\\).\n\\(\\mathbf{X}\\) es la matriz de diseño de dimensión \\(n \\times p\\).\n\\(\\beta\\) es el vector de coeficientes de regresión de dimensión \\(p \\times 1\\).\n\\(\\boldsymbol{\\varepsilon}\\) es el vector de errores aleatorios.\n\nEn mínimos cuadrados ordinarios (OLS), los coeficientes se estiman minimizando la suma de los errores al cuadrado:\n\\[\nSSE = \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2 = \\| \\mathbf{Y} - \\mathbf{X} \\beta \\|^2.\n\\]\nSin embargo, cuando hay multicolinealidad, la matriz \\(X^T X\\) puede ser casi singular, generando coeficientes inestables. Para evitar esto, la regresión Ridge añade un término de penalización \\(\\lambda\\), de la siguiente manera:\n\\[\nSSE_{ridge} = \\| \\mathbf{Y} - \\mathbf{X} \\beta \\|^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2.\n\\]\nEste término adicional, es un término de penalización (\\(L_2=\\sum \\beta_j^2\\)) impone una restricción sobre los coeficientes, evitando que tomen valores excesivamente grandes. La estimación de \\(\\beta\\) en Ridge se obtiene resolviendo:\n\\[\n\\hat{\\beta}_{ridge} = (\\mathbf{X}^T \\mathbf{X} + \\lambda I)^{-1} \\mathbf{X}^T \\mathbf{Y}.\n\\]\ndonde \\(I\\) es la matriz identidad y \\(\\lambda \\geq 0\\) es un hiperparámetro que controla la cantidad de penalización aplicada.\nInterpretación del parámetro \\(\\lambda\\)\n\nSi \\(\\lambda = 0\\), el modelo Ridge es equivalente a la regresión lineal tradicional (OLS).\nA medida que \\(\\lambda\\) aumenta, los coeficientes \\(\\beta_j\\) se reducen en magnitud, lo que ayuda a controlar la varianza del modelo y a prevenir el sobreajuste.\nSi \\(\\lambda\\) es demasiado grande, los coeficientes se acercan a cero y el modelo puede perder interpretabilidad.\n\nLa elección óptima de \\(\\lambda\\) se determina generalmente mediante validación cruzada.\n\n\n\n\n\n\nAviso\n\n\n\nLos detalles de la validación cruzada son tratados en la asignatura de Aprendizaje Automático.\n\n\n\n\n\n\n\n\nPropiedades Clave\n\n\n\n\n\n\nManejo de la multicolinealidad: La regularización reduce la sensibilidad del modelo cuando los predictores están altamente correlacionados.\nMenor varianza en las predicciones: El modelo Ridge tiende a ser más estable en comparación con OLS, lo que mejora la capacidad de generalización en conjuntos de datos nuevos.\nNo realiza selección de variables: A diferencia de Lasso, Ridge no anula coeficientes, sino que reduce su magnitud. Esto es útil cuando se sospecha que todas las variables tienen algún grado de importancia en el modelo.\n\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Cargar librerías\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-8\n\n# Datos simulados\nset.seed(123)\nX &lt;- matrix(rnorm(100 * 10), 100, 10)  # 100 observaciones, 10 predictores\nY &lt;- X %*% rnorm(10) + rnorm(100)  # Variable de respuesta con ruido\n\n# Ajustar modelo Ridge\nmodelo_ridge &lt;- glmnet(X, Y, alpha = 0)  # alpha = 0 indica regresión Ridge\n\n# Seleccionar lambda óptimo con validación cruzada\ncv_ridge &lt;- cv.glmnet(X, Y, alpha = 0)\nlambda_optimo &lt;- cv_ridge$lambda.min  # Mejor valor de lambda\n\nprint(lambda_optimo)\n\n[1] 0.2583753\n\n# Ajustar modelo final con lambda óptimo\nmodelo_ridge_final &lt;- glmnet(X, Y, alpha = 0, lambda = lambda_optimo)\n\nmodelo_ridge_final\n\n\nCall:  glmnet(x = X, y = Y, alpha = 0, lambda = lambda_optimo) \n\n  Df  %Dev Lambda\n1 10 93.55 0.2584\n\n# Comparación modelo clásico\n\nmodelo_lm &lt;- lm(Y~X)\n\n# Mostrar coeficientes\noutput=cbind(round(coef(modelo_ridge_final),3),\n            round(coef(modelo_lm),3))\n\ncolnames(output)=c(\"RIDGE\",\"OLS\")\n\noutput\n\n11 x 2 sparse Matrix of class \"dgCMatrix\"\n             RIDGE    OLS\n(Intercept)  0.118  0.132\nV1          -0.874 -0.995\nV2          -1.019 -1.131\nV3           0.040  0.039\nV4           0.002  0.001\nV5          -2.500 -2.703\nV6           1.001  1.104\nV7           0.247  0.274\nV8           2.125  2.244\nV9           0.635  0.658\nV10         -0.390 -0.427\n\n\n\n\n\n\nLa regresión Ridge es una técnica poderosa para mejorar la estabilidad de los modelos de regresión en presencia de multicolinealidad. A diferencia de OLS, que puede generar coeficientes inestables, Ridge introduce una penalización que reduce la magnitud de los coeficientes, evitando valores extremos. Aunque Ridge no realiza selección de variables, su capacidad para reducir la varianza y mejorar la capacidad predictiva lo convierte en una herramienta esencial en el análisis de datos modernos.\nEn la siguiente sección, exploraremos la regresión Lasso, que extiende este concepto permitiendo la eliminación de variables irrelevantes del modelo.\n\n\n3.6.2 Regresión Lasso\nCuando se tiene un conjunto de predictores con posibles redundancias o ruido, Lasso permite identificar cuáles son las variables más relevantes para el modelo, lo que facilita la interpretación y reduce la complejidad del análisis.\nAl igual que ocurría en Ridge Regression, el modelo de regresión Lasso se basa en la minimización de la siguiente función de error (Ranstam y Cook 2018): \\[\nSSE_{lasso} = \\| \\mathbf{Y}- \\mathbf{X} \\beta \\|^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n\\]\ndonde el término de penalización, (\\(L_1=\\sum |\\beta_j|\\)) hace que algunos coeficientes se reduzcan exactamente a cero, lo que significa que esas variables son eliminadas del modelo.\nLa diferencia clave con Ridge Regressión, visto anteriormente, es que Ridge reduce la magnitud de los coeficientes pero no los anula, mientras que Lasso puede eliminar variables por completo.\nInterpretación del parámetro \\(\\lambda\\)\n\nSi \\(\\lambda = 0\\), el modelo es equivalente a la regresión lineal tradicional (OLS).\nA medida que \\(\\lambda\\) aumenta, más coeficientes se reducen a cero, lo que equivale a realizar selección de variables.\nSi \\(\\lambda\\) es demasiado grande, se eliminan demasiadas variables, lo que puede resultar en un modelo subóptimo.\n\nAl igual que en el método Risge, la selección óptima de \\(\\lambda\\) se realiza generalmente mediante validación cruzada.\n\n\n\n\n\n\n\nPropiedades Clave\n\n\n\n\n\n\nSelección de variables automática: Lasso no solo regulariza, sino que también selecciona las variables más importantes eliminando aquellas menos relevantes.\nManejo de la multicolinealidad: Puede mejorar la interpretación del modelo cuando hay muchas variables correlacionadas.\nSimplicidad y interpretabilidad: Un modelo con menos variables es más fácil de interpretar y aplicar en la práctica.\nReduce el sobreajuste: La penalización \\(L_1\\) evita que el modelo se ajuste demasiado a los datos de entrenamiento, mejorando su capacidad predictiva en datos nuevos.\n\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Ajustar modelo Lasso\nmodelo_lasso &lt;- glmnet(X, Y, alpha = 1)  # alpha = 1 indica regresión Lasso\n\n# Seleccionar lambda óptimo con validación cruzada\ncv_lasso &lt;- cv.glmnet(X, Y, alpha = 1)\nlambda_optimo &lt;- cv_lasso$lambda.min  # Mejor valor de lambda\n\nprint(lambda_optimo)\n\n[1] 0.03260326\n\n# Ajustar modelo final con lambda óptimo\nmodelo_lasso_final &lt;- glmnet(X, Y, alpha = 1, lambda = lambda_optimo)\n\n\n# Mostrar coeficientes\noutput=cbind(round(coef(modelo_lasso_final),3),output)\n\ncolnames(output)=c(\"LASSO\",\"RIDGE\",\"OLS\")\n\noutput\n\n11 x 3 sparse Matrix of class \"dgCMatrix\"\n             LASSO  RIDGE    OLS\n(Intercept)  0.131  0.118  0.132\nV1          -0.950 -0.874 -0.995\nV2          -1.078 -1.019 -1.131\nV3           0.006  0.040  0.039\nV4           .      0.002  0.001\nV5          -2.652 -2.500 -2.703\nV6           1.058  1.001  1.104\nV7           0.235  0.247  0.274\nV8           2.213  2.125  2.244\nV9           0.629  0.635  0.658\nV10         -0.392 -0.390 -0.427\n\n\n\n\n\nConsideraciones Importantes\nLa regresión Lasso es una poderosa técnica de regularización que no solo mejora la estabilidad del modelo en presencia de muchas variables predictoras, sino que también realiza una selección automática de las más relevantes. Su capacidad para reducir coeficientes a cero la convierte en una herramienta esencial en el análisis de datos de alta dimensión.\n\nLasso puede eliminar demasiadas variables si \\(\\lambda\\) es demasiado grande, lo que puede llevar a la pérdida de información importante.\nNo maneja bien grupos de predictores altamente correlacionados, ya que selecciona solo uno de ellos y elimina los demás.\nElastic Net, que combina Ridge y Lasso, puede ser una mejor opción cuando hay multicolinealidad fuerte en los datos.\n\nEn la siguiente sección, exploraremos Elastic Net, una técnica híbrida que combina las ventajas de Ridge y Lasso para mejorar la selección de variables en presencia de predictores altamente correlacionados.\n\n\n3.6.3 Elastic Net\nLa regresión Elastic Net es una técnica de regularización que combina las propiedades de Ridge y Lasso, abordando algunas de sus limitaciones individuales (Zou y Hastie 2005). Mientras que Ridge es útil para manejar la multicolinealidad sin eliminar variables y Lasso selecciona un subconjunto de predictores, Elastic Net equilibra ambos enfoques permitiendo la selección de variables en presencia de alta correlación entre los predictores.\nEste método es particularmente efectivo cuando el número de predictores es grande y existe multicolinealidad, ya que permite controlar simultáneamente la reducción de la magnitud de los coeficientes y la eliminación de variables irrelevantes.\nElastic Net introduce una penalización que combina los términos de Ridge (\\(L_2\\)) y Lasso (\\(L_1\\)):\n\\[\nSSE_{\\text{Elastic Net}} = \\| Y - X \\beta \\|^2 + \\lambda_1 \\sum_{j=1}^{p} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{p} \\beta_j^2\n\\]\ndonde:\n\n\\(\\lambda_1\\) (asociado a Lasso) controla la cantidad de coeficientes que se reducen a cero.\n\\(\\lambda_2\\) (asociado a Ridge) controla la reducción de magnitud de los coeficientes sin anularlos.\n\\(\\alpha\\) es un parámetro adicional que pondera la combinación entre Lasso y Ridge, con:\n\n\\(\\alpha = 1\\) → Elastic Net se comporta como Lasso.\n\\(\\alpha = 0\\) → Elastic Net se comporta como Ridge.\n\\(0 &lt; \\alpha &lt; 1\\) → Elastic Net combina ambos métodos.\n\n\nLa estimación de los coeficientes en Elastic Net se obtiene resolviendo:\n\\[\n\\hat{\\beta}_{\\text{Elastic Net}} = \\arg \\min_{\\beta} \\left( \\| Y - X \\beta \\|^2 + \\lambda \\left( \\alpha \\sum |\\beta_j| + (1 - \\alpha) \\sum \\beta_j^2 \\right) \\right)\n\\]\n\n\n\n\n\n\n\nPropiedades Clave\n\n\n\n\n\n\nManejo de la Multicolinealidad: A diferencia de Lasso, que selecciona solo una de las variables correlacionadas y elimina las demás, Elastic Net distribuye la penalización entre todas las variables correlacionadas, evitando una selección arbitraria.\nSelección de variables más estable: La combinación de Lasso y Ridge permite una selección más robusta, manteniendo información relevante del modelo sin eliminar predictores clave.\nMejora del rendimiento predictivo: Al utilizar validación cruzada para seleccionar los hiperparámetros \\(\\lambda_1\\), \\(\\lambda_2\\) y \\(\\alpha\\), se optimiza la capacidad del modelo para generalizar a nuevos datos.\n\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Ajustar modelo Elastic Net\nmodelo_elastic_net &lt;- glmnet(X, Y, alpha = 0.5)  # Alpha = 0.5 (50% Ridge, 50% Lasso)\n\n# Seleccionar lambda óptimo con validación cruzada\ncv_elastic_net &lt;- cv.glmnet(X, Y, alpha = 0.5)\nlambda_optimo &lt;- cv_elastic_net$lambda.min  # Mejor valor de lambda\n\nprint(lambda_optimo)\n\n[1] 0.0213522\n\n# Ajustar modelo final con lambda óptimo\nmodelo_elastic_final &lt;- glmnet(X, Y, alpha = 0.5, lambda = lambda_optimo)\n\n# Mostrar coeficientes\noutput=cbind(round(coef(modelo_elastic_final),3),output)\n\ncolnames(output)=c(\"ELASTIC\",\"LASSO\",\"RIDGE\",\"OLS\")\n\noutput\n\n11 x 4 sparse Matrix of class \"dgCMatrix\"\n            ELASTIC  LASSO  RIDGE    OLS\n(Intercept)   0.131  0.131  0.118  0.132\nV1           -0.975 -0.950 -0.874 -0.995\nV2           -1.108 -1.078 -1.019 -1.131\nV3            0.028  0.006  0.040  0.039\nV4            .      .      0.002  0.001\nV5           -2.677 -2.652 -2.500 -2.703\nV6            1.084  1.058  1.001  1.104\nV7            0.260  0.235  0.247  0.274\nV8            2.229  2.213  2.125  2.244\nV9            0.647  0.629  0.635  0.658\nV10          -0.414 -0.392 -0.390 -0.427\n\n\n\n\n\nPara determinar el mejor valor de \\(\\alpha\\), se usa validación cruzada probando distintos valores entre \\(0\\) y 1. Algunas estrategias comunes incluyen:\n\nSi hay muchas variables irrelevantes, se recomienda \\(\\alpha\\) cercano a 1 (Lasso).\nSi hay fuerte multicolinealidad, se recomienda \\(\\alpha\\) cercano a 0 (Ridge).\nSi se desea un balance entre selección y estabilidad, se suele usar \\(\\alpha = 0.5\\).\n\nLa regresión Elastic Net combina lo mejor de Ridge y Lasso, ofreciendo un método de regularización robusto para modelos con muchas variables predictoras y posible multicolinealidad. Su capacidad para seleccionar variables sin eliminar información clave lo convierte en una opción ideal para modelos complejos y de alta dimensionalidad.\n\n\n3.6.4 Comparación de los métodos de Regularización\n\n\n\n\n\n\n\n\nMétodo\nPenalización\nEfecto sobre los coeficientes\n\n\n\n\nOLS\nNinguna\nSin restricción, puede haber multicolinealidad\n\n\nRidge\n\\(L_2\\)\nReduce la magnitud de los coeficientes, pero no los anula\n\n\nLasso\n\\(L_1\\)\nPuede anular coeficientes, permitiendo selección de variables\n\n\nElastic Net\n\\(L_1 + L_2\\)\nCombinación de Ridge y Lasso\n\n\n\n\nLasso es especialmente útil cuando se sospecha que muchas variables son irrelevantes, mientras que Ridge es preferido cuando se espera que todas las variables aporten información al modelo.\nElastic Net es ideal cuando hay muchas variables correlacionadas y se desea un modelo estable y parsimonioso.\n\nElastic Net mejora la estabilidad del modelo en comparación con Lasso, especialmente cuando hay variables predictoras altamente correlacionadas.\nEs más flexible que Ridge y Lasso individualmente, permitiendo un ajuste más fino a distintos tipos de problemas.\nRequiere la selección de hiperparámetros (\\(\\lambda\\) y \\(\\alpha\\)), por lo que debe usarse validación cruzada para encontrar la combinación óptima.\n\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al. 2013. An introduction to statistical learning. Vol. 112. Springer.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, y William Li. 2005. Applied linear statistical models. McGraw-hill.\n\n\nMarquardt, Donald W, y Ronald D Snee. 1975. «Ridge regression in practice». The American Statistician 29 (1): 3-20.\n\n\nRanstam, Jonas, y Jonathan A Cook. 2018. «LASSO regression». Journal of British Surgery 105 (10): 1348-48.\n\n\nZou, Hui, y Trevor Hastie. 2005. «Regularization and variable selection via the elastic net». Journal of the Royal Statistical Society Series B: Statistical Methodology 67 (2): 301-20.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Métodos de selección de variables y problemas de regularización</span>"
    ]
  },
  {
    "objectID": "tema2.html#resumen-del-proceso-de-construcción-del-modelo-de-regresión-múltiple",
    "href": "tema2.html#resumen-del-proceso-de-construcción-del-modelo-de-regresión-múltiple",
    "title": "2  Métodos de selección de variables y problemas de regularización",
    "section": "",
    "text": "Definición del problema y variables de interés:\n\nIdentificar claramente el objetivo del análisis, ya sea realizar predicciones, evaluar relaciones o controlar por efectos de variables confusoras.\nSeleccionar las variables predictoras potenciales en función de su relevancia teórica, conocimiento previo o exploración inicial de los datos.\n\nAnálisis Exploratorio de Datos (EDA):\n\nInspeccionar los datos mediante análisis descriptivo y visual para identificar posibles problemas como valores atípicos, datos faltantes y multicolinealidad.\nEscalar o transformar las variables si es necesario, especialmente si están en diferentes escalas o presentan distribuciones no lineales.\n\nAjuste del modelo:\n\nEspecificar el modelo de regresión múltiple en su forma general:\n\\[\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p + \\varepsilon,\n\\] donde \\(\\varepsilon\\) representa los errores aleatorios.\nEstimar los coeficientes del modelo (\\(\\beta_0, \\beta_1, \\dots, \\beta_p\\)) utilizando el método de mínimos cuadrados, que minimiza la suma de los errores al cuadrado.\n\nEvaluación del modelo:\n\nAnalizar el ajuste general del modelo utilizando métricas como \\(R^2\\) y \\(R^2\\) ajustado, que miden la proporción de la variabilidad explicada.\nExaminar la tabla ANOVA para evaluar la significancia global del modelo.\nRealizar pruebas de hipótesis para los coeficientes individuales, verificando si las variables predictoras tienen un efecto significativo en la variable respuesta.\n\nDiagnóstico del modelo:\n\nExaminar los residuos para evaluar supuestos como la linealidad, homocedasticidad, normalidad de los errores y ausencia de autocorrelación.\nIdentificar observaciones atípicas, leverage y puntos de influencia utilizando herramientas como la distancia de Cook, DFBETAS y DFFITS.\n\nSelección de variables:\n\nSimplificar el modelo eliminando variables irrelevantes mediante métodos de selección de variables (directos, automáticos o por regularización) para mejorar la parsimonia y evitar el sobreajuste.\nLos métodos de selección de variables permiten identificar subconjuntos óptimos de predictores, mejorando tanto la simplicidad como la precisión del modelo. Además, discutiremos los problemas de regularización, donde se introducen penalizaciones al modelo para controlar la complejidad y prevenir el sobreajuste. Estos métodos, como Ridge, Lasso y Elastic Net, se han convertido en herramientas esenciales en el análisis de datos modernos.\n\nValidación del modelo:\n\nEvaluar el desempeño del modelo con datos de validación o mediante técnicas como validación cruzada para garantizar su capacidad predictiva en nuevos conjuntos de datos.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de selección de variables y problemas de regularización</span>"
    ]
  },
  {
    "objectID": "tema2.html#proceso-de-construcción-del-modelo-de-regresión",
    "href": "tema2.html#proceso-de-construcción-del-modelo-de-regresión",
    "title": "3  Métodos de selección de variables y problemas de regularización",
    "section": "",
    "text": "Definición del problema y variables de interés:\n\nIdentificar claramente el objetivo del análisis, ya sea realizar predicciones, evaluar relaciones o controlar por efectos de variables confusoras.\nSeleccionar las variables predictoras potenciales en función de su relevancia teórica, conocimiento previo o exploración inicial de los datos.\n\nRecogida de datos:\n\n\nLa calidad de los datos recogidos influye directamente en la validez de los resultados y conclusiones obtenidas. El proceso de recogida de datos consiste en recopilar información de manera organizada y sistemática para responder a las preguntas de investigación planteadas. Dependiendo del diseño del estudio y los objetivos del análisis, se pueden emplear diferentes tipos de experimentos o métodos de recogida de datos.\nDebemos asegurar las siguientes características sobre los datos.\n\nFiabilidad: Asegurar que los datos sean consistentes y puedan reproducirse bajo condiciones similares.\nValidez: Garantizar que los datos recojan realmente la información necesaria para responder a las preguntas de investigación.\nÉtica: Asegurar la privacidad y el consentimiento informado de los participantes.\nControl de Sesgos: Diseñar el estudio de manera que se minimicen los sesgos que puedan distorsionar los resultados.\n\n\n\n\n\n\n\n\nTipos de experimentos\n\n\n\n\n\nLa elección del tipo de experimento o método de recogida de datos dependerá de la naturaleza del problema a investigar, los recursos disponibles y las limitaciones del estudio. Una correcta planificación y ejecución de esta etapa sienta las bases para un análisis robusto y confiable.\n\nExperimentos controlados:\n\nLos experimentos controlados son diseñados de manera que los investigadores manipulan deliberadamente una o más variables independientes (llamadas factores o variables controladas) para observar su efecto en la variable dependiente.\nIncluyen la aleatorización de sujetos entre grupos (por ejemplo, grupos de control y tratamiento) para minimizar sesgos y asegurar comparabilidad.\nEn muchas ocasiones la información suplementaria no se puede incorporar en el diseño del experimento. A esas variables, no controladas, se les suel llamar covariables.\nEjemplo: Un estudio clínico donde se prueba un nuevo medicamento y se compara su efecto con un placebo.\n\nEstudios observacionales exploratorios:\n\nEn este enfoque, los datos se recogen sin intervenir ni manipular las condiciones. Los investigadores observan y registran los fenómenos tal como ocurren en la naturaleza.\nPueden clasificarse en:\n\nEstudios transversales: Los datos se recogen en un único punto temporal.\nEstudios longitudinales: Los datos se recogen durante un periodo para analizar cambios a lo largo del tiempo.\n\nEjemplo: Investigar los hábitos alimenticios y su asociación con enfermedades cardiovasculares en una población.\n\nEstudios observacionales confirmatorios:\n\nEn este enfoque, los datos se recogen para testear (confirmar o no) hipótesis derivadas de estudios previos o de ideas que pueden tener los investigadores.\nEn este contexto, las variables que aparecen involucradas en la hipótesis que se quiere confirmar se denominan variables primarias, y las variables explicativas que se sabe inluyen en la respuesta se llaman variables de control (en Epidemiología nos referimos a ellas como factores de riesgo)\nEjemplo: Un equipo de investigadores, basándose en estudios previos, plantea la hipótesis de que existe una relación positiva entre el hábito de fumar (variable explicativa principal) y la incidencia de cáncer de pulmón (variable respuesta). Para confirmar esta hipótesis, realizan un estudio observacional en el que recopilan datos de una población durante un periodo determinado. Dado que no es ético inducir a las personas a fumar para realizar un experimento controlado, este estudio se realiza de forma observacional. Los datos se analizan para evaluar la asociación entre las variables, permitiendo confirmar (o refutar) la hipótesis planteada con un diseño adecuado y controlando los posibles factores de confusión.\n\nEncuestas y cuestionarios:\n\nLas encuestas son una técnica común para recoger datos de manera estructurada sobre actitudes, opiniones, comportamientos o características demográficas.\nPueden aplicarse en formato presencial, en línea, por teléfono o mediante correo.\nEjemplo: Una encuesta para medir el grado de satisfacción de los clientes con un servicio.\n\nExperimentos naturales:\n\nSe producen cuando un fenómeno natural o social actúa como una intervención en un entorno sin que los investigadores tengan control sobre el experimento.\nEste tipo de estudio aprovecha eventos únicos para analizar sus impactos.\nEjemplo: Estudiar los efectos económicos de una nueva política fiscal aplicada en una región específica.\n\nEstudios de simulación:\n\nLos datos se generan a través de modelos matemáticos o computacionales que representan un sistema real o hipotético.\nEste método se usa cuando es difícil o costoso realizar experimentos reales.\nEjemplo: Simular el comportamiento de un mercado financiero bajo diferentes escenarios económicos.\n\nRecogida de datos secundarios:\n\nEn lugar de recoger datos nuevos, se utilizan datos ya existentes recopilados por terceros, como censos, registros administrativos o bases de datos públicas.\nAunque es eficiente en tiempo y costos, el investigador tiene menor control sobre la calidad y las características de los datos.\nEjemplo: Analizar datos de encuestas nacionales para estudiar tendencias sociales.\n\n\n\n\n\n\nAnálisis Exploratorio de Datos (EDA):\n\nInspeccionar los datos mediante análisis descriptivo y visual para identificar posibles problemas como valores atípicos, datos faltantes y multicolinealidad.\nEscalar o transformar las variables si es necesario, especialmente si están en diferentes escalas o presentan distribuciones no lineales.\n\nAjuste del modelo:\n\nEspecificar el modelo de regresión múltiple en su forma general:\n\\[\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p + \\varepsilon,\n\\] donde \\(\\varepsilon\\) representa los errores aleatorios.\nEstimar los coeficientes del modelo (\\(\\beta_0, \\beta_1, \\dots, \\beta_p\\)) utilizando el método de mínimos cuadrados, que minimiza la suma de los errores al cuadrado.\n\nEvaluación del modelo:\n\nAnalizar el ajuste general del modelo utilizando métricas como \\(R^2\\) y \\(R^2\\) ajustado, que miden la proporción de la variabilidad explicada.\nExaminar la tabla ANOVA para evaluar la significancia global del modelo.\nRealizar pruebas de hipótesis para los coeficientes individuales, verificando si las variables predictoras tienen un efecto significativo en la variable respuesta.\n\nDiagnóstico del modelo:\n\nExaminar los residuos para evaluar supuestos como la linealidad, homocedasticidad, normalidad de los errores y ausencia de autocorrelación.\nIdentificar observaciones atípicas, leverage y puntos de influencia utilizando herramientas como la distancia de Cook, DFBETAS y DFFITS.\n\nReducción de variables:\n\nEn análisis de regresión, especialmente cuando se trabaja con conjuntos de datos de alta dimensionalidad, es común enfrentar situaciones en las que el número de variables explicativas es muy grande. Esto puede llevar a problemas como el sobreajuste, dificultades en la interpretación del modelo y una mayor complejidad computacional. Por ello, reducir el número de variables explicativas, sin perder información relevante, se convierte en un paso crucial para construir modelos más eficientes y robustos.\n\nValidación del modelo:\n\nEvaluar el desempeño del modelo con datos de validación o mediante técnicas como validación cruzada para garantizar su capacidad predictiva en nuevos conjuntos de datos.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Métodos de selección de variables y problemas de regularización</span>"
    ]
  },
  {
    "objectID": "tema2.html#selección-de-variables",
    "href": "tema2.html#selección-de-variables",
    "title": "3  Métodos de selección de variables y problemas de regularización",
    "section": "3.3 Selección de variables",
    "text": "3.3 Selección de variables\nSimplificar un modelo eliminando variables irrelevantes es fundamental para mejorar su parsimonia y evitar el sobreajuste. Este objetivo puede lograrse mediante métodos de selección de variables, ya sean directos, automáticos o basados en regularización. Estas técnicas permiten identificar subconjuntos óptimos de predictores, optimizando tanto la simplicidad como la precisión del modelo. En particular, los métodos de regularización, como Ridge, Lasso y Elastic Net, introducen penalizaciones al modelo para controlar la complejidad y prevenir el sobreajuste, convirtiéndose en herramientas clave en el análisis de datos modernos.\nCuando se dispone de \\(p\\) variables explicativas, es posible construir hasta \\(2^p\\) modelos diferentes considerando todas las combinaciones posibles de estas variables. Sin embargo, explorar de manera exhaustiva todos estos modelos puede ser inviable, especialmente si \\(p\\) es grande. Por ejemplo, con solo 10 variables regresoras, se generarían \\(2^{10} = 1024\\) modelos posibles. Aunque la tecnología actual permite ajustar todos estos modelos, evaluar cada uno en términos de bondad de ajuste, gráficos de residuos, detección de observaciones influyentes y otros diagnósticos sería extremadamente complejo y costoso.\nPara superar este desafío, se han desarrollado criterios específicos de selección de variables que ayudan a los analistas a identificar un pequeño subconjunto de modelos que cumplan con los estándares de calidad deseados. Este enfoque permite centrar el análisis en un grupo reducido de modelos “buenos”, generalmente entre 4 y 6, y realizar un estudio más profundo y detallado de ellos. Esta estrategia facilita tanto la interpretación como la eficiencia del proceso analítico, optimizando el uso de recursos computacionales y asegurando que los resultados sean robustos y fiables.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Métodos de selección de variables y problemas de regularización</span>"
    ]
  },
  {
    "objectID": "tema2.html#reducción-de-variables",
    "href": "tema2.html#reducción-de-variables",
    "title": "3  Métodos de selección de variables y problemas de regularización",
    "section": "3.2 Reducción de variables",
    "text": "3.2 Reducción de variables\nEn análisis de regresión, especialmente cuando se trabaja con conjuntos de datos de alta dimensionalidad, es común enfrentar situaciones en las que el número de variables explicativas es muy grande. Esto puede llevar a problemas como el sobreajuste, dificultades en la interpretación del modelo y una mayor complejidad computacional. Por ello, reducir el número de variables explicativas, sin perder información relevante, se convierte en un paso crucial para construir modelos más eficientes y robustos.\nEs especialmente importante reducir el número de variables explicativas en los estudios observacionales exploratorios, ya que en otros tipos de estudios, como los diseñados previamente, las variables incluidas suelen estar seleccionadas de antemano porque se conoce su relación con la variable respuesta o porque han sido identificadas como relevantes en investigaciones previas.\nLa reducción de variables explicativas busca simplificar el modelo al seleccionar un subconjunto de predictores que capturen la mayor parte de la información relevante de los datos. Este proceso puede realizarse a través de diferentes enfoques, dependiendo del contexto y de las características del conjunto de datos.\n\n3.2.1 Motivaciones para reducir variables\nAl limitar el número de predictores, no solo se simplifica el modelo, sino que también se optimizan diversos aspectos fundamentales en el análisis.\n\nEvitar el sobreajuste:\n\nCuando hay demasiadas variables en relación al número de observaciones, el modelo puede ajustarse demasiado a los datos de entrenamiento y perder capacidad predictiva en nuevos conjuntos de datos.\n\nMejorar la interpretabilidad:\n\nUn modelo con menos variables es más fácil de interpretar, lo que resulta fundamental en aplicaciones como ciencias sociales, biomedicina o economía.\n\nReducción de complejidad computacional:\n\nAl disminuir el número de variables, se reducen los costos de tiempo y memoria en el ajuste y evaluación del modelo.\n\nManejo de multicolinealidad:\n\nLa reducción puede eliminar variables redundantes que presentan una alta correlación entre sí, estabilizando las estimaciones del modelo.\n\n\n\n\n3.2.2 Métodos de reducción de variables\nAlgunas de las ideas más comunes para tratar de reducir el número de variables de un modelo son:\n\nSelección de variables:\n\nUtiliza estrategias como selección directa, métodos automáticos (forward, backward o stepwise), o técnicas basadas en regularización (Lasso, Elastic Net) para seleccionar las variables más relevantes.\n\nTécnicas de transformación de datos:\n\nSe proyectan las variables explicativas en un nuevo espacio de menor dimensionalidad, manteniendo la mayor cantidad posible de información. Algunas de estas técnicas se estudian en la asignatura de Aprendizaje Automático:\n\nAnálisis de Componentes Principales (PCA): Reduce las variables explicativas a un conjunto de componentes ortogonales que explican la mayor parte de la varianza.\nAnálisis de Factores: Agrupa variables relacionadas en factores latentes que capturan la esencia de la información.\n\n\nFiltrado basado en información:\n\nIdentifica y descarta variables con baja variabilidad o poca relación con la variable respuesta, utilizando métricas como la correlación o importancia estadística.\n\nMétodos de selección basados en modelos:\n\nAjusta modelos iterativamente para evaluar la contribución de cada variable explicativa y descartar aquellas con menor relevancia según criterios como el \\(p\\)-valor, AIC o BIC.\n\n\n\n\n\n\n\n\nConsideraciones importantes\n\n\n\n\nLa reducción de variables debe realizarse cuidadosamente para evitar la pérdida de información clave que pueda comprometer la calidad del modelo.\nEs fundamental validar el modelo resultante, asegurándose de que mantenga su capacidad predictiva mediante técnicas como validación cruzada.\nEn algunos casos, la selección o transformación de variables puede implicar compromisos entre simplicidad e interpretabilidad.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Métodos de selección de variables y problemas de regularización</span>"
    ]
  },
  {
    "objectID": "tema3.html",
    "href": "tema3.html",
    "title": "3  Modelos no lineales. Transformación de variables. Ingeniería de características.",
    "section": "",
    "text": "3.1 Modelos no lineales\nLos modelos de regresión no lineal son herramientas esenciales cuando las relaciones entre las variables no pueden capturarse adecuadamente con modelos lineales. La regresión polinómica, exponencial, logarítmica y los modelos por tramos ofrecen diferentes enfoques para representar patrones complejos en los datos. Comprender cuándo y cómo aplicar estos modelos es fundamental para mejorar la precisión y la interpretabilidad en el análisis de datos.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos no lineales. Transformación de variables. Ingeniería de características.</span>"
    ]
  },
  {
    "objectID": "tema3.html#modelos-no-lineales",
    "href": "tema3.html#modelos-no-lineales",
    "title": "3  Modelos no lineales. Transformación de variables. Ingeniería de características.",
    "section": "",
    "text": "3.1.1 Regresión Polinómica\nLa regresión polinómica es una extensión de la regresión lineal que permite capturar relaciones no lineales mediante la inclusión de términos polinómicos (cuadráticos, cúbicos, etc.). Aunque sigue siendo un modelo lineal en los parámetros, la inclusión de potencias de las variables independientes permite ajustar curvas en lugar de líneas rectas.\n\\[\nY = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\dots + \\beta_k X^k + \\varepsilon\n\\]\nDonde \\(k\\) es el grado del polinomio. A medida que aumenta el grado, el modelo se vuelve más flexible y puede ajustarse a relaciones más complejas.\nEste tipo de modelos son capaces de capturar curvaturas suaves en los datos. Se trata de modelos fáciles de implementar y comprender, aunque la interpretación de los coeficientes asociados a altos grados del polinomio puede ser compleja. De hecho, exite un claro riesgo de sobreajuste cuando se utilizan polinomios de alto grado.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Datos simulados\nset.seed(123)\nx &lt;- 1:20\ny &lt;- 3 + 2 * x + 0.5 * x^2 + rnorm(20, mean = 0, sd = 10)\n\n# Ajuste del modelo polinómico de grado 2\nmodelo_polinomico &lt;- lm(y ~ poly(x, 2))\n\nsummary(modelo_polinomico)\n\n\nCall:\nlm(formula = y ~ poly(x, 2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.9643  -6.4011  -0.8541   5.8504  17.2160 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    97.17       2.27  42.796  &lt; 2e-16 ***\npoly(x, 2)1   318.21      10.15  31.339  &lt; 2e-16 ***\npoly(x, 2)2    60.98      10.15   6.006 1.42e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.15 on 17 degrees of freedom\nMultiple R-squared:  0.9836,    Adjusted R-squared:  0.9816 \nF-statistic: 509.1 on 2 and 17 DF,  p-value: 6.778e-16\n\n# Visualización\nplot(x, y, main = \"Regresión Polinómica de Segundo Grado\", pch = 19, col = \"blue\")\nlines(x, predict(modelo_polinomico), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1.2 Modelos de Regresión Exponencial y Logarítmica\nCuando la relación entre la variable dependiente y la independiente sigue un crecimiento o decaimiento exponencial, o una relación logarítmica, los modelos lineales tradicionales no son suficientes. En estos casos, se pueden utilizar transformaciones exponenciales o logarítmicas.\nRegresión Exponencial\nEste modelo es útil cuando la variable dependiente crece (o decrece) a una tasa proporcional a su valor actual.\n\\[\nY = \\beta_0 e^{\\beta_1 X} + \\varepsilon\n\\]\nEste modelo puede linearizarse tomando el logaritmo de la variable dependiente:\n\\[\n\\log(Y) = \\log(\\beta_0) + \\beta_1 X + \\varepsilon\n\\]\nRegresión Logarítmica\nÚtil cuando la tasa de cambio de la variable dependiente disminuye a medida que aumenta la variable independiente.\n\\[\nY = \\beta_0 + \\beta_1 \\log(X) + \\varepsilon\n\\]\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Datos simulados para un modelo exponencial\nset.seed(123)\nx &lt;- 1:20\ny &lt;- exp(0.3 * x) + rnorm(20, mean = 0, sd = 20)\n\n# Asegurarse de que todos los valores de 'y' sean positivos para aplicar logaritmo\ny[y &lt;= 0] &lt;- min(y[y &gt; 0]) * 0.5  # Reemplaza valores no positivos por un valor pequeño positivo\n\n# Ajuste del modelo exponencial (transformación logarítmica)\nmodelo_exponencial &lt;- lm(log(y) ~ x)\n\n# Resumen del modelo\nsummary(modelo_exponencial)\n\n\nCall:\nlm(formula = log(y) ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9269 -0.1997  0.1612  0.3837  2.6104 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.02791    0.59598   0.047    0.963    \nx            0.29240    0.04975   5.877 1.45e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.283 on 18 degrees of freedom\nMultiple R-squared:  0.6574,    Adjusted R-squared:  0.6384 \nF-statistic: 34.54 on 1 and 18 DF,  p-value: 1.45e-05\n\n# Visualización\nplot(x, y, main = \"Regresión Exponencial\", pch = 19, col = \"blue\", ylab = \"y\", xlab = \"x\")\n\n# Predicciones para los mismos valores de x\npredicciones &lt;- predict(modelo_exponencial, newdata = data.frame(x = x))\n\n# Convertir predicciones a la escala original (exponencial inverso del log)\nlines(x, exp(predicciones), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1.3 Regresión Spline y modelos basados en Segmentos\nLos splines y los modelos segmentados son técnicas que permiten ajustar relaciones no lineales mediante la división de los datos en segmentos y el ajuste de funciones lineales o polinómicas en cada segmento. Estos métodos son especialmente útiles cuando la relación entre las variables cambia en diferentes rangos de los datos.\nModelos por tramos (Piecewise Regression)\nEn este enfoque, se ajustan diferentes regresiones lineales a distintos rangos de la variable independiente. A diferencia de los splines, las transiciones entre segmentos no necesariamente son suaves.\nEstos modelos permiten capturar relaciones complejas con menor riesgo de sobreajuste en comparación con polinomios de alto grado.\nSplines\nLos splines son una poderosa herramienta en el análisis de regresión para modelar relaciones no lineales entre variables. A diferencia de los modelos polinómicos tradicionales, que ajustan un solo polinomio a todos los datos, los splines permiten dividir el rango de la variable independiente en diferentes tramos y ajustar polinomios separados en cada uno de ellos. Esto proporciona mayor flexibilidad para capturar patrones complejos en los datos, sin los problemas de inestabilidad y sobreajuste que pueden surgir al utilizar polinomios de alto grado.\nUn spline es una función que está compuesta por múltiples polinomios por tramos que se ajustan en diferentes intervalos del dominio de la variable independiente. Estos polinomios están conectados en puntos específicos llamados nudos (knots), que marcan el final de un tramo y el inicio de otro. La principal característica de los splines es que estos polinomios están diseñados para unirse de manera suave en los nudos, asegurando que la función resultante sea continua y, en muchos casos, que sus derivadas también sean continuas.\n\n\n\n\n\n\nElementos clave\n\n\n\n\n\n\nTramos: Intervalos del dominio de la variable independiente en los que se ajusta un polinomio distinto.\nNudos: Puntos donde los tramos se conectan. Los nudos definen la estructura del spline y determinan dónde la función puede cambiar de forma.\nContinuidad: Los splines están construidos para que no haya saltos abruptos en la función o en sus derivadas en los nudos. Por ejemplo, un spline cúbico asegura continuidad en la función, la primera derivada (pendiente) y la segunda derivada (curvatura).\n\n\n\n\n\n\n\n\n\n\nTipos de Splines\n\n\n\n\n\nSplines Lineales:\n\nSe ajustan líneas rectas entre los nudos.\nGarantizan la continuidad en los puntos de unión, pero no necesariamente en la pendiente.\nSon simples, pero pueden generar ángulos abruptos en los nudos.\n\nSplines Cuadráticos:\n\nSe utilizan polinomios de segundo grado en cada tramo.\nAseguran continuidad en la función y en la pendiente, pero no en la curvatura.\n\nSplines Cúbicos:\n\nLos splines cúbicos son los más utilizados en análisis de regresión.\nUtilizan polinomios de tercer grado en cada tramo.\nGarantizan suavidad en la función y en sus primeras dos derivadas, lo que significa que la función es continua, su pendiente es continua y la curvatura es suave.\nEvitan el sobreajuste que puede ocurrir con polinomios de alto grado, proporcionando un ajuste flexible sin perder estabilidad.\nA diferencia de los polinomios globales de alto grado, los splines cúbicos pueden capturar patrones complejos sin oscilar de manera excesiva entre los puntos de datos.\nLos splines permiten que el modelo se adapte localmente a diferentes patrones en distintos tramos del dominio de la variable independiente.\n\nSplines Naturales:\n\nSon una variante de los splines cúbicos que imponen condiciones adicionales en los extremos del rango de los datos, forzando la segunda derivada a ser cero en los extremos. Esto ayuda a evitar oscilaciones no deseadas fuera del rango de los datos.\n\n\n\n\nEl uso de splines en regresión permite modelar relaciones no lineales de manera flexible. La elección del número y la ubicación de los nudos es un aspecto fundamental del ajuste con splines:\n\nNúmero de nudos: Demasiados nudos pueden llevar a un sobreajuste, mientras que muy pocos pueden no capturar adecuadamente la relación entre las variables. El uso de técnicas de validación cruzada puede ayudar a encontrar el equilibrio adecuado.\nUbicación de los nudos: Los nudos pueden colocarse en puntos equidistantes, en cuantiles de la variable independiente, o en puntos donde se sospecha que la relación entre las variables cambia. La colocación de nudos es clave para obtener un buen ajuste. Los nudos pueden seleccionarse de manera automática (por ejemplo, en los cuantiles de la variable independiente) o manualmente según el conocimiento del problema.\n\n\n\n\n\n\n\nAviso\n\n\n\nA diferencia de la regresión lineal simple, los coeficientes de los splines no tienen una interpretación directa. El enfoque se centra en la forma general del ajuste en lugar de en el valor de los coeficientes individuales.\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Cargar librería para splines\nlibrary(splines)\n\n# Datos simulados\nset.seed(123)\nx &lt;- seq(1, 100, by = 1)\ny &lt;- ifelse(x &lt;= 50, 2 * x + rnorm(100, 0, 10), 0.5 * x + rnorm(100, 0, 10))\n\n# Ajuste del modelo spline\nmodelo_spline &lt;- lm(y ~ bs(x, knots = c(30, 60, 80)))\n\n# Visualización\nplot(x, y, main = \"Regresión Spline\", pch = 19, col = \"blue\")\nlines(x, predict(modelo_spline), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n# Cambio en la posición de los nodos\n# Ajuste del modelo spline\nmodelo_spline &lt;- lm(y ~ bs(x, knots = c(40, 50)))\n\n# Visualización\nplot(x, y, main = \"Regresión Spline\", pch = 19, col = \"blue\")\nlines(x, predict(modelo_spline), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n# Ajustamos un spline cuadrático \n\n# Ajuste del modelo spline\nmodelo_spline &lt;- lm(y ~ bs(x, degree=2, knots = c(30, 60, 80)))\n\n# Visualización\nplot(x, y, main = \"Regresión Spline\", pch = 19, col = \"blue\")\nlines(x, predict(modelo_spline), col = \"red\", lwd = 2)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos no lineales. Transformación de variables. Ingeniería de características.</span>"
    ]
  },
  {
    "objectID": "tema3.html#transformación-de-variables",
    "href": "tema3.html#transformación-de-variables",
    "title": "3  Modelos no lineales. Transformación de variables. Ingeniería de características.",
    "section": "3.2 Transformación de variables",
    "text": "3.2 Transformación de variables\nEn el análisis de datos y la construcción de modelos estadísticos, no siempre es posible capturar adecuadamente la relación entre las variables independientes y la variable dependiente utilizando modelos lineales en su forma original. Aquí es donde entran en juego las transformaciones de variables, que permiten modificar la estructura de los datos para mejorar el ajuste del modelo, cumplir con los supuestos de la regresión y, en muchos casos, facilitar la interpretación.\nLas transformaciones de variables son una herramienta fundamental para mejorar el rendimiento y la precisión de los modelos estadísticos. Estas transformaciones pueden aplicarse tanto a la variable dependiente como a las variables independientes.\n\n\n\n\n\n\nObjetivos\n\n\n\n\n\nLinearizar relaciones no lineales:\nMuchas relaciones entre variables no son lineales en su forma original. Aplicar una transformación adecuada puede convertir una relación no lineal en lineal, permitiendo el uso de técnicas de regresión lineal. Por ejemplo, una relación exponencial \\[Y = \\beta_0 e^{\\beta_1 X}\\] puede linearizarse tomando el logaritmo de \\(Y\\):\n\\[\n   \\log(Y) = \\log(\\beta_0) + \\beta_1 X\n   \\]\nCorregir problemas de heterocedasticidad:\nLa regresión lineal asume que los errores tienen varianza constante (homocedasticidad). Sin embargo, en la práctica, es común encontrar datos con heterocedasticidad (la varianza de los errores cambia con el nivel de la variable independiente). Las transformaciones pueden ayudar a estabilizar la varianza. Por ejemplo, transformar la variable dependiente \\(Y\\) usando un logaritmo o una raíz cuadrada puede reducir la heterocedasticidad.\nNormalizar la distribución de los errores:\nLa regresión lineal también asume que los errores están normalmente distribuidos. Las transformaciones pueden ayudar a que los residuos del modelo se ajusten mejor a una distribución normal, lo que mejora la validez de los intervalos de confianza y las pruebas de hipótesis.\nReducir la influencia de valores atípicos:\nAlgunas transformaciones pueden disminuir la influencia de los valores atípicos en el modelo, haciendo que el ajuste sea más robusto.\nMejorar la interpretabilidad del modelo:\nAunque algunas transformaciones pueden complicar la interpretación directa de los coeficientes, otras pueden facilitar el entendimiento de la relación entre variables (por ejemplo, tasas de crecimiento constantes).\n\n\n\n\n3.2.1 Tipos de transformaciones comunes\nExisten diversas transformaciones que pueden aplicarse a los datos según el problema que se desea abordar. A continuación, se describen las transformaciones más utilizadas en el análisis de regresión.\n\nTransformación Logarítmica (\\(\\log\\))\nSe emplea para linearizar relaciones exponenciales, reducir la heterocedasticidad, y estabilizar la varianza: - \\[ Y = \\log(Y) \\] o \\[ X = \\log(X) \\]\nEs una transformación adecuada cuando la variable tiene una distribución sesgada a la derecha o cuando el efecto marginal disminuye con el valor de la variable (Ingresos, crecimiento poblacional, y tasas de interés, etc).\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Datos simulados con crecimiento exponencial\nset.seed(123)\nx &lt;- 1:20\ny &lt;- exp(0.3 * x) + rnorm(20, mean = 0, sd = 20)\n\n\n# Transformación logarítmica para linearizar la relación\nmodelo_log &lt;- lm(log(y) ~ x)\n\nWarning in log(y): Se han producido NaNs\n\nsummary(modelo_log)\n\n\nCall:\nlm(formula = log(y) ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.00071 -0.11760  0.04277  0.35876  1.73316 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.10652    0.59757   1.852 0.083853 .  \nx            0.22528    0.04655   4.839 0.000217 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 15 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.6096,    Adjusted R-squared:  0.5835 \nF-statistic: 23.42 on 1 and 15 DF,  p-value: 0.0002166\n\n# Visualización\nplot(x, y, main = \"Transformación Log\", pch = 19, col = \"blue\", ylab = \"y\", xlab = \"x\")\n\n# Predicciones para los mismos valores de x\npredicciones &lt;- predict(modelo_log, newdata = data.frame(x = x))\n\n# Convertir predicciones a la escala original (exponencial inverso del log)\nlines(x, exp(predicciones), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\nTransformación de Raíz Cuadrada (\\(\\sqrt{}\\))\nSe emplea para reducir la heterocedasticidad, especialmente cuando la varianza aumenta linealmente con la media: \\[Y = \\sqrt{Y}\\] o \\[X = \\sqrt{X}\\]\nSe emplea comúnmente en conteos de eventos o variables positivas (número de llamadas, defectos, etc.).\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Datos simulados con variabilidad creciente\nset.seed(123)\nx &lt;- 1:50\ny &lt;- x + rnorm(50, mean = 0, sd = x)\n\n# Aplicando raíz cuadrada a la variable dependiente\nmodelo_sqrt &lt;- lm(sqrt(y) ~ x)\n\nWarning in sqrt(y): Se han producido NaNs\n\nsummary(modelo_sqrt)\n\n\nCall:\nlm(formula = sqrt(y) ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4838 -1.3312  0.1822  1.3441  4.4278 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.12028    0.53305   3.978 0.000284 ***\nx            0.11955    0.01819   6.574 7.39e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.752 on 40 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.5193,    Adjusted R-squared:  0.5073 \nF-statistic: 43.22 on 1 and 40 DF,  p-value: 7.387e-08\n\n# Visualización\nplot(x, y, main = \"Transformación SQRT\", pch = 19, col = \"blue\", ylab = \"y\", xlab = \"x\")\n\n# Predicciones para los mismos valores de x\npredicciones &lt;- predict(modelo_sqrt, newdata = data.frame(x = x))\n\n# Convertir predicciones a la escala original\nlines(x, predicciones^2, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\nTransformación Inversa (\\(\\frac{1}{X}\\))\nSe emplea la tranformación inversa para modelar relaciones donde el efecto de la variable independiente disminuye rápidamente. \\[ Y = \\frac{1}{X} \\]\nEs especialmente útil cuando se espera que un aumento en \\(X\\) tenga un efecto decreciente en \\(Y\\) (Relaciones físicas como la ley de la gravitación, velocidad vs. tiempo en fricción, etc).\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Datos simulados con relación inversa\nset.seed(123)\nx &lt;- 1:50\ny &lt;- 1 / x + rnorm(50, mean = 0, sd = 0.05)\n\n# Ajuste del modelo con transformación inversa\nmodelo_inverso &lt;- lm(y ~ I(1/x))\nsummary(modelo_inverso)\n\n\nCall:\nlm(formula = y ~ I(1/x))\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.100193 -0.028703 -0.005628  0.033009  0.106450 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.002092   0.007633   0.274    0.785    \nI(1/x)      0.995869   0.042338  23.522   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.04677 on 48 degrees of freedom\nMultiple R-squared:  0.9202,    Adjusted R-squared:  0.9185 \nF-statistic: 553.3 on 1 and 48 DF,  p-value: &lt; 2.2e-16\n\n# Visualización\nplot(x, y, main = \"Transformación Inversa\", pch = 19, col = \"blue\", ylab = \"y\", xlab = \"x\")\n\n# Predicciones para los mismos valores de x\npredicciones &lt;- predict(modelo_inverso, newdata = data.frame(x = x))\n\n# Convertir predicciones a la escala original\nlines(x, predicciones, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.2 Transformación de Box-Cox\nLa transformación de Box-Cox es un método que busca automáticamente la mejor transformación para estabilizar la varianza y aproximar la normalidad de los errores. La transformación se define como:\n\\[\nY(\\lambda) =\n\\begin{cases}\n\\frac{Y^\\lambda - 1}{\\lambda}, & \\lambda \\neq 0 \\\\\n\\log(Y), & \\lambda = 0\n\\end{cases}\n\\]\nSe emplea para encontrar la transformación óptima para los datos. Se utiliza cuando no está claro qué transformación aplicar. Por ejemplo, en caso de variables continuas con varianza no constante o distribución no normal.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Cargar librería para Box-Cox\nlibrary(MASS)\n\n# Datos simulados\nset.seed(123)\nx &lt;- 1:20\ny &lt;- x^2 + rnorm(20, mean = 0, sd = 5)\n\n# Verificar si hay valores negativos\nmin(y)\n\n[1] -1.802378\n\n# Si hay valores negativos, sumar una constante para que todos los valores sean positivos\nif (min(y) &lt;= 0) {\n  y &lt;- y + abs(min(y)) + 1  # Desplaza todos los valores para que sean positivos\n}\n\n# Ajuste de un modelo lineal simple\nmodelo_bc &lt;- lm(y ~ x)\n\n# Aplicación de la transformación de Box-Cox\nboxcox(modelo_bc)\n\n\n\n\n\n\n\n\n\n\n\nLa gráfica de Box-Cox mostrará el valor óptimo de \\(\\lambda\\), que indica la mejor transformación para los datos.\n\n\n\n3.2.3 Consideracione sobre las transformaciones\nAntes de aplicar transformaciones, es importante diagnosticar si realmente son necesarias. Existen varias herramientas para identificar problemas en los datos que pueden solucionarse con transformaciones:\n\nGráficos de Dispersión: Visualizar la relación entre la variable dependiente y las independientes puede revelar patrones no lineales o heterocedasticidad.\nAnálisis de Residuos: Un gráfico de los residuos frente a los valores ajustados debe mostrar una distribución aleatoria. Patrones sistemáticos o “abanicos” indican la necesidad de transformación. El gráfico de QQ-Plot de los residuos ayuda a evaluar la normalidad.\nPruebas Estadísticas: Pruebas de normalidad como Shapiro-Wilk para los residuos. Pruebas de heterocedasticidad como Breusch-Pagan.\n\nSi bien las transformaciones pueden mejorar el ajuste del modelo, también pueden afectar la interpretación de los coeficientes. Es importante tener en cuenta cómo cambia el significado de los resultados:\n\nTransformaciones en la variable dependiente:\n\nSi aplicas \\(\\log(Y)\\), los coeficientes representan cambios proporcionales en \\(Y\\).\nSi aplicas \\(\\sqrt{Y}\\), los coeficientes representan la tasa de cambio en la raíz cuadrada de \\(Y\\).\n\nTransformaciones en la variable independiente:\n\nSi transformas \\(X\\) con \\(\\log(X)\\), los coeficientes indican cómo cambia \\(Y\\) por cada incremento porcentual en \\(X\\).\nSi transformas \\(X\\) con \\(\\frac{1}{X}\\), los coeficientes representan el cambio en \\(Y\\) por cada unidad de disminución en \\(X\\).\n\nRevertir Transformaciones para Interpretación: Después de ajustar un modelo, es posible transformar las predicciones de nuevo a la escala original para facilitar la interpretación.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos no lineales. Transformación de variables. Ingeniería de características.</span>"
    ]
  },
  {
    "objectID": "tema3.html#ingeniería-de-características",
    "href": "tema3.html#ingeniería-de-características",
    "title": "3  Modelos no lineales. Transformación de variables. Ingeniería de características.",
    "section": "3.3 Ingeniería de características",
    "text": "3.3 Ingeniería de características\nLa ingeniería de características es el arte y la ciencia de transformar los datos brutos en representaciones que faciliten el aprendizaje y mejoren la capacidad predictiva de los modelos. Consiste en el proceso de crear, transformar y seleccionar las variables que se utilizan en un modelo para mejorar su rendimiento. Una característica bien diseñada puede hacer que un modelo simple supere a modelos más complejos, mientras que características irrelevantes o mal definidas pueden degradar significativamente la calidad del análisis. Este proceso incluye:\n\nCreación de nuevas variables a partir de las existentes.\nTransformación de variables para mejorar su distribución o relación con la variable objetivo.\nSelección de las características más relevantes, eliminando aquellas que no aportan valor o introducen ruido.\nPreparación de datos para modelos específicos, asegurando que las variables cumplan con los requisitos del algoritmo (por ejemplo, escalado, normalización o codificación).\n\n\n3.3.1 Creación de nuevas variables\nUna de las tareas más importantes en la ingeniería de características es la creación de nuevas variables que puedan capturar relaciones complejas entre las variables independientes y la variable objetivo.\nLas interacciones entre variables permiten capturar relaciones no lineales entre las variables al considerar cómo el efecto de una variable puede depender del valor de otra. Si se dispone de dos variables \\(X_i\\) y \\(X_j\\), es posible crear una nueva variable de interacción $X_{} = X_i X_j $.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Ejemplo en R de interacción de variables\nset.seed(123)\nX1 &lt;- rnorm(100)\nX2 &lt;- rnorm(100)\nY &lt;- 3 + 2 * X1 + 4 * X2 + 1.5 * X1 * X2 + rnorm(100)\n\n# Crear variable de interacción\nX_interaccion &lt;- X1 * X2\n\n# Ajustar modelo con interacción\nmodelo_interaccion &lt;- lm(Y ~ X1 + X2 + X_interaccion)\nsummary(modelo_interaccion)\n\n\nCall:\nlm(formula = Y ~ X1 + X2 + X_interaccion)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8719 -0.6777 -0.1086  0.5897  2.3166 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    3.14098    0.09578   32.80   &lt;2e-16 ***\nX1             1.90719    0.10834   17.60   &lt;2e-16 ***\nX2             4.03434    0.09881   40.83   &lt;2e-16 ***\nX_interaccion  1.65911    0.11449   14.49   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9468 on 96 degrees of freedom\nMultiple R-squared:  0.953, Adjusted R-squared:  0.9516 \nF-statistic: 649.2 on 3 and 96 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nTal y como estudiamos cuando tratamos el tema de regresión polinómica, agregar términos polinómicos permite capturar relaciones no lineales al incluir potencias de las variables independientes. Por ejemplo, para una variable \\(X\\), se puede crear \\(X^2\\), \\(X^3\\), etc.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Datos simulados\nx &lt;- 1:20\ny &lt;- 5 + 2 * x + 0.5 * x^2 + rnorm(20, mean = 0, sd = 5)\n\n# Incluir término cuadrático en el modelo\nmodelo_polinomico &lt;- lm(y ~ x + I(x^2))\nsummary(modelo_polinomico)\n\n\nCall:\nlm(formula = y ~ x + I(x^2))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.9235 -2.9142  0.6081  3.0085  9.6944 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.49742    4.18141  -0.119  0.90670    \nx            2.85574    0.91705   3.114  0.00631 ** \nI(x^2)       0.47433    0.04242  11.182 2.94e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.62 on 17 degrees of freedom\nMultiple R-squared:  0.9953,    Adjusted R-squared:  0.9947 \nF-statistic:  1792 on 2 and 17 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nCrear combinaciones simples de variables puede capturar relaciones ocultas en los datos. Por ejemplo:\n\nSumas o diferencias: \\(X_{\\text{nuevo}} = X_1 + X_2\\)\nRatios: \\(X_{\\text{ratio}} = \\frac{X_1}{X_2}\\)\nVariables categóricas combinadas: Fusionar categorías relacionadas en una nueva variable. Deben ser categorías que, desde un punto de vista del dominio de aplicación, tenga sentido combinar.\n\n\n\n\n3.3.2 Selección y Reducción de variables\nUna vez que se han creado nuevas características, es importante seleccionar las que son más relevantes para el modelo y eliminar aquellas que no aportan valor o introducen ruido.\nSe trató con detalle estos conceptos en el tema anterior. Planteamos un ejemplo de selección Stepwise.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Datos simulados\nset.seed(123)\nX1 &lt;- rnorm(100)\nX2 &lt;- rnorm(100)\nX3 &lt;- rnorm(100)\nY &lt;- 3 + 2 * X1 + 4 * X2 + rnorm(100)\n\n# Modelo completo con todas las variables\nmodelo_completo &lt;- lm(Y ~ X1 + X2 + X3)\n\n# Selección de variables usando stepwise\nmodelo_seleccionado &lt;- step(modelo_completo, direction = \"both\")\n\nStart:  AIC=13.96\nY ~ X1 + X2 + X3\n\n       Df Sum of Sq     RSS     AIC\n- X3    1      0.29  106.43  12.236\n&lt;none&gt;               106.15  13.964\n- X1    1    306.06  412.21 147.636\n- X2    1   1510.95 1617.09 284.321\n\nStep:  AIC=12.24\nY ~ X1 + X2\n\n       Df Sum of Sq     RSS     AIC\n&lt;none&gt;               106.43  12.236\n+ X3    1      0.29  106.15  13.964\n- X1    1    313.60  420.04 147.517\n- X2    1   1510.83 1617.26 282.332\n\nsummary(modelo_seleccionado)\n\n\nCall:\nlm(formula = Y ~ X1 + X2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.47672 -0.67285  0.09839  0.70676  2.62566 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.9729     0.1059   28.08   &lt;2e-16 ***\nX1            1.9522     0.1155   16.91   &lt;2e-16 ***\nX2            4.0449     0.1090   37.11   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.048 on 97 degrees of freedom\nMultiple R-squared:  0.943, Adjusted R-squared:  0.9418 \nF-statistic: 802.3 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nRecordemos que los métodos de regularización, estudiados en el tema 2, no solo ajustan el modelo, sino que también penalizan la complejidad, lo que ayuda a eliminar variables irrelevantes.\n\n\n3.3.3 Escalado y Normalización de Variables\nMuchos algoritmos de aprendizaje automático, como la regresión, las redes neuronales y los métodos basados en distancia (k-NN, SVM), son sensibles a la escala de las variables. Por lo tanto, es fundamental escalar o normalizar los datos para garantizar que todas las variables contribuyan de manera equitativa al modelo.\nEstandarización (Z-Score Normalization)\nLa estandarización consiste en restar la media y dividir por la desviación estándar, lo que produce variables con media cero y desviación estándar uno.\n\\[\nX_{\\text{estandarizado}} = \\frac{X - \\bar{X}}{\\sigma_X}\n\\]\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Estandarización de una variable\nX1_estandarizado &lt;- scale(X1)\n\n\n\n\n\nNormalización Min-Max\nLa normalización Min-Max escala las variables a un rango específico, típicamente entre 0 y 1.\n\\[\nX_{\\text{normalizado}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\n\\]\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Normalización Min-Max\nX1_min_max &lt;- (X1 - min(X1)) / (max(X1) - min(X1))\n\n\n\n\n\n\n3.3.4 Técnicas avanzadas de Ingeniería de Características\nCuando se trabaja con grandes conjuntos de datos o con variables altamente correlacionadas, puede ser necesario aplicar técnicas más avanzadas para reducir la dimensionalidad y extraer características relevantes.\n\n3.3.4.1 Análisis de Componentes Principales (PCA)\nEl Análisis de Componentes Principales (PCA) es una técnica de reducción de dimensionalidad que transforma un conjunto de variables correlacionadas en un conjunto más pequeño de componentes principales no correlacionados que explican la mayor parte de la varianza en los datos.\n\n\n\n\n\n\nAviso\n\n\n\nLos detalles del Análisis de Componentes Principales son tratados en la asignatura de Aprendizaje Automático.\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Datos simulados\nset.seed(123)\ndatos &lt;- data.frame(X1, X2, X3)\n\n# Aplicar PCA\npca_resultado &lt;- prcomp(datos, scale. = TRUE)\n\n# Visualización de los resultados\nsummary(pca_resultado)\n\nImportance of components:\n                          PC1    PC2    PC3\nStandard deviation     1.0726 0.9900 0.9324\nProportion of Variance 0.3835 0.3267 0.2898\nCumulative Proportion  0.3835 0.7102 1.0000\n\nbiplot(pca_resultado)\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.4.2 Codificación de variables categóricas\nLas variables categóricas deben convertirse en variables numéricas antes de ser utilizadas en muchos modelos. Esto puede hacerse mediante:\nCodificación One-Hot\nEl One-Hot Encoding es una técnica utilizada en el preprocesamiento de datos para convertir variables categóricas en variables numéricas. Muchos algoritmos de aprendizaje automático y estadística (como la regresión lineal, redes neuronales y máquinas de soporte vectorial) requieren que las variables de entrada sean numéricas, ya que no pueden manejar directamente datos categóricos.\nEl One-Hot Encoding transforma cada categoría en una nueva columna binaria (0 o 1), donde el 1 indica la presencia de una categoría específica y el 0 su ausencia.\nSupongamos que tienes una variable categórica llamada Color con tres categorías: Rojo, Verde, y Azul.\n\n\n\nID\nColor\n\n\n\n\n1\nRojo\n\n\n2\nVerde\n\n\n3\nAzul\n\n\n4\nRojo\n\n\n5\nVerde\n\n\n\nCon One-Hot Encoding, creamos una nueva columna para cada categoría única:\n\n\n\nID\nColor_Rojo\nColor_Verde\nColor_Azul\n\n\n\n\n1\n1\n0\n0\n\n\n2\n0\n1\n0\n\n\n3\n0\n0\n1\n\n\n4\n1\n0\n0\n\n\n5\n0\n1\n0\n\n\n\nCada fila tiene un único 1 que indica la categoría correspondiente y ceros en las otras columnas. Esto convierte la información categórica en un formato que los algoritmos numéricos pueden procesar.\n\n\n\n\n\n\nPropiedades Clave\n\n\n\n\n\nVentajas del One-Hot Encoding\n\nCompatibilidad con algoritmos numéricos: La mayoría de los modelos de aprendizaje automático requieren variables numéricas. El One-Hot Encoding convierte las categorías en un formato adecuado.\nEvita suposiciones erróneas A diferencia de la codificación ordinal, que asigna valores numéricos secuenciales a categorías (por ejemplo, Rojo = 1, Verde = 2, Azul = 3), el One-Hot Encoding no introduce un orden artificial entre las categorías. Esto es importante cuando no hay una jerarquía natural.\nMejora la Interpretabilidad en Modelos Lineales: En modelos como la regresión lineal, cada columna creada mediante One-Hot Encoding representa el efecto específico de esa categoría.\n\nDesventajas del One-Hot Encoding\n\nIncremento de la Dimensionalidad: Si la variable categórica tiene muchas categorías únicas (por ejemplo, países o códigos postales), el número de columnas creadas puede ser muy grande. Esto puede conducir a problemas de “curse of dimensionality” (la maldición de la dimensionalidad), afectando el rendimiento del modelo y aumentando el tiempo de computación.\nColinealidad Perfecta (Dummy Variable Trap): Al crear una columna para cada categoría, una de las columnas se puede representar como una combinación lineal de las demás. Esto puede causar problemas en modelos lineales. Para evitarlo, se elimina una categoría de referencia (típicamente la primera), lo que se conoce como evitar la trampa de las variables ficticias (dummy variable trap).\n\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Variable categórica simulada\ncategoria &lt;- factor(c(\"bajo\", \"medio\", \"alto\", \"medio\", \"alto\"))\n\n# Codificación one-hot\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\ndummies &lt;- dummyVars(~ categoria, data = data.frame(categoria))\nprint(dummies)\n\nDummy Variable Object\n\nFormula: ~categoria\n1 variables, 1 factors\nVariables and levels will be separated by '.'\nA less than full rank encoding is used\n\ndatos_codificados &lt;- predict(dummies, newdata = data.frame(categoria))\n\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Instalar y cargar la librería caret\nlibrary(caret)\n\n# Crear un data frame\ndatos &lt;- data.frame(ID = 1:5, Color = c(\"Rojo\", \"Verde\", \"Azul\", \"Rojo\", \"Verde\"))\n\n# Aplicar One-Hot Encoding usando dummyVars\ndummy &lt;- dummyVars(~ Color, data = datos)\none_hot &lt;- predict(dummy, newdata = datos)\n\n# Ver los resultados\nprint(one_hot)\n\n  ColorAzul ColorRojo ColorVerde\n1         0         1          0\n2         0         0          1\n3         1         0          0\n4         0         1          0\n5         0         0          1\n\n\n\n\n\n\n\n\n\n\n\nAviso\n\n\n\nCuando aplicamos One-Hot Encoding, el conjunto de variables resultantes puede generar colinealidad perfecta, lo que puede ser problemático en modelos lineales. Para evitarlo, es común eliminar una de las columnas creadas (que actuará como categoría de referencia).\n\n# One-Hot Encoding con eliminación de una categoría (categoría de referencia)\none_hot_ref &lt;- model.matrix(~ Color, data = datos)[, -1]  # Eliminar la primera columna\n\nprint(one_hot_ref)\n\n  ColorRojo ColorVerde\n1         1          0\n2         0          1\n3         0          0\n4         1          0\n5         0          1\n\n\nEsto elimina una columna (por ejemplo, ColorAzul) y permite que las otras columnas se interpreten en relación a la categoría de referencia.\n\n\n\n\n3.3.4.3 Codificación Ordinal\nLa codificación ordinal es una técnica de preprocesamiento de datos utilizada para convertir variables categóricas en valores numéricos manteniendo el orden natural entre las categorías. A diferencia del One-Hot Encoding, que trata a todas las categorías como independientes y sin relación entre sí, la codificación ordinal es útil cuando las categorías tienen una jerarquía o un orden lógico.\nEn la codificación ordinal, a cada categoría se le asigna un número entero que refleja su posición o nivel en un orden determinado. Esto permite que los modelos estadísticos y de aprendizaje automático interpreten que algunas categorías son mayores o menores que otras, lo que es especialmente útil en variables que representan rangos, niveles o clasificaciones.\nSupongamos que tenemos una variable llamada “Nivel de Satisfacción” con las siguientes categorías:\n\n\n\nNivel de Satisfacción\n\n\n\n\nMuy Insatisfecho\n\n\nInsatisfecho\n\n\nNeutral\n\n\nSatisfecho\n\n\nMuy Satisfecho\n\n\n\nAquí, hay un orden lógico desde “Muy Insatisfecho” hasta “Muy Satisfecho”. Aplicando codificación ordinal, asignamos números que reflejen esta jerarquía:\n\n\n\nNivel de Satisfacción\nCodificación Ordinal\n\n\n\n\nMuy Insatisfecho\n1\n\n\nInsatisfecho\n2\n\n\nNeutral\n3\n\n\nSatisfecho\n4\n\n\nMuy Satisfecho\n5\n\n\n\n\n\n\n\n\n\n\nPropiedades clave\n\n\n\n\n\nLa codificación ordinal es adecuada cuando:\n\nLas categorías tienen un orden natural: Ejemplos incluyen niveles educativos (Primaria, Secundaria, Universidad), calificaciones (Bajo, Medio, Alto), o satisfacción del cliente (Insatisfecho, Neutral, Satisfecho).\nEl modelo puede interpretar la relación de orden: Algunos algoritmos, como la regresión lineal o las máquinas de vectores soporte (SVM), pueden beneficiarse de la codificación ordinal si el orden es relevante para la variable objetivo.\nReducción de dimensionalidad: A diferencia del One-Hot Encoding, que puede crear muchas columnas para variables con múltiples categorías, la codificación ordinal mantiene la variable en una sola columna, lo que es más eficiente para conjuntos de datos grandes.\n\n\n\n\n\n\n\n\n\n\nVentajas y desventajas\n\n\n\n\n\nVentajas\n\nPreserva la jerarquía de las categorías: Permite que el modelo entienda que ciertas categorías son mayores o menores que otras.\nReducción de la dimensionalidad: A diferencia del One-Hot Encoding, no aumenta el número de columnas, lo que reduce el riesgo de la maldición de la dimensionalidad.\nSimplicidad Computacional: Es más eficiente en términos de almacenamiento y tiempo de computación, especialmente para variables con muchas categorías.\n\nDesventajas\n\nRiesgo de interpretación incorrecta del orden: Si la variable categórica no tiene un orden lógico, la codificación ordinal puede inducir al modelo a asumir relaciones que no existen. Por ejemplo, supongamos que tienes una variable Color con categorías Rojo, Verde, y Azul. Asignarles valores como Rojo = 1, Verde = 2, Azul = 3 podría inducir al modelo a pensar que Verde es “mayor” que Rojo y Azul es “mayor” que Verde, lo cual no tiene sentido en este contexto.\nNo Captura la Magnitud de la Diferencia: Aunque las categorías están ordenadas, la codificación ordinal no refleja la magnitud real de las diferencias entre categorías. Por ejemplo, la diferencia entre “Insatisfecho” y “Neutral” puede no ser la misma que entre “Satisfecho” y “Muy Satisfecho”.\n\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Crear un data frame con una variable categórica ordinal\ndatos &lt;- data.frame(\n  ID = 1:5,\n  Satisfaccion = c(\"Muy Insatisfecho\", \"Insatisfecho\", \"Neutral\", \"Satisfecho\", \"Muy Satisfecho\")\n)\n\n# Convertir la variable en un factor ordenado\ndatos$Satisfaccion_ordinal &lt;- factor(datos$Satisfaccion, \n                                     levels = c(\"Muy Insatisfecho\", \"Insatisfecho\", \"Neutral\", \"Satisfecho\", \"Muy Satisfecho\"), \n                                     ordered = TRUE)\n\n# Ver la estructura del factor\nstr(datos)\n\n'data.frame':   5 obs. of  3 variables:\n $ ID                  : int  1 2 3 4 5\n $ Satisfaccion        : chr  \"Muy Insatisfecho\" \"Insatisfecho\" \"Neutral\" \"Satisfecho\" ...\n $ Satisfaccion_ordinal: Ord.factor w/ 5 levels \"Muy Insatisfecho\"&lt;..: 1 2 3 4 5\n\n# Asignar valores numéricos a las categorías ordenadas\ndatos$Satisfaccion_codificada &lt;- as.numeric(datos$Satisfaccion_ordinal)\n\n# Ver el resultado\nprint(datos)\n\n  ID     Satisfaccion Satisfaccion_ordinal Satisfaccion_codificada\n1  1 Muy Insatisfecho     Muy Insatisfecho                       1\n2  2     Insatisfecho         Insatisfecho                       2\n3  3          Neutral              Neutral                       3\n4  4       Satisfecho           Satisfecho                       4\n5  5   Muy Satisfecho       Muy Satisfecho                       5\n\n\n\n\n\nPodemos usar la variable codificada ordinalmente en un modelo de regresión para evaluar su impacto en una variable dependiente, como una puntuación de satisfacción general.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Simular una variable de respuesta (puntuación general)\nset.seed(123)\ndatos$Puntuacion &lt;- c(2, 4, 6, 8, 10) + rnorm(5, mean = 0, sd = 0.5)\n\n# Ajustar un modelo de regresión lineal usando la variable codificada\nmodelo_ordinal &lt;- lm(Puntuacion ~ Satisfaccion_codificada, data = datos)\n\n# Resumen del modelo\nsummary(modelo_ordinal)\n\n\nCall:\nlm(formula = Puntuacion ~ Satisfaccion_codificada, data = datos)\n\nResiduals:\n      1       2       3       4       5 \n-0.2090 -0.1279  0.6826 -0.1455 -0.2002 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              -0.1552     0.4640  -0.335 0.759968    \nSatisfaccion_codificada   2.0840     0.1399  14.896 0.000657 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4424 on 3 degrees of freedom\nMultiple R-squared:  0.9867,    Adjusted R-squared:  0.9822 \nF-statistic: 221.9 on 1 and 3 DF,  p-value: 0.0006565\n\n\n\n\n\nEl modelo ajustará la relación entre la puntuación de satisfacción y el nivel de satisfacción codificado ordinalmente. El coeficiente de la variable Satisfaccion_codificada indicará cómo cambia la puntuación a medida que aumenta el nivel de satisfacción.\n\n\n3.3.4.4 Diferencias entre Codificación Ordinal y One-Hot Encoding\n\n\n\n\n\n\n\n\nCaracterística\nCodificación Ordinal\nOne-Hot Encoding\n\n\n\n\nPreserva el orden\nSí, refleja la jerarquía entre categorías.\nNo, trata cada categoría como independiente.\n\n\nAumenta la dimensionalidad\nNo, mantiene la variable en una sola columna.\nSí, crea una columna para cada categoría única.\n\n\nAdecuado para\nVariables con orden natural (ej. educación).\nVariables sin orden (ej. color, género, ciudad).\n\n\nRiesgo\nPuede inducir al modelo a asumir relaciones falsas si no hay orden real.\nIncremento de la complejidad del modelo.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos no lineales. Transformación de variables. Ingeniería de características.</span>"
    ]
  },
  {
    "objectID": "tema4.html",
    "href": "tema4.html",
    "title": "4  Modelos de regresión generalizada",
    "section": "",
    "text": "4.1 Introducción a los GLM",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelos de regresión generalizada</span>"
    ]
  },
  {
    "objectID": "tema4.html#introducción-a-los-glm",
    "href": "tema4.html#introducción-a-los-glm",
    "title": "4  Modelos de regresión generalizada",
    "section": "",
    "text": "4.1.1 ¿Qué son los Modelos Lineales Generalizados?\nLos Modelos Lineales Generalizados (GLM) son una extensión de los modelos de regresión lineal que permiten manejar una mayor variedad de tipos de datos y relaciones entre variables (Nelder y Wedderburn 1972). Mientras que la regresión lineal tradicional asume que la variable dependiente es continua y sigue una distribución normal, los GLM permiten trabajar con variables dependientes que:\n\nSon binarias (como éxito/fracaso o sí/no).\nRepresentan conteos de eventos (número de llamadas, accidentes, etc.).\nSon continuas positivas y no siguen una distribución normal (como tiempos o costos).\n\nLos GLM proporcionan una estructura flexible para modelar la relación entre una o más variables independientes y una variable dependiente que sigue alguna distribución de la familia exponencial (binomial, Poisson, gamma, entre otras).\n\n\n4.1.2 Componentes de un Modelo Lineal Generalizado\nUn GLM se define por tres componentes clave:\n\nComponente Aleatorio:\nEste componente describe la distribución de la variable dependiente. En la regresión lineal, la variable dependiente sigue una distribución normal. En los GLM, puede seguir otras distribuciones de la familia exponencial, como:\n\nDistribución Binomial: Para variables categóricas binarias (0/1, éxito/fracaso).\nDistribución de Poisson: Para datos de conteo (número de eventos).\nDistribución Gamma: Para variables continuas y positivas (como costos o tiempos).\n\nComponente Sistemático:\nEste componente describe cómo las variables independientes se combinan linealmente en el modelo. Se define como:\n\\[\n\\eta = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p\n\\]\nDonde \\(\\eta\\) es el predictor lineal y \\(\\beta\\) representa los coeficientes del modelo.\nFunción de Enlace:\nLa función de enlace conecta el componente sistemático con la media de la variable dependiente. Mientras que en la regresión lineal la relación es directa ($Y = $), en los GLM se utiliza una función de enlace \\(g(\\mu)\\) para transformar la media \\(\\mu\\) y ajustar diferentes tipos de datos.\n\\[\ng(\\mu) = \\eta\n\\]\n\nEjemplos de funciones de enlace:\n\nLogística (Logit): Para la regresión logística, que modela la probabilidad de un evento. \\[\ng(\\mu) = \\log\\left(\\frac{\\mu}{1 - \\mu}\\right)\n\\]\nLogarítmica: Para la regresión de Poisson, que modela tasas de eventos. \\[\ng(\\mu) = \\log(\\mu)\n\\]\nIdentidad: Para la regresión lineal estándar. \\[\ng(\\mu) = \\mu\n\\]\n\n\n\n\n\n\n\nAplicaciones\n\n\n\n\n\nLos GLM se utilizan en una amplia variedad de disciplinas para resolver problemas del mundo real:\nRegresión Logística (para variables binarias):\n\nMedicina: Predicción de la presencia o ausencia de una enfermedad basada en factores de riesgo.\nMarketing: Determinación de la probabilidad de que un cliente compre un producto.\nFinanzas: Evaluación de la probabilidad de incumplimiento de pago de un préstamo.\n\nRegresión de Poisson (para datos de conteo):\n\nTransporte: Modelado del número de accidentes en una carretera en un período de tiempo.\nEcología: Conteo de especies en un área determinada.\nTelecomunicaciones: Número de llamadas recibidas por un centro de atención.\n\nRegresión Binomial Negativa (para conteos con sobredispersión):\n\nSalud Pública: Modelado del número de visitas al médico o incidentes de una enfermedad en una población.\n\nModelos Gamma (para variables continuas positivas):\n\nSeguros: Estimación de los costos de reclamos de seguros.\nIngeniería: Modelado de tiempos de falla en procesos industriales.\n\n\n\n\n\n\n4.1.3 Diferencias clave entre la Regresión Lineal y los GLM\n\n\n\n\n\n\n\n\nCaracterística\nRegresión Lineal\nModelos Lineales Generalizados (GLM)\n\n\n\n\nDistribución de la variable dependiente\nNormal\nFamilia exponencial (binomial, Poisson, gamma, etc.)\n\n\nTipo de variable dependiente\nContinua\nBinaria, de conteo, continua positiva\n\n\nRelación entre las variables\nLineal directa\nRelación transformada mediante una función de enlace\n\n\nFunción de Enlace\nIdentidad (\\(g(\\mu) = \\mu\\))\nLogit, logarítmica, inversa, etc.\n\n\n\n\nLas ventajas principales de los GLM son:\n\nFlexibilidad: Los GLM permiten modelar diferentes tipos de variables dependientes, lo que amplía significativamente el rango de problemas que se pueden abordar.\nInterpretación Coherente: Aunque se utilizan funciones de enlace, los coeficientes de los GLM pueden interpretarse de manera similar a los modelos lineales, proporcionando información sobre el impacto de cada variable independiente.\nEvaluación Estadística Robusta: Los GLM permiten la realización de pruebas de hipótesis, la construcción de intervalos de confianza y la evaluación de la bondad del ajuste mediante medidas como el AIC y el BIC.\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Cargar librería y datos\nlibrary(MASS)\ndata(Pima.tr)  # Datos sobre diabetes en mujeres de origen pima\n\n# Ajustar un modelo de regresión logística\nmodelo_logistico &lt;- glm(type ~ npreg + glu + bmi, data = Pima.tr, family = binomial)\n\n# Resumen del modelo\nsummary(modelo_logistico)\n\n\nCall:\nglm(formula = type ~ npreg + glu + bmi, family = binomial, data = Pima.tr)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -8.718723   1.411080  -6.179 6.46e-10 ***\nnpreg        0.149213   0.051833   2.879  0.00399 ** \nglu          0.033879   0.006327   5.355 8.55e-08 ***\nbmi          0.094817   0.032405   2.926  0.00343 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 256.41  on 199  degrees of freedom\nResidual deviance: 189.89  on 196  degrees of freedom\nAIC: 197.89\n\nNumber of Fisher Scoring iterations: 5\n\n# Predicciones de la probabilidad de tener diabetes\npredicciones &lt;- predict(modelo_logistico, type = \"response\")\n\n# Ver primeras predicciones\nhead(predicciones)\n\n         1          2          3          4          5          6 \n0.10014804 0.78786795 0.12244031 0.80425012 0.06975347 0.21233644 \n\n\n\n\n\n\nLos Modelos Lineales Generalizados amplían el alcance de la regresión lineal clásica, proporcionando herramientas para modelar una amplia variedad de tipos de datos, desde variables binarias hasta datos de conteo y variables continuas no normales. A través del uso de funciones de enlace y distribuciones flexibles, los GLM permiten resolver problemas complejos del mundo real en campos tan diversos como la medicina, el marketing, la ingeniería y las ciencias sociales.\nEn las próximas secciones, exploraremos en detalle cómo aplicar estos modelos específicos, como la regresión logística y la regresión de Poisson, y cómo interpretar sus resultados en diferentes contextos.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelos de regresión generalizada</span>"
    ]
  },
  {
    "objectID": "tema4.html#regresión-logística",
    "href": "tema4.html#regresión-logística",
    "title": "4  Modelos de regresión generalizada",
    "section": "4.2 Regresión Logística",
    "text": "4.2 Regresión Logística\nLa regresión logística es una herramienta fundamental para modelar la probabilidad de eventos binarios en una variedad de contextos, desde la medicina hasta la economía y el marketing (Hosmer Jr, Lemeshow, y Sturdivant 2013). La correcta interpretación de los coeficientes mediante odds ratios, así como la evaluación del ajuste del modelo mediante curvas ROC y matrices de confusión, son esenciales para extraer conclusiones válidas de los datos.\n\n4.2.1 Fundamentos de la Regresión Logística\nLa regresión logística es una técnica estadística utilizada para modelar la probabilidad de ocurrencia de un evento binario, es decir, cuando la variable dependiente toma solo dos posibles valores (por ejemplo, éxito/fracaso, sí/no, enfermo/sano). A diferencia de la regresión lineal, que modela una relación lineal entre variables, la regresión logística utiliza una función logística para asegurar que las predicciones estén en el rango [0,1], lo cual es necesario para interpretar los resultados como probabilidades.\nLa función Logística (Sigmoide)\nLa función logística transforma cualquier valor real en un valor comprendido entre 0 y 1. La forma matemática de la función logística es:\n\\[\nP(Y = 1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p)}}\n\\]\nDonde:\n\n\\(P(Y = 1 | X)\\) es la probabilidad de que el evento ocurra.\n\\(\\beta_0\\) es el intercepto y \\(\\beta_1, \\beta_2, \\dots, \\beta_p\\) son los coeficientes asociados a las variables independientes \\(X_1, X_2, \\dots, X_p\\).\n\nLa curva sigmoide que representa esta función tiene forma de “S”, lo que refleja que para valores muy pequeños o muy grandes del predictor, la probabilidad se aplana hacia 0 o 1, respectivamente.\nFunción de Enlace Logit\nEn la regresión logística, la relación entre el predictor lineal y la probabilidad se establece mediante la función de enlace logit. El logit de una probabilidad \\(p\\) se define como:\n\\[\n\\text{logit}(p) = \\log\\left(\\frac{p}{1 - p}\\right)\n\\]\nEsta transformación convierte una probabilidad en una escala que va de \\(-\\infty\\) a \\(+\\infty\\), lo que permite ajustar un modelo lineal a los datos. El modelo logístico puede expresarse como:\n\\[\n\\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p\n\\]\n\n\n4.2.2 Interpretación de coeficientes y Odds Ratios\nUno de los aspectos más importantes de la regresión logística es la interpretación de los coeficientes. Dado que los coeficientes están en la escala del logit, su interpretación directa no es tan intuitiva como en la regresión lineal. Sin embargo, podemos interpretarlos utilizando odds y odds ratios.\nEl odds o razón de probabilidades de que ocurra un evento es el cociente entre la probabilidad de que ocurra el evento y la probabilidad de que no ocurra:\n\\[\n\\text{odds} = \\frac{p}{1 - p}\n\\]\nPor ejemplo, si la probabilidad de éxito es 0.8, el odds sería:\n\\[\n\\text{odds} = \\frac{0.8}{1 - 0.8} = 4\n\\]\nEsto significa que el evento es 4 veces más probable que no ocurra.\nEl odds ratio (OR) mide el cambio en los odds cuando una variable independiente aumenta en una unidad. Se calcula como el exponencial del coeficiente de la regresión logística:\n\\[\n\\text{OR} = e^{\\beta}\n\\]\nInterpretación de OR:\n\nSi OR &gt; 1, el evento es más probable a medida que aumenta la variable independiente.\nSi OR &lt; 1, el evento es menos probable a medida que aumenta la variable independiente.\nSi OR = 1, no hay efecto.\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nSupongamos que ajustamos un modelo de regresión logística para predecir la probabilidad de tener diabetes en función del índice de masa corporal (BMI). El coeficiente asociado a BMI es 0.08.\n\\[\n\\text{OR} = e^{0.08} \\approx 1.083\n\\]\nEsto significa que por cada incremento de 1 unidad en el BMI, la odds de tener diabetes aumentan en un 8.3%.\n\n\n\n\n\n4.2.3 Evaluación del modelo Logístico\nA diferencia de la regresión lineal, donde se usa el coeficiente de determinación (\\(R^2\\)) para evaluar el ajuste, en la regresión logística se utilizan otros métodos para medir la calidad del modelo.\nMatriz de Confusión\nLa matriz de confusión compara las predicciones del modelo con los valores reales, clasificando las observaciones en:\n\nVerdaderos Positivos (VP): Predijo positivo y es positivo.\nFalsos Positivos (FP): Predijo positivo pero es negativo.\nVerdaderos Negativos (VN): Predijo negativo y es negativo.\nFalsos Negativos (FN): Predijo negativo pero es positivo.\n\nA partir de esta matriz, se pueden calcular métricas importantes como:\n\nPrecisión (Accuracy): \\(\\frac{VP + VN}{\\text{Total}}\\)\nSensibilidad (Recall o Tasa de Verdaderos Positivos): \\(\\frac{VP}{VP + FN}\\)\nEspecificidad (Tasa de Verdaderos Negativos): \\(\\frac{VN}{VN + FP}\\)\n\n\n\n\n\n\n\nAviso\n\n\n\nLos detalles de la evaluación de un modelo empleando la Matríz de Confusión son ampliamente tratados en la asignatura de Aprendizaje Automático.\n\n\nCurva ROC y AUC\nLa Curva ROC (Receiver Operating Characteristic) muestra la relación entre la tasa de verdaderos positivos y la tasa de falsos positivos a diferentes umbrales de clasificación.\nEl AUC (Área Bajo la Curva ROC) mide la capacidad del modelo para discriminar entre las clases. Un AUC de \\(0.5\\) indica que el modelo no tiene capacidad predictiva, mientras que un AUC de \\(1.0\\) indica un modelo perfecto.\nPseudo R² (Nagelkerke, McFadden)\nAunque el \\(R^2\\) tradicional no se aplica directamente a la regresión logística, existen medidas como el pseudo \\(R^2\\) que proporcionan una idea de la bondad del ajuste del modelo.\n\nMcFadden’s \\(R^2\\):\n\\[\nR^2_{\\text{McFadden}} = 1 - \\frac{\\log L_{\\text{modelo}}}{\\log L_{\\text{modelo nulo}}}\n\\]\nDonde \\(\\log L_{\\text{modelo}}\\) es el log-likelihood del modelo ajustado y \\(\\log L_{\\text{modelo nulo}}\\) es el log-likelihood de un modelo sin predictores.\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nVamos a aplicar la regresión logística en R utilizando el conjunto de datos Pima.tr del paquete MASS, que contiene información sobre mujeres pima y si tienen o no diabetes.\n\n# Cargar la librería y el conjunto de datos\nlibrary(MASS)\ndata(Pima.tr)\n\n# Ajustar el modelo de regresión logística\nmodelo_logistico &lt;- glm(type ~ npreg + glu + bmi, data = Pima.tr, family = binomial)\n\n# Resumen del modelo\nsummary(modelo_logistico)\n\n\nCall:\nglm(formula = type ~ npreg + glu + bmi, family = binomial, data = Pima.tr)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -8.718723   1.411080  -6.179 6.46e-10 ***\nnpreg        0.149213   0.051833   2.879  0.00399 ** \nglu          0.033879   0.006327   5.355 8.55e-08 ***\nbmi          0.094817   0.032405   2.926  0.00343 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 256.41  on 199  degrees of freedom\nResidual deviance: 189.89  on 196  degrees of freedom\nAIC: 197.89\n\nNumber of Fisher Scoring iterations: 5\n\n# Predicciones de probabilidad\npredicciones_prob &lt;- predict(modelo_logistico, type = \"response\")\n\n# Clasificación con un umbral de 0.5\npredicciones_clase &lt;- ifelse(predicciones_prob &gt; 0.5, \"Yes\", \"No\")\n\n# Crear matriz de confusión\ntabla_confusion &lt;- table(Predicted = predicciones_clase, Actual = Pima.tr$type)\nprint(tabla_confusion)\n\n         Actual\nPredicted  No Yes\n      No  114  29\n      Yes  18  39\n\n# Calcular precisión\naccuracy &lt;- sum(diag(tabla_confusion)) / sum(tabla_confusion)\nprint(paste(\"Precisión:\", round(accuracy, 3)))\n\n[1] \"Precisión: 0.765\"\n\n# Cargar librería para curvas ROC\nlibrary(pROC)\n\nType 'citation(\"pROC\")' for a citation.\n\n\n\nAttaching package: 'pROC'\n\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\n# Curva ROC\nroc_obj &lt;- roc(Pima.tr$type, predicciones_prob)\n\nSetting levels: control = No, case = Yes\n\n\nSetting direction: controls &lt; cases\n\nplot(roc_obj, main = \"Curva ROC para Regresión Logística\")\n\n\n\n\n\n\n\n# Calcular AUC\nauc_valor &lt;- auc(roc_obj)\nprint(paste(\"AUC:\", round(auc_valor, 3)))\n\n[1] \"AUC: 0.831\"",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelos de regresión generalizada</span>"
    ]
  },
  {
    "objectID": "tema4.html#regresión-de-poisson",
    "href": "tema4.html#regresión-de-poisson",
    "title": "4  Modelos de regresión generalizada",
    "section": "4.3 Regresión de Poisson",
    "text": "4.3 Regresión de Poisson\nLa regresión de Poisson es una técnica estadística utilizada para modelar datos de conteo, es decir, situaciones en las que la variable dependiente representa el número de veces que ocurre un evento en un período de tiempo o espacio específico (Coxe, West, y Aiken 2009). Este tipo de modelo es adecuado cuando la variable dependiente toma valores enteros no negativos (\\(0, 1, 2, \\dots\\)) y sigue una distribución de Poisson.\nLa distribución de Poisson describe la probabilidad de que ocurra un número determinado de eventos en un intervalo fijo, dado que estos eventos ocurren de forma independiente y a una tasa constante.\nLa función de probabilidad de la distribución de Poisson es:\n\\[\nP(Y = y) = \\frac{e^{-\\lambda} \\lambda^y}{y!}\n\\]\nDonde:\n\n\\(Y\\) es la variable aleatoria que representa el número de eventos.\n\\(\\lambda\\) es la tasa media de ocurrencia de los eventos (esperanza de \\(Y\\)).\n\\(y\\) es el número de eventos observados (\\(y = 0, 1, 2, \\dots\\)).\n\n\n4.3.1 Modelo de regresión de Poisson\nEn la regresión de Poisson, el objetivo es modelar la relación entre la tasa de ocurrencia de los eventos (\\(\\lambda\\)) y un conjunto de variables predictoras \\(X_1, X_2, \\dots, X_p\\).\nLa forma funcional del modelo de Poisson es:\n\\[\n\\log(\\lambda) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p\n\\]\nDonde:\n\n\\(\\log(\\lambda)\\) es la función de enlace logarítmica que asegura que la tasa \\(\\lambda\\) sea siempre positiva.\n\\(\\beta_0, \\beta_1, \\dots, \\beta_p\\) son los coeficientes del modelo que describen la influencia de cada predictor sobre la tasa de eventos.\n\nEl modelo puede expresarse en términos de la tasa esperada de eventos como:\n\\[\n\\lambda = e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p}\n\\]\n\n\n4.3.2 Supuestos y limitaciones de la regresión de Poisson\nTal y como ocurre en el modelo de regresión lineal, para que la regresión de Poisson sea adecuada, se deben cumplir ciertos supuestos:\n\nIndependencia de los eventos: Los eventos deben ocurrir de manera independiente unos de otros.\nDistribución de Poisson de la variable dependiente: La variable de respuesta debe seguir una distribución de Poisson, donde la media y la varianza son iguales:\n\n\\[\n   E(Y) = Var(Y) = \\lambda\n   \\]\n\nNo sobredispersión: Uno de los problemas comunes en los datos de conteo es la sobredispersión, que ocurre cuando la varianza de los datos es mayor que la media (\\(Var(Y) &gt; E(Y)\\)). La presencia de sobredispersión indica que el modelo de Poisson puede no ser adecuado, y puede ser necesario considerar modelos alternativos como la regresión binomial negativa.\nNo exceso de ceros: Si hay demasiados ceros en los datos (por ejemplo, en el número de accidentes en diferentes localidades donde muchas tienen cero accidentes), puede ser necesario utilizar modelos de Poisson inflados en ceros (ZIP) (Lambert 1992).\n\n\n\n4.3.3 Interpretación de los resultados\nLa interpretación de los coeficientes en la regresión de Poisson difiere de la regresión lineal debido al uso de la función de enlace logarítmica.\nLos coeficientes \\(\\beta\\) representan el logaritmo de la tasa de eventos asociados con un cambio en la variable independiente. Para interpretar en términos de la tasa de ocurrencia, se utiliza el exponencial de los coeficientes:\n\\[\n  e^{\\beta_i}\n\\]\nEsto representa el factor de cambio multiplicativo en la tasa de eventos por cada unidad adicional en la variable \\(X_i\\).\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nSi \\(\\beta_1 = 0.5\\), entonces \\(e^{0.5} \\approx 1.65\\). Esto significa que por cada unidad adicional en \\(X_1\\), la tasa de ocurrencia de eventos aumenta en un 65%.\nSi \\(\\beta_1 = -0.3\\), entonces \\(e^{-0.3} \\approx 0.74\\). Esto indica que por cada unidad adicional en \\(X_1\\), la tasa de eventos disminuye en un 26%.\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nVamos a utilizar R para ajustar un modelo de regresión de Poisson. Supongamos que tenemos datos sobre el número de accidentes de tráfico en diferentes intersecciones de una ciudad, junto con variables como el volumen de tráfico y la visibilidad.\n\n# Simulación de datos para el número de accidentes\nset.seed(123)\nn &lt;- 100  # Número de observaciones\n\n# Variables predictoras\ntrafico &lt;- rnorm(n, mean = 1000, sd = 300)  # Volumen de tráfico en vehículos por día\nvisibilidad &lt;- rnorm(n, mean = 5, sd = 2)   # Visibilidad en kilómetros\n\n# Generar la tasa de accidentes (lambda) usando un modelo logarítmico\nlambda &lt;- exp(0.01 * trafico - 0.2 * visibilidad)\n\n# Generar el número de accidentes como una variable de Poisson\naccidentes &lt;- rpois(n, lambda = lambda)\n\n# Crear el data frame\ndatos_accidentes &lt;- data.frame(accidentes, trafico, visibilidad)\nhead(datos_accidentes)\n\n  accidentes   trafico visibilidad\n1       2102  831.8573    3.579187\n2       3744  930.9468    5.513767\n3     959848 1467.6125    4.506616\n4      11356 1021.1525    4.304915\n5      17411 1038.7863    3.096763\n6    1415794 1514.5195    4.909945\n\n# Ajustar el modelo de regresión de Poisson\nmodelo_poisson &lt;- glm(accidentes ~ trafico + visibilidad, data = datos_accidentes, family = poisson)\n\n# Resumen del modelo\nsummary(modelo_poisson)\n\n\nCall:\nglm(formula = accidentes ~ trafico + visibilidad, family = poisson, \n    data = datos_accidentes)\n\nCoefficients:\n              Estimate Std. Error   z value Pr(&gt;|z|)    \n(Intercept) -1.610e-03  2.116e-03    -0.761    0.447    \ntrafico      1.000e-02  1.323e-06  7557.681   &lt;2e-16 ***\nvisibilidad -2.001e-01  1.025e-04 -1951.765   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1.2621e+08  on 99  degrees of freedom\nResidual deviance: 9.5511e+01  on 97  degrees of freedom\nAIC: 1216.4\n\nNumber of Fisher Scoring iterations: 3\n\n\nEl coeficiente asociado a trafico indica cómo el volumen de tráfico afecta la tasa de accidentes.\nEl coeficiente asociado a visibilidad muestra cómo la visibilidad afecta la frecuencia de accidentes.\n\n# Exponenciar los coeficientes para interpretar en términos de tasas\nexp(coef(modelo_poisson))\n\n(Intercept)     trafico visibilidad \n  0.9983910   1.0100516   0.8186814 \n\n\nUn coeficiente positivo implica que un aumento en la variable está asociado con un aumento en la tasa de accidentes.\nUn coeficiente negativo implica que un aumento en la variable está asociado con una disminución en la tasa de accidentes.\n\n\n\n\n\n4.3.4 Evaluación del modelo de Poisson\nLa sobredispersión ocurre cuando la varianza de los datos es mayor que la media, lo que puede invalidar los supuestos de la regresión de Poisson.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Calcular la relación entre el deviance y los grados de libertad\ndeviance &lt;- modelo_poisson$deviance\ngrados_libertad &lt;- modelo_poisson$df.residual\nsobredispersion &lt;- deviance / grados_libertad\n\nprint(paste(\"Índice de Sobredispersión:\", round(sobredispersion, 2)))\n\n[1] \"Índice de Sobredispersión: 0.98\"\n\n\n\n\n\nUn valor del índice de sobredispersión cercano a \\(1\\) sugiere que no hay sobredispersión. Por contra, un valor significativamente mayor que \\(1\\) sugiere la presencia de sobredispersión, y puede ser necesario considerar una regresión binomial negativa.\nDiagnóstico de Residuos:\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Gráfico de residuos deviance para evaluar el ajuste\nplot(residuals(modelo_poisson, type = \"deviance\"), main = \"Residuos Deviance\", ylab = \"Residuos\", xlab = \"Índice\")\nabline(h = 0, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEjemplo sobre datos reales\n\n\n\n\n\nUn conjunto de datos clásico en R es warpbreaks, que contiene el número de roturas de hilo en diferentes condiciones de tensión y longitud del hilo.\n\n# Datos de ejemplo: número de roturas de hilo\ndata(warpbreaks)\n\n# Ajustar un modelo de Poisson para el número de roturas en función de la tensión\nmodelo_poisson_real &lt;- glm(breaks ~ wool + tension, data = warpbreaks, family = poisson)\n\n# Resumen del modelo\nsummary(modelo_poisson_real)\n\n\nCall:\nglm(formula = breaks ~ wool + tension, family = poisson, data = warpbreaks)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.69196    0.04541  81.302  &lt; 2e-16 ***\nwoolB       -0.20599    0.05157  -3.994 6.49e-05 ***\ntensionM    -0.32132    0.06027  -5.332 9.73e-08 ***\ntensionH    -0.51849    0.06396  -8.107 5.21e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 297.37  on 53  degrees of freedom\nResidual deviance: 210.39  on 50  degrees of freedom\nAIC: 493.06\n\nNumber of Fisher Scoring iterations: 4\n\n# Interpretación de coeficientes\nexp(coef(modelo_poisson_real))\n\n(Intercept)       woolB    tensionM    tensionH \n 40.1235380   0.8138425   0.7251908   0.5954198 \n\n\n\n\n\n\n\n4.3.5 Limitaciones y alternativas\nSi la varianza de los datos es mayor que la media, el modelo de Poisson no será adecuado. En este caso, se recomienda utilizar la regresión binomial negativa, que introduce un parámetro adicional para manejar la sobredispersión.\nSi hay más ceros de los esperados (por ejemplo, muchas intersecciones con cero accidentes), puede ser necesario utilizar modelos de Poisson inflados en ceros (ZIP) o binomial negativa inflada en ceros (ZINB).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelos de regresión generalizada</span>"
    ]
  },
  {
    "objectID": "tema4.html#otros-glms",
    "href": "tema4.html#otros-glms",
    "title": "4  Modelos de regresión generalizada",
    "section": "4.4 Otros GLMs",
    "text": "4.4 Otros GLMs\nLa regresión binomial negativa y los modelos basados en distribuciones como Gamma e Inversa Gaussiana amplían la capacidad de los Modelos Lineales Generalizados (GLM) para adaptarse a una amplia variedad de situaciones del mundo real. Estos modelos son especialmente útiles cuando los datos presentan características como sobredispersión, sesgo o restricciones en el dominio (por ejemplo, solo valores positivos). La elección adecuada del modelo y la función de enlace garantiza predicciones precisas y válidas, contribuyendo a la toma de decisiones informadas en campos como la salud, la ingeniería y la economía.\n\n4.4.1 Regresión Binomial Negativa\nTal y como hemos visto en apartados anteriores, la sobredispersión ocurre cuando la varianza de los datos de conteo es mayor que la media, lo cual viola uno de los supuestos clave de la regresión de Poisson, que asume que la media y la varianza son iguales (\\(E(Y) = Var(Y)\\)). La sobredispersión puede surgir por varias razones:\n\nHeterogeneidad no modelada: Existen factores que afectan la variable dependiente pero no han sido incluidos en el modelo.\nDependencia entre eventos: Los eventos no ocurren de forma independiente.\nExceso de ceros: Hay más ceros en los datos de los que predice la distribución de Poisson.\n\nCuando la sobredispersión está presente, la regresión de Poisson subestima los errores estándar, lo que puede llevar a conclusiones incorrectas sobre la significancia de los predictores.\nLa regresión binomial negativa es una extensión de la regresión de Poisson que introduce un parámetro adicional para manejar la sobredispersión. Este modelo permite que la varianza sea mayor que la media:\n\\[\nVar(Y) = \\lambda + \\alpha \\lambda^2\n\\]\nDonde \\(\\alpha\\) es el parámetro de dispersión. Si \\(\\alpha = 0\\), el modelo se reduce a la regresión de Poisson.\nLa forma funcional del modelo binomial negativa es similar al de Poisson:\n\\[\n\\log(\\lambda) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\n\\]\nPero la varianza ahora incluye el término adicional \\(\\alpha\\) para capturar la sobredispersión.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Instalar y cargar la librería MASS que contiene la función glm.nb\nlibrary(MASS)\n\n# Ajuste de un modelo binomial negativo con los datos simulados de accidentes\nmodelo_binom_neg &lt;- glm.nb(accidentes ~ trafico + visibilidad, data = datos_accidentes)\n\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\n\n\nWarning in glm.nb(accidentes ~ trafico + visibilidad, data = datos_accidentes):\nalternation limit reached\n\n# Resumen del modelo\nsummary(modelo_binom_neg)\n\n\nCall:\nglm.nb(formula = accidentes ~ trafico + visibilidad, data = datos_accidentes, \n    init.theta = 245451587.1, link = log)\n\nCoefficients:\n              Estimate Std. Error   z value Pr(&gt;|z|)    \n(Intercept) -1.620e-03  2.122e-03    -0.763    0.445    \ntrafico      1.000e-02  1.329e-06  7523.489   &lt;2e-16 ***\nvisibilidad -2.001e-01  1.036e-04 -1931.078   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(245394240) family taken to be 1)\n\n    Null deviance: 1.2568e+08  on 99  degrees of freedom\nResidual deviance: 9.5469e+01  on 97  degrees of freedom\nAIC: 1218.5\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  245451587 \n          Std. Err.:  488350955 \nWarning while fitting theta: alternation limit reached \n\n 2 x log-likelihood:  -1210.497 \n\n# Comparar la dispersión con el modelo de Poisson\ncat(\"Dispersión en Poisson:\", modelo_poisson$deviance / modelo_poisson$df.residual, \"\\n\")\n\nDispersión en Poisson: 0.9846515 \n\ncat(\"Dispersión en Binomial Negativa:\", modelo_binom_neg$theta, \"\\n\")\n\nDispersión en Binomial Negativa: 245451587 \n\n\n\n\n\n\nEl parámetro \\(\\theta\\) (dispersión) ajustado en el modelo binomial negativa ayuda a corregir la varianza subestimada en el modelo de Poisson.\nSi \\(\\theta\\) es significativamente mayor que 1, se confirma la presencia de sobredispersión.\n\n\n\n\n4.4.2 Modelos para variables continuas No Normales\nExisten situaciones en las que la variable dependiente es continua, pero no sigue una distribución normal. En estos casos, los Modelos Lineales Generalizados (GLM) permiten utilizar distribuciones alternativas como Gamma o Inversa Gaussiana, junto con funciones de enlace específicas.\n\n4.4.2.1 Regresión Gamma para datos positivos y sesgados\nLa regresión Gamma es adecuada para modelar variables continuas que son positivas y tienen una distribución sesgada a la derecha. Ejemplos típicos incluyen tiempos de espera, costos médicos o duración de procesos.\n\nLa distribución Gamma asume que la variable dependiente es continua y positiva.\nLa varianza de la variable dependiente aumenta proporcionalmente al cuadrado de la media.\n\nFunción de Enlace Común: \\[\n\\log(\\mu) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\n\\]\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Simulación de costos médicos\nset.seed(123)\nn &lt;- 100\ningresos &lt;- rnorm(n, mean = 50000, sd = 10000)\nedad &lt;- rnorm(n, mean = 45, sd = 10)\ncostos &lt;- rgamma(n, shape = 2, rate = 0.00005 * ingresos + 0.01 * edad)\n\n# Ajuste del modelo Gamma\nmodelo_gamma &lt;- glm(costos ~ ingresos + edad, family = Gamma(link = \"log\"))\n\n# Resumen del modelo\nsummary(modelo_gamma)\n\n\nCall:\nglm(formula = costos ~ ingresos + edad, family = Gamma(link = \"log\"))\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  3.440e-01  5.294e-01   0.650   0.5173  \ningresos    -1.807e-05  7.804e-06  -2.316   0.0227 *\nedad         3.584e-03  7.366e-03   0.487   0.6277  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.5010938)\n\n    Null deviance: 60.771  on 99  degrees of freedom\nResidual deviance: 58.345  on 97  degrees of freedom\nAIC: 105.47\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nLos coeficientes muestran cómo los ingresos y la edad afectan los costos médicos esperados.\nEl enlace logarítmico asegura que las predicciones sean siempre positivas.\n\n\n\n\n\n\n4.4.2.2 Regresión Inversa Gaussiana\nLa regresión Inversa Gaussiana es útil para modelar tiempos de respuesta o variables donde la varianza disminuye rápidamente a medida que la media aumenta. Este modelo se aplica en campos como la ingeniería, donde se analizan tiempos hasta fallas de sistemas.\nFunción de Enlace Común: \\[\n\\frac{1}{\\mu^2} = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\n\\]\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Instalar y cargar la librería correcta\nlibrary(statmod)\n\n# Simulación de datos\nset.seed(123)\nn &lt;- 100\ncarga_trabajo &lt;- rnorm(n, mean = 50, sd = 10)\n\n# Generar tiempos hasta el fallo usando la distribución inversa gaussiana\n# Aseguramos que los valores de carga_trabajo sean positivos para evitar problemas numéricos\ncarga_trabajo[carga_trabajo &lt;= 0] &lt;- 1\ntiempo_fallo &lt;- rinvgauss(n, mean = 100 / carga_trabajo, dispersion = 1)\n\n# Ajuste del modelo Inversa Gaussiana con enlace logarítmico\nmodelo_inversa_gauss &lt;- glm(tiempo_fallo ~ carga_trabajo, family = inverse.gaussian(link = \"1/mu^2\"),start = c(0.01, 0.01))\n\nWarning in sqrt(eta): Se han producido NaNs\n\n\nWarning: step size truncated due to divergence\n\n# Resumen del modelo\nsummary(modelo_inversa_gauss)\n\n\nCall:\nglm(formula = tiempo_fallo ~ carga_trabajo, family = inverse.gaussian(link = \"1/mu^2\"), \n    start = c(0.01, 0.01))\n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   -0.106143   0.462230  -0.230    0.819\ncarga_trabajo  0.008261   0.009623   0.859    0.393\n\n(Dispersion parameter for inverse.gaussian family taken to be 1.348442)\n\n    Null deviance: 94.085  on 99  degrees of freedom\nResidual deviance: 93.171  on 98  degrees of freedom\nAIC: 294.2\n\nNumber of Fisher Scoring iterations: 5",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelos de regresión generalizada</span>"
    ]
  },
  {
    "objectID": "tema4.html#comparación-de-modelos-y-evaluación-del-ajuste",
    "href": "tema4.html#comparación-de-modelos-y-evaluación-del-ajuste",
    "title": "4  Modelos de regresión generalizada",
    "section": "4.5 Comparación de modelos y evaluación del ajuste",
    "text": "4.5 Comparación de modelos y evaluación del ajuste\nUna vez que se han ajustado varios modelos, es crucial evaluar su rendimiento y seleccionar el más adecuado para el problema en cuestión. La evaluación del ajuste y la comparación de modelos permiten identificar cuál modelo describe mejor los datos sin caer en el sobreajuste, es decir, ajustarse demasiado a los datos de entrenamiento a costa de un mal desempeño en datos nuevos.\nEsta sección abordará los métodos más comunes para evaluar y comparar modelos, incluyendo criterios estadísticos, técnicas de validación y análisis de residuos.\n\n4.5.1 La Deviance\nLa deviance (o desviación) es una medida estadística que evalúa qué tan bien se ajusta un modelo estadístico a los datos observados. Se utiliza principalmente en modelos de regresión que no se basan en supuestos de normalidad estricta, como los modelos lineales generalizados (GLM), incluyendo:\n\nRegresión logística\nRegresión de Poisson\nModelos exponenciales, etc.\n\nLa deviance mide la diferencia entre el modelo propuesto y el modelo saturado (el modelo que predice perfectamente los datos observados).\n\nEl modelo saturado tiene tantos parámetros como observaciones, por lo que ajusta cada punto de datos perfectamente.\nEl modelo propuesto es el modelo que intentamos evaluar.\n\nLa deviance se interpreta como una generalización de la suma de cuadrados de los residuos utilizada en regresión lineal.\nLa deviance se define como:\n\\[\nD = 2 \\sum_{i=1}^{n} \\left[ \\ell(y_i; y_i) - \\ell(y_i; \\hat{y}_i) \\right]\n\\]\ndonde:\n\n\\(D\\) = Deviance\n\\(\\ell(y_i; y_i)\\) = Log-verosimilitud del modelo saturado (máxima verosimilitud posible)\n\\(\\ell(y_i; \\hat{y}_i)\\) = Log-verosimilitud del modelo propuesto\n\nEn términos simples, mide cuánto peor es el modelo propuesto comparado con el modelo que se ajusta perfectamente.\n\n4.5.1.1 Interpretación\n\nValores pequeños de deviance → Indican que el modelo se ajusta bien a los datos.\nValores grandes de deviance → Indican que el modelo no se ajusta bien.\n\nSi el modelo es perfecto, la deviance es cero.\n\n\n4.5.1.2 Deviance Residuals\nEn lugar de evaluar la deviance global, los residuos de deviance permiten identificar qué observaciones individuales no se ajustan bien.\n\\[\nd_i = \\text{sign}(y_i - \\hat{y}_i) \\sqrt{2 \\left[ \\ell(y_i; y_i) - \\ell(y_i; \\hat{y}_i) \\right]}\n\\]\nLos residuos de deviance se comportan de forma similar a los residuos en regresión lineal, permitiendo identificar observaciones atípicas o mal ajustadas.\n\n\n4.5.1.3 Relación con otros conceptos\n\nEn regresión lineal, la deviance es equivalente a la suma de los cuadrados de los residuos (RSS).\nEn regresión logística, la deviance se utiliza como alternativa a la suma de errores cuadráticos debido a que los residuos no se distribuyen normalmente.\nEn pruebas de bondad de ajuste, se usa la deviance nula para comparar el modelo sin predictores (solo la media) con el modelo completo.\n\n\n\n4.5.1.4 Ejemplo en Regresión Logística\nSupongamos que estamos modelando la probabilidad de que una persona compre un producto en función de su edad. La deviance nos ayuda a evaluar qué tan bien el modelo logra predecir esas probabilidades comparado con un modelo perfecto.\n\nDeviance baja: El modelo ajusta bien las probabilidades.\nDeviance alta: El modelo falla en capturar los patrones en los datos.\n\n\n\n\n4.5.2 Criterios de selección de modelos (AIC, BIC)\nLos criterios de selección de modelos como el AIC y el BIC permiten comparar modelos que han sido ajustados a los mismos datos, penalizando la complejidad para evitar el sobreajuste.\nAkaike Information Criterion (AIC)\nEl Criterio de Información de Akaike (AIC) es una medida que equilibra la calidad del ajuste y la complejidad del modelo. Se calcula como:\n\\[\nAIC = -2 \\log(L) + 2k\n\\]\nDonde:\n\n\\(L\\) es el log-likelihood del modelo (medida de la probabilidad de los datos dados los parámetros del modelo).\n\\(k\\) es el número de parámetros del modelo.\n\nRespecto a su interpretación, el AIC no tiene una interpretación absoluta, solo es útil para comparar modelos ajustados a los mismos datos. Un AIC menor indica un mejor equilibrio entre ajuste y simplicidad.\nBayesian Information Criterion (BIC)\nEl Criterio de Información Bayesiano (BIC) es similar al AIC, pero penaliza más severamente la complejidad del modelo, especialmente cuando el número de observaciones es grande. Se calcula como:\n\\[\nBIC = -2 \\log(L) + k \\log(n)\n\\]\nDonde: - \\(n\\) es el número de observaciones.\nEl BIC favorece modelos más simples en comparación con el AIC. Un BIC menor indica un mejor modelo.\n\n\n\n\n\n\nEjemplo: comparación de AIC y BIC\n\n\n\n\n\n\n# Comparación de modelos: Poisson vs Binomial Negativa\n\n# Modelo de Poisson\nmodelo_poisson &lt;- glm(accidentes ~ trafico + visibilidad, family = poisson, data = datos_accidentes)\n\n# Modelo Binomial Negativa\nlibrary(MASS)\nmodelo_binom_neg &lt;- glm.nb(accidentes ~ trafico + visibilidad, data = datos_accidentes)\n\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\n\n\nWarning in glm.nb(accidentes ~ trafico + visibilidad, data = datos_accidentes):\nalternation limit reached\n\n# Calcular AIC y BIC\nAIC(modelo_poisson, modelo_binom_neg)\n\n                 df      AIC\nmodelo_poisson    3 1216.421\nmodelo_binom_neg  4 1218.497\n\nBIC(modelo_poisson, modelo_binom_neg)\n\n                 df      BIC\nmodelo_poisson    3 1224.236\nmodelo_binom_neg  4 1228.918\n\n\n\n\n\nEl modelo con el menor AIC o BIC es preferido. Si ambos modelos tienen valores similares, se puede preferir el modelo más simple (con menos parámetros).\n\n\n4.5.3 Validación cruzada y técnicas de evaluación predictiva\nLa validación cruzada y otras técnicas de evaluación predictiva permiten estimar cómo se desempeñará un modelo en datos no vistos, lo que es esencial para evitar el sobreajuste.\nLa validación cruzada k-fold divide el conjunto de datos en k subconjuntos (o “folds”). El modelo se ajusta k veces, cada vez utilizando \\(k-1\\) folds para el entrenamiento y el fold restante para la prueba. El rendimiento se promedia sobre todas las iteraciones. La validación cruzada utiliza eficientemente los datos disponibles, proporcionando una estimación robusta del rendimiento del modelo.\n\n\n\n\n\n\nEjemplo: Validación cruzada\n\n\n\n\n\n\n# Instalar y cargar la librería caret\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\n# Definir la validación cruzada de 5-fold\ncontrol &lt;- trainControl(method = \"cv\", number = 5)\n\n# Ajustar un modelo de regresión logística con validación cruzada\nmodelo_cv &lt;- train(type ~ npreg + glu + bmi, data = Pima.tr, method = \"glm\", family = \"binomial\", trControl = control)\n\n# Resultados de la validación cruzada\nprint(modelo_cv)\n\nGeneralized Linear Model \n\n200 samples\n  3 predictor\n  2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 161, 160, 161, 159, 159 \nResampling results:\n\n  Accuracy   Kappa    \n  0.7603815  0.4378568\n\n\n\n\n\nOtra técnica común es dividir el conjunto de datos en un conjunto de entrenamiento (por ejemplo, el \\(70\\%\\)) y un conjunto de prueba (\\(30\\%\\)). El modelo se ajusta en el conjunto de entrenamiento y se evalúa en el conjunto de prueba.\n\n\n\n\n\n\nEjemplo: Train/Test\n\n\n\n\n\n\n# Dividir el conjunto de datos en entrenamiento y prueba\nset.seed(123)\nlibrary(caret)\nindices &lt;- createDataPartition(Pima.tr$type, p = 0.7, list = FALSE)\nentrenamiento &lt;- Pima.tr[indices, ]\nprueba &lt;- Pima.tr[-indices, ]\n\n# Ajustar el modelo en el conjunto de entrenamiento\nmodelo_entrenamiento &lt;- glm(type ~ npreg + glu + bmi, data = entrenamiento, family = binomial)\n\n# Realizar predicciones en el conjunto de prueba\npredicciones &lt;- predict(modelo_entrenamiento, newdata = prueba, type = \"response\")\n\n# Evaluar precisión\npredicciones_clase &lt;- ifelse(predicciones &gt; 0.5, \"Yes\", \"No\")\nconfusionMatrix(as.factor(predicciones_clase), as.factor(prueba$type))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction No Yes\n       No  34  10\n       Yes  5  10\n                                          \n               Accuracy : 0.7458          \n                 95% CI : (0.6156, 0.8502)\n    No Information Rate : 0.661           \n    P-Value [Acc &gt; NIR] : 0.1062          \n                                          \n                  Kappa : 0.3959          \n                                          \n Mcnemar's Test P-Value : 0.3017          \n                                          \n            Sensitivity : 0.8718          \n            Specificity : 0.5000          \n         Pos Pred Value : 0.7727          \n         Neg Pred Value : 0.6667          \n             Prevalence : 0.6610          \n         Detection Rate : 0.5763          \n   Detection Prevalence : 0.7458          \n      Balanced Accuracy : 0.6859          \n                                          \n       'Positive' Class : No              \n                                          \n\n\n\n\n\n\n\n4.5.4 Diagnóstico de residuos y buenas prácticas\nEl análisis de residuos es fundamental para evaluar el ajuste del modelo y detectar problemas como la falta de ajuste, valores atípicos o violaciones de los supuestos del modelo.\nLos residuos deviance son una medida común en los Modelos Lineales Generalizados (GLM). Representan la diferencia entre el modelo ajustado y el modelo perfecto (donde la predicción es exactamente igual al valor observado).\nTipos de residuos:\n\nResiduos Pearson:\n\\[\nr_i = \\frac{y_i - \\hat{y}_i}{\\sqrt{\\hat{V}(y_i)}}\n\\] Donde \\(\\hat{V}(y_i)\\) es la varianza estimada de \\(y_i\\).\nResiduos Deviance:\nRepresentan la contribución de cada observación al deviance total del modelo.\n\n\n\n\n\n\n\n\nEjemplo: Análisis de los residuos\n\n\n\n\n\n\n# Gráfico de residuos deviance para un modelo de Poisson\nplot(residuals(modelo_poisson, type = \"deviance\"), \n     main = \"Residuos Deviance del Modelo Poisson\", \n     ylab = \"Residuos Deviance\", xlab = \"Índice\")\nabline(h = 0, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n# Gráfico de residuos para la regresión logística\nplot(residuals(modelo_entrenamiento, type = \"deviance\"), \n     main = \"Residuos Deviance del Modelo Logístico\", \n     ylab = \"Residuos Deviance\", xlab = \"Índice\")\nabline(h = 0, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\nPara detectar valores atípicos y su influencia emplearemos:\n\nDistancia de Cook: Identifica observaciones influyentes que tienen un impacto significativo en los coeficientes del modelo.\nLeverage: Mide el impacto potencial de una observación en el ajuste del modelo.\n\n\n\n\n\n\n\nEjemplo: Detección de observaciones influyentes\n\n\n\n\n\n\n# Distancia de Cook para identificar observaciones influyentes\ncooksd &lt;- cooks.distance(modelo_poisson)\n\n# Gráfico de la distancia de Cook\nplot(cooksd, main = \"Distancia de Cook\", ylab = \"Influencia\", xlab = \"Índice\")\nabline(h = 4 / length(cooksd), col = \"red\")  # Línea de referencia\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoxe, Stefany, Stephen G West, y Leona S Aiken. 2009. «The analysis of count data: A gentle introduction to Poisson regression and its alternatives». Journal of personality assessment 91 (2): 121-36.\n\n\nHosmer Jr, David W, Stanley Lemeshow, y Rodney X Sturdivant. 2013. Applied logistic regression. John Wiley & Sons.\n\n\nLambert, Diane. 1992. «Zero-inflated Poisson regression, with an application to defects in manufacturing». Technometrics 34 (1): 1-14.\n\n\nNelder, John Ashworth, y Robert WM Wedderburn. 1972. «Generalized linear models». Journal of the Royal Statistical Society Series A: Statistics in Society 135 (3): 370-84.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelos de regresión generalizada</span>"
    ]
  },
  {
    "objectID": "tema5.html",
    "href": "tema5.html",
    "title": "6  Otros modelos de regresión: Modelos Aditivos Generalizados (GAMs)",
    "section": "",
    "text": "6.1 Fundamentos de los GAMs\nEn el análisis de datos y la modelización estadística, a menudo asumimos que las relaciones entre las variables independientes y la variable dependiente son lineales o que pueden transformarse fácilmente para cumplir con esta suposición. Sin embargo, en muchos contextos del mundo real, las relaciones entre las variables son no lineales y complejas, lo que limita la efectividad de los modelos de regresión tradicionales como la regresión lineal o polinomial.\nLos Modelos de Regresión Aditiva Generalizada (GAMs) ofrecen una solución poderosa y flexible para este problema. Los GAMs permiten modelar relaciones no lineales sin necesidad de especificar de antemano la forma exacta de la no linealidad (Hastie 2017). En lugar de ajustar una única función global para todos los predictores, los GAMs aplican funciones de suavizado a cada variable independiente por separado, lo que permite capturar patrones complejos y sutiles en los datos.\nUn Modelo Aditivo Generalizado (GAM) es una extensión de los Modelos Lineales Generalizados (GLM) vistos en el tema anterior y que permite que la relación entre la variable dependiente y las variables independientes sea no lineal y flexible. En un GAM, la variable dependiente se modela como una suma de funciones suavizadas de las variables independientes:\n\\[\ng(\\mu) = \\beta_0 + f_1(X_1) + f_2(X_2) + \\dots + f_p(X_p)\n\\]\nDonde:\nA diferencia de la regresión lineal, donde los predictores tienen una relación lineal con la variable dependiente, en los GAMs cada predictor puede tener una forma funcional diferente, permitiendo capturar curvas, patrones no lineales y efectos complejos en los datos.\nLos principales casos de uso de los GAMS son:\nLos Modelos Aditivos Generalizados (GAMs) tienen aplicaciones en una amplia variedad de disciplinas debido a su capacidad para capturar relaciones no lineales complejas. En medicina y epidemiología, se utilizan para modelar el riesgo de enfermedades en función de múltiples factores de riesgo que interactúan de manera no lineal, permitiendo identificar patrones sutiles en la salud de las poblaciones. En economía y finanzas, los GAMs son útiles para analizar la relación entre variables económicas, como la inflación y el crecimiento del PIB, donde las interacciones y los efectos pueden variar a lo largo del tiempo. En el campo de las ciencias ambientales, permiten modelar la relación entre la temperatura y la concentración de contaminantes atmosféricos, lo cual es crucial para entender el impacto del cambio climático. Finalmente, en marketing y negocios, los GAMs ayudan a analizar el comportamiento del cliente, como la probabilidad de compra, en función de variables como el ingreso y la edad, proporcionando insights valiosos para la toma de decisiones estratégicas.\nLos Modelos Aditivos Generalizados (GAMs) son una extensión de los Modelos Lineales Generalizados (GLM) que permiten capturar relaciones no lineales entre la variable dependiente y las variables independientes. Mientras que los GLM asumen una relación lineal (o lineal después de una transformación mediante una función de enlace), los GAMs relajan esta suposición al permitir que cada predictor tenga su propia forma funcional no paramétrica.\nUn GLM se expresa como:\n\\[\ng(\\mu) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p\n\\]\nDonde: - \\(g(\\mu)\\) es la función de enlace que relaciona la media de la variable dependiente (\\(\\mu\\)) con el predictor lineal. - \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\) son los coeficientes del modelo que representan el efecto lineal de cada predictor.\nEn contraste, un GAM se define como:\n\\[\ng(\\mu) = \\beta_0 + f_1(X_1) + f_2(X_2) + \\dots + f_p(X_p)\n\\]\nDonde \\(f_i(X_i)\\) son funciones suavizadas que capturan la relación (posiblemente no lineal) entre el predictor \\(X_i\\) y la variable dependiente.\nEn los GLM, los efectos de los predictores son estrictamente lineales o transformados de forma lineal. En los GAMs, la relación puede ser cualquier forma no paramétrica, determinada por los datos. Los GAMs ofrecen mayor flexibilidad al permitir que la forma de la relación entre cada predictor y la respuesta sea modelada directamente a partir de los datos. Aunque los GAMs son más flexibles, siguen siendo interpretables, ya que el efecto de cada variable puede visualizarse y analizarse individualmente.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Otros modelos de regresión: Modelos Aditivos Generalizados (GAMs)</span>"
    ]
  },
  {
    "objectID": "tema5.html#fundamentos-de-los-modelos-aditivos-generalizados-gams",
    "href": "tema5.html#fundamentos-de-los-modelos-aditivos-generalizados-gams",
    "title": "5  Otros modelos de regresión: GAMS",
    "section": "",
    "text": "5.1.1 Suavizado en los GAMs\nEl componente fundamental de los GAMs es el uso de funciones de suavizado, que permiten modelar relaciones no lineales de manera flexible y controlada. El suavizado evita el sobreajuste (overfitting) al no intentar seguir cada fluctuación en los datos, sino al capturar las tendencias generales subyacentes.\nEl suavizado consiste en ajustar una curva a los datos de tal manera que se capturen las tendencias generales sin que el modelo sea demasiado sensible al ruido o a las fluctuaciones aleatorias. En el contexto de los GAMs, cada predictor tiene su propia función de suavizado que determina cómo se ajusta la relación entre esa variable y la variable dependiente.\n\\[\ng(\\mu) = \\beta_0 + s_1(X_1) + s_2(X_2) + \\dots + s_p(X_p)\n\\]\nDonde \\(s_i(X_i)\\) representa una función suavizada para el predictor \\(X_i\\).\nEl grado de suavizado controla cuánto sigue el modelo las fluctuaciones de los datos:\n\nSuavizado bajo: El modelo se ajusta demasiado a los datos, capturando incluso el ruido aleatorio. Esto puede llevar al sobreajuste.\nSuavizado alto: El modelo puede no capturar adecuadamente la estructura subyacente de los datos, llevando al subajuste (underfitting).\n\nEl criterio de suavizado óptimo se selecciona automáticamente mediante técnicas como la minimización del criterio de información de Akaike (AIC) o el uso de validación cruzada.\nEl suavizado en los Modelos Aditivos Generalizados (GAMs) ofrece múltiples ventajas que los convierten en una herramienta poderosa para el análisis de datos complejos. En primer lugar, permite capturar no linealidades complejas, detectando patrones que no pueden ser representados adecuadamente por términos lineales o polinomiales simples. Esta flexibilidad es crucial para modelar relaciones reales que rara vez son estrictamente lineales. Además, el suavizado ayuda a evitar el sobreajuste; a diferencia de los polinomios de alto grado, que pueden generar oscilaciones indeseadas y seguir de manera excesiva las fluctuaciones del ruido en los datos, el suavizado controlado proporciona una representación más estable y generalizable de la relación entre las variables. Finalmente, una de las características más valiosas del suavizado en GAMs es su interpretación intuitiva. Las funciones suavizadas pueden visualizarse de manera clara y directa, lo que facilita la comprensión del impacto de cada predictor sobre la variable de respuesta, haciendo que los GAMs sean no solo potentes, sino también accesibles desde el punto de vista interpretativo.\n\n\n5.1.2 Splines\nEn los GAMs, las funciones de suavizado se implementan comúnmente mediante splines, que son funciones polinómicas definidas por tramos. Estas permiten una flexibilidad controlada al ajustar diferentes tramos de los datos mientras se mantiene la continuidad y la suavidad en los puntos de unión (nudos).\n\nSplines Lineales:\nSon polinomios de primer grado ajustados por tramos. Aunque permiten cierta flexibilidad, pueden generar ángulos agudos en los puntos de unión.\nSplines Cúbicos:\nUtilizan polinomios de tercer grado en cada tramo, asegurando continuidad en la primera y segunda derivada en los nudos. Los splines cúbicos son los más utilizados en la práctica debido a su capacidad para capturar curvaturas suaves sin introducir oscilaciones no deseadas.\n\nLos splines penalizados añaden una penalización al modelo para controlar la suavidad de la curva. Esto se logra añadiendo un término de penalización al proceso de ajuste que limita la complejidad de la función suavizada.\n\\[\n\\min \\left( \\sum_{i=1}^{n} (y_i - f(x_i))^2 + \\lambda \\int [f''(x)]^2 dx \\right)\n\\]\nDonde:\n\n\\(\\lambda\\) es el parámetro de suavizado que controla el equilibrio entre el ajuste a los datos y la suavidad de la curva.\nSi \\(\\lambda\\) es grande, el modelo será más suave; si es pequeño, el modelo se ajustará más a los datos.\n\nEl número y la ubicación de los nudos (puntos donde cambian los tramos polinómicos) es un aspecto crucial en el ajuste de splines:\n\nMuchos nudos: Mayor flexibilidad, pero riesgo de sobreajuste.\nPocos nudos: Modelo más simple, pero riesgo de no capturar la estructura subyacente de los datos.\n\nLa elección óptima del número de nudos puede realizarse mediante criterios automáticos como el AIC o mediante validación cruzada.\n\n\n\n\n\n\nEjemplo: Ajuste de un GAM con Splines\n\n\n\n\n\nVamos a ajustar un GAM utilizando la librería mgcv en R, que es una de las herramientas más utilizadas para trabajar con GAMs.\n\n# Instalar y cargar la librería mgcv\ninstall.packages(\"mgcv\")\n\nInstalling package into '/home/pedro/R/x86_64-pc-linux-gnu-library/4.3'\n(as 'lib' is unspecified)\n\nlibrary(mgcv)\n\nLoading required package: nlme\n\n\nThis is mgcv 1.9-1. For overview type 'help(\"mgcv-package\")'.\n\n# Simulación de datos\nset.seed(123)\nn &lt;- 100\nx &lt;- seq(0, 10, length.out = n)\ny &lt;- sin(x) + rnorm(n, sd = 0.3)\n\n# Ajuste de un GAM con splines cúbicos\nmodelo_gam &lt;- gam(y ~ s(x), method = \"REML\")\n\n# Resumen del modelo\nsummary(modelo_gam)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.20631    0.02749   7.505 4.01e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n      edf Ref.df     F p-value    \ns(x) 8.03   8.75 62.31  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.846   Deviance explained = 85.9%\n-REML =  30.36  Scale est. = 0.075575  n = 100\n\n# Visualización de la función suavizada\nplot(modelo_gam, main = \"Ajuste GAM con Splines Cúbicos\", shade = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Otros modelos de regresión: GAMS</span>"
    ]
  },
  {
    "objectID": "tema5.html#ajuste-e-interpretación",
    "href": "tema5.html#ajuste-e-interpretación",
    "title": "5  Otros modelos de regresión: GAMS",
    "section": "5.2 Ajuste e interpretación",
    "text": "5.2 Ajuste e interpretación\n\n\n5.2.0.1 Estructura del Tema\nEste tema abordará los Modelos Aditivos Generalizados (GAMs) de manera integral, desde su fundamentación teórica hasta su aplicación práctica:\n\nFundamentos de los GAMs:\n\nDefinición y comparación con los Modelos Lineales Generalizados (GLM).\nLa importancia del suavizado en los GAMs.\nElección de funciones de suavizado (splines, splines cúbicos, etc.).\n\nAjuste e Interpretación de los GAMs:\n\nCómo ajustar GAMs en R usando la librería mgcv.\nInterpretación de los resultados y visualización de los efectos suavizados.\n\nEvaluación del Modelo y Selección de Parámetros:\n\nTécnicas para evaluar la calidad del ajuste en GAMs.\nSelección del grado de suavizado y control del sobreajuste.\n\nCasos Prácticos con GAMs:\n\nAplicación de GAMs en problemas del mundo real en diferentes disciplinas.\nComparación de GAMs con modelos lineales y GLMs tradicionales.\n\n\n\n\n\n5.2.0.2 Conclusión\nLos Modelos Aditivos Generalizados (GAMs) son una herramienta esencial para cualquier analista de datos que necesite modelar relaciones complejas y no lineales. Su capacidad para adaptarse a patrones complejos sin perder interpretabilidad los convierte en una opción poderosa en una amplia gama de aplicaciones, desde la medicina hasta la economía y la ingeniería.\nA lo largo de este tema, aprenderás cómo aplicar los GAMs para resolver problemas complejos, interpretar sus resultados y comparar su rendimiento con otros modelos de regresión. Al finalizar, tendrás una comprensión sólida de cuándo y cómo utilizar GAMs en tus propios análisis.\n\n\n\n\nHastie, Trevor J. 2017. «Generalized additive models». En Statistical models in S, 249-307. Routledge.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Otros modelos de regresión: GAMS</span>"
    ]
  },
  {
    "objectID": "tema5.html#fundamentos-de-los-gams",
    "href": "tema5.html#fundamentos-de-los-gams",
    "title": "6  Otros modelos de regresión: Modelos Aditivos Generalizados (GAMs)",
    "section": "",
    "text": "6.1.1 Suavizado en los GAMs\nEl componente fundamental de los GAMs es el uso de funciones de suavizado, que permiten modelar relaciones no lineales de manera flexible y controlada. El suavizado evita el sobreajuste (overfitting) al no intentar seguir cada fluctuación en los datos, sino al capturar las tendencias generales subyacentes.\nEl suavizado consiste en ajustar una curva a los datos de tal manera que se capturen las tendencias generales sin que el modelo sea demasiado sensible al ruido o a las fluctuaciones aleatorias. En el contexto de los GAMs, cada predictor tiene su propia función de suavizado que determina cómo se ajusta la relación entre esa variable y la variable dependiente.\n\\[\ng(\\mu) = \\beta_0 + s_1(X_1) + s_2(X_2) + \\dots + s_p(X_p)\n\\]\nDonde \\(s_i(X_i)\\) representa una función suavizada para el predictor \\(X_i\\).\nEl grado de suavizado controla cuánto sigue el modelo las fluctuaciones de los datos:\n\nSuavizado bajo: El modelo se ajusta demasiado a los datos, capturando incluso el ruido aleatorio. Esto puede llevar al sobreajuste.\nSuavizado alto: El modelo puede no capturar adecuadamente la estructura subyacente de los datos, llevando al subajuste (underfitting).\n\nEl criterio de suavizado óptimo se selecciona automáticamente mediante técnicas como la minimización del criterio de información de Akaike (AIC) o el uso de validación cruzada.\nEl suavizado en los Modelos Aditivos Generalizados (GAMs) ofrece múltiples ventajas que los convierten en una herramienta poderosa para el análisis de datos complejos. En primer lugar, permite capturar no linealidades complejas, detectando patrones que no pueden ser representados adecuadamente por términos lineales o polinomiales simples. Esta flexibilidad es crucial para modelar relaciones reales que rara vez son estrictamente lineales. Además, el suavizado ayuda a evitar el sobreajuste; a diferencia de los polinomios de alto grado, que pueden generar oscilaciones indeseadas y seguir de manera excesiva las fluctuaciones del ruido en los datos, el suavizado controlado proporciona una representación más estable y generalizable de la relación entre las variables. Finalmente, una de las características más valiosas del suavizado en GAMs es su interpretación intuitiva. Las funciones suavizadas pueden visualizarse de manera clara y directa, lo que facilita la comprensión del impacto de cada predictor sobre la variable de respuesta, haciendo que los GAMs sean no solo potentes, sino también accesibles desde el punto de vista interpretativo.\n\n\n6.1.2 Splines\nEn los GAMs, las funciones de suavizado se implementan comúnmente mediante splines, que son funciones polinómicas definidas por tramos. Estas permiten una flexibilidad controlada al ajustar diferentes tramos de los datos mientras se mantiene la continuidad y la suavidad en los puntos de unión (nudos).\n\nSplines Lineales:\nSon polinomios de primer grado ajustados por tramos. Aunque permiten cierta flexibilidad, pueden generar ángulos agudos en los puntos de unión.\nSplines Cúbicos:\nUtilizan polinomios de tercer grado en cada tramo, asegurando continuidad en la primera y segunda derivada en los nudos. Los splines cúbicos son los más utilizados en la práctica debido a su capacidad para capturar curvaturas suaves sin introducir oscilaciones no deseadas.\n\nLos splines penalizados añaden una penalización al modelo para controlar la suavidad de la curva. Esto se logra añadiendo un término de penalización al proceso de ajuste que limita la complejidad de la función suavizada.\n\\[\n\\min \\left( \\sum_{i=1}^{n} (y_i - f(x_i))^2 + \\lambda \\int [f''(x)]^2 dx \\right)\n\\]\nDonde:\n\n\\(\\lambda\\) es el parámetro de suavizado que controla el equilibrio entre el ajuste a los datos y la suavidad de la curva.\nSi \\(\\lambda\\) es grande, el modelo será más suave; si es pequeño, el modelo se ajustará más a los datos.\n\nEl número y la ubicación de los nudos (puntos donde cambian los tramos polinómicos) es un aspecto crucial en el ajuste de splines:\n\nMuchos nudos: Mayor flexibilidad, pero riesgo de sobreajuste.\nPocos nudos: Modelo más simple, pero riesgo de no capturar la estructura subyacente de los datos.\n\nLa elección óptima del número de nudos puede realizarse mediante criterios automáticos como el AIC o mediante validación cruzada.\n\n\n\n\n\n\nEjemplo: Ajuste de un GAM con Splines\n\n\n\n\n\nVamos a ajustar un GAM utilizando la librería mgcv en R, que es una de las herramientas más utilizadas para trabajar con GAMs.\n\n# Instalar y cargar la librería mgcv\nlibrary(mgcv)\n\nLoading required package: nlme\n\n\nThis is mgcv 1.9-1. For overview type 'help(\"mgcv-package\")'.\n\n# Simulación de datos\nset.seed(123)\nn &lt;- 100\nx &lt;- seq(0, 10, length.out = n)\ny &lt;- sin(x) + rnorm(n, sd = 0.3)\n\n# Ajuste de un GAM con splines cúbicos\nmodelo_gam &lt;- gam(y ~ s(x), method = \"REML\")\n\n# Resumen del modelo\nsummary(modelo_gam)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.20631    0.02749   7.505 4.01e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n      edf Ref.df     F p-value    \ns(x) 8.03   8.75 62.31  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.846   Deviance explained = 85.9%\n-REML =  30.36  Scale est. = 0.075575  n = 100\n\n# Visualización de la función suavizada\nplot(modelo_gam, main = \"Ajuste GAM con Splines Cúbicos\", shade = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Otros modelos de regresión: Modelos Aditivos Generalizados (GAMs)</span>"
    ]
  },
  {
    "objectID": "tema5.html#interpretación-de-los-resultados",
    "href": "tema5.html#interpretación-de-los-resultados",
    "title": "6  Otros modelos de regresión: Modelos Aditivos Generalizados (GAMs)",
    "section": "6.2 Interpretación de los resultados",
    "text": "6.2 Interpretación de los resultados\nDespués de ajustar el modelo, el siguiente paso es interpretar los resultados proporcionados por la función summary() de R.\nEl comando summary(modelo_gam) proporciona la siguiente información clave:\n\nResumen de la suavización:\n\nedf (effective degrees of freedom): Indica el grado de flexibilidad del suavizado.\n\nUn edf cercano a 1 sugiere una relación lineal.\n\nUn edf mayor que 1 indica una relación no lineal.\n\n\nSignificancia de los predictores:\n\np-values: Indican si la función suavizada para cada predictor es significativa. Un valor \\(p &lt; 0.05\\) sugiere que la relación entre el predictor y la variable dependiente es estadísticamente significativa.\n\nMedidas de ajuste:\n\nDeviance explained: Similar al \\(R^2\\) en regresión lineal, indica el porcentaje de la variabilidad de los datos explicada por el modelo.\nGCV score y AIC: Utilizados para evaluar la calidad del ajuste y comparar diferentes modelos.\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\nsummary(modelo_gam)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.20631    0.02749   7.505 4.01e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n      edf Ref.df     F p-value    \ns(x) 8.03   8.75 62.31  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.846   Deviance explained = 85.9%\n-REML =  30.36  Scale est. = 0.075575  n = 100\n\n\n\nEl intercepto (0.01852) no es significativo, lo que sugiere que el valor medio de \\(y\\) cuando \\(x = 0\\) no difiere significativamente de cero.\nLa función suavizada \\(s(x)\\) tiene un edf de 7.54, indicando que la relación entre \\(x\\) e \\(y\\) es altamente no lineal.\nEl valor p (&lt;2e-16) para \\(s(x)\\) sugiere que la relación no lineal es estadísticamente significativa.\nEl 82.1% de la devianza explicada indica que el modelo captura bien la variabilidad de los datos.\nUn GCV (Generalized Cross-Validation) bajo indica un buen ajuste.\n\n\n\n\nUna de las principales ventajas de los GAMs es la posibilidad de visualizar fácilmente la relación entre cada predictor y la variable dependiente. La función plot() de mgcv permite crear gráficos claros e intuitivos de los efectos suavizados.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Visualización del efecto suavizado de x en y\nplot(modelo_gam, shade = TRUE, main = \"Relación No Lineal entre x e y\")\n\n\n\n\n\n\n\n\nEl área sombreada alrededor de la curva representa el intervalo de confianza al 95%. La forma de la curva muestra la relación entre \\(x\\) e $ y$; en este caso, debería reflejar una forma sinusoidal. Si la curva es recta, la relación es aproximadamente lineal.\n\n\n\nEs posible personalizar el gráfico para mejorar la presentación:\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Personalización avanzada del gráfico\nplot(modelo_gam, \n     residuals = TRUE,  # Muestra los residuos\n     pch = 19,          # Estilo de los puntos de datos\n     col = \"blue\",      # Color de la curva suavizada\n     seWithMean = TRUE, # Muestra intervalos de confianza ajustados al promedio\n     rug = TRUE,        # Añade marcas en el eje x para indicar la densidad de los datos\n     main = \"Efecto Suavizado de x sobre y\")\n\n\n\n\n\n\n\n\n\n\n\nSi el modelo incluye múltiples predictores suavizados, plot() creará un gráfico para cada uno.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Simulación de un segundo predictor\nset.seed(123)\nx2 &lt;- seq(0, 5, length.out = n)\ny2 &lt;- sin(x) + log(x2 + 1) + rnorm(n, sd = 0.3)\n\n# Ajuste del GAM con dos predictores suavizados\nmodelo_gam_multi &lt;- gam(y2 ~ s(x) + s(x2), method = \"REML\")\n\n# Visualización de los efectos suavizados\nplot(modelo_gam_multi, pages = 1, shade = TRUE, main = \"Efectos Suavizados de x y x2\")\n\n\n\n\n\n\n\n\nEl parámetro pages = 1 muestra todos los efectos en una sola página. Cada gráfico muestra cómo cada predictor afecta la variable dependiente, permitiendo una interpretación clara de efectos individuales.\n\n\n\nUn valor de edf cercano a 1 indica un efecto lineal, mientras que valores mayores sugieren una relación no lineal más compleja. La significancia estadística de los efectos suavizados indica qué predictores tienen una relación significativa con la variable dependiente. La devianza explicada y el AIC proporcionan medidas para evaluar la calidad del ajuste y comparar diferentes modelos. Los gráficos permiten identificar patrones no lineales complejos y facilitan la comunicación de los resultados a audiencias no técnicas.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Otros modelos de regresión: Modelos Aditivos Generalizados (GAMs)</span>"
    ]
  },
  {
    "objectID": "tema5.html#evaluación-del-modelo-y-selección-de-parámetros-en-gams",
    "href": "tema5.html#evaluación-del-modelo-y-selección-de-parámetros-en-gams",
    "title": "6  Otros modelos de regresión: Modelos Aditivos Generalizados (GAMs)",
    "section": "6.3 Evaluación del modelo y selección de parámetros en GAMs",
    "text": "6.3 Evaluación del modelo y selección de parámetros en GAMs\nUna vez ajustado un Modelo Aditivo Generalizado (GAM), es fundamental evaluar su calidad y ajustar adecuadamente el grado de suavizado para garantizar que el modelo capture las relaciones relevantes sin caer en el sobreajuste o subajuste. Esta sección explora las técnicas para evaluar el rendimiento de los GAMs, identificar problemas en el ajuste y seleccionar los parámetros óptimos de suavizado.\n\n6.3.1 Técnicas para evaluar la calidad del ajuste en GAMs\nLa evaluación de la calidad del ajuste en GAMs implica una combinación de métricas estadísticas y diagnósticos gráficos. Estas herramientas permiten determinar qué tan bien el modelo se ajusta a los datos y si los supuestos subyacentes son válidos.\nLa deviance explicada es una medida análoga al \\(R^2\\) en la regresión lineal. Indica la proporción de la variabilidad de la variable dependiente que es explicada por el modelo:\n\\[\n\\text{Deviance Explicada} = 1 - \\frac{\\text{Deviance del Modelo}}{\\text{Deviance del Modelo Nulo}}\n\\]\n\nValores cercanos a 1 indican que el modelo explica bien la variabilidad de los datos.\nValores cercanos a 0 sugieren que el modelo no captura adecuadamente la estructura de los datos.\n\nEl \\(R^2\\) ajustado también puede interpretarse en modelos GAM cuando la familia es gaussian. Este valor se reporta en la salida de summary(modelo_gam).\n\n\n6.3.2 Criterios de Información (AIC, GCV)\nLos criterios de información permiten comparar modelos y evaluar su capacidad para generalizar a nuevos datos. Dos de las métricas más comunes en GAMs son:\nAIC (Akaike Information Criterion):\n\\[\nAIC = -2 \\log(L) + 2k\n\\]\nVisto en temas anteriores, donde \\(L\\) es el log-likelihood del modelo y \\(k\\) es el número de parámetros. Un AIC más bajo sugiere un modelo mejor ajustado.\nGCV (Generalized Cross-Validation):\nEl GCV es una medida específica para modelos de suavizado y penalización. Estima el error de predicción esperado usando una forma eficiente de validación cruzada. Un GCV bajo indica un buen ajuste sin sobreajuste.\n\n\n6.3.3 Análisis de residuos\nComo en otros modelos de regresión, el análisis de residuos es una herramienta esencial para diagnosticar el ajuste del modelo y detectar patrones no explicados.\n\nResiduos Pearson y Deviance:\nDeben distribuirse aleatoriamente alrededor de cero si el modelo está bien especificado.\nGráficos de residuos:\nPermiten identificar valores atípicos, heterocedasticidad y patrones no capturados por el modelo.\n\n\n\n\n\n\n\n\nEjemplo: Evaluación del ajuste de un GAM\n\n\n\n\n\n\n# Ajuste del modelo GAM\nlibrary(mgcv)\nset.seed(123)\nn &lt;- 200\nx &lt;- seq(0, 10, length.out = n)\ny &lt;- sin(x) + rnorm(n, sd = 0.3)\n\nmodelo_gam &lt;- gam(y ~ s(x), method = \"REML\")\n\n# Resumen del modelo\nsummary(modelo_gam)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.17902    0.02011   8.902  4.3e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n       edf Ref.df     F p-value    \ns(x) 8.414  8.904 123.7  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.847   Deviance explained = 85.3%\n-REML =  52.84  Scale est. = 0.080887  n = 200\n\n# Verificar la deviance explicada y el AIC\ncat(\"Deviance Explicada:\", summary(modelo_gam)$dev.expl, \"\\n\")\n\nDeviance Explicada: 0.8533086 \n\ncat(\"AIC del Modelo:\", AIC(modelo_gam), \"\\n\")\n\nAIC del Modelo: 75.89817 \n\n# Diagnóstico de residuos\npar(mfrow = c(2, 2))\ngam.check(modelo_gam)\n\n\n\n\n\n\n\n\n\nMethod: REML   Optimizer: outer newton\nfull convergence after 8 iterations.\nGradient range [-3.529919e-09,3.476724e-09]\n(score 52.83958 & scale 0.08088732).\nHessian positive definite, eigenvalue range [3.67346,99.14414].\nModel rank =  10 / 10 \n\nBasis dimension (k) checking results. Low p-value (k-index&lt;1) may\nindicate that k is too low, especially if edf is close to k'.\n\n       k'  edf k-index p-value\ns(x) 9.00 8.41     1.1    0.94\n\n\n\n\n\nEl comando gam.check() proporciona múltiples gráficos de diagnóstico para evaluar la adecuación del modelo, incluyendo la distribución de los residuos y la comprobación del grado de suavizado.\n\n\n6.3.4 Selección del grado de suavizado y control del sobreajuste\nEl grado de suavizado en los GAMs controla la flexibilidad del modelo. Un suavizado adecuado permite capturar la estructura subyacente de los datos sin ajustarse al ruido aleatorio.\nEl grado de suavizado determina la complejidad de la función que se ajusta a los datos. Esto se representa mediante los grados de libertad efectivos (edf):\n\nedf cercano a 1: Indica una relación casi lineal.\nedf mayor que 1: Sugiere una relación no lineal más compleja.\n\nUn grado de suavizado demasiado bajo puede llevar al subajuste (el modelo no captura adecuadamente los patrones de los datos), mientras que un suavizado excesivo puede llevar al sobreajuste (el modelo sigue el ruido en lugar de la tendencia general).\nLa librería mgcv ajusta automáticamente el grado de suavizado utilizando métodos de optimización como REML (Restricted Maximum Likelihood) o GCV (Generalized Cross-Validation).\n\nmethod = \"REML\": Proporciona un ajuste más robusto y es menos propenso al sobreajuste que GCV.\nmethod = \"GCV.Cp\": Utiliza la validación cruzada para seleccionar el suavizado, pero puede ser más sensible al ruido.\n\n\n\n\n\n\n\nEjemplo: Comparación de métodos de suavizado\n\n\n\n\n\n\n# Ajuste usando REML\nmodelo_reml &lt;- gam(y ~ s(x), method = \"REML\")\n\n# Ajuste usando GCV\nmodelo_gcv &lt;- gam(y ~ s(x), method = \"GCV.Cp\")\n\n# Comparación de AIC y GCV\ncat(\"AIC (REML):\", AIC(modelo_reml), \"\\n\")\n\nAIC (REML): 75.89817 \n\ncat(\"GCV (GCV.Cp):\", modelo_gcv$gcv.ubre, \"\\n\")\n\nGCV (GCV.Cp): 0.08456186 \n\n\n\n\n\nAunque mgcv selecciona automáticamente el grado de suavizado, es posible controlar manualmente la complejidad del modelo especificando el número de bases de suavizado mediante el argumento k en la función s():\n\nk pequeño: Menor flexibilidad, puede llevar al subajuste.\nk grande: Mayor flexibilidad, riesgo de sobreajuste.\n\n\n\n\n\n\n\nEjemplo: Control manual del suavizado\n\n\n\n\n\n\n# Menor suavizado (k = 3)\nmodelo_suave &lt;- gam(y ~ s(x, k = 3), method = \"REML\")\n\n# Mayor suavizado (k = 20)\nmodelo_mas_flexible &lt;- gam(y ~ s(x, k = 20), method = \"REML\")\n\n# Visualización de los diferentes ajustes\npar(mfrow = c(1, 2))\nplot(modelo_suave, main = \"Suavizado Bajo (k=3)\", shade = TRUE)\nplot(modelo_mas_flexible, main = \"Suavizado Alto (k=20)\", shade = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nLa validación cruzada es una técnica robusta para seleccionar el grado de suavizado. Tal y como hemos visto en temas anteriores, en este enfoque, el conjunto de datos se divide en varios subconjuntos (folds), y el modelo se entrena y evalúa en diferentes combinaciones de estos subconjuntos.\n\n\n\n\n\n\nEjemplo: Valización cruzada para GAMs\n\n\n\n\n\n\n# Cargar librerías necesarias\nlibrary(cvTools)\n\nLoading required package: lattice\n\n\nLoading required package: robustbase\n\nlibrary(mgcv)\n\n# Configuración de la validación cruzada 5-fold\nset.seed(123)\nfolds &lt;- cvFolds(n = n, K = 5)\n\n# Inicializar un vector para almacenar el error\nerrores &lt;- numeric(5)\n\n# Validación cruzada manual\nfor (i in 1:5) {\n  # Dividir en conjunto de entrenamiento y prueba\n  test_idx &lt;- which(folds$which == i)\n  train_idx &lt;- setdiff(1:n, test_idx)\n  \n  # Ajustar el modelo en el conjunto de entrenamiento\n  modelo_gam_cv &lt;- gam(y ~ s(x), data = data.frame(x = x[train_idx], y = y[train_idx]), method = \"REML\")\n  \n  # Predecir en el conjunto de prueba\n  predicciones &lt;- predict(modelo_gam_cv, newdata = data.frame(x = x[test_idx]))\n  \n  # Calcular el error cuadrático medio (RMSE)\n  errores[i] &lt;- sqrt(mean((y[test_idx] - predicciones)^2))\n}\n\n# Promedio del error de la validación cruzada\nerror_promedio &lt;- mean(errores)\ncat(\"Error Promedio (RMSE) de la Validación Cruzada:\", error_promedio)\n\nError Promedio (RMSE) de la Validación Cruzada: 0.2946876\n\n\n\n\n\n\n\n6.3.5 Diagnóstico de sobreajuste\nUn modelo sobreajustado sigue de cerca las fluctuaciones del ruido en los datos, lo que puede detectarse mediante:\n\nResiduos estructurados: Los residuos no deberían mostrar patrones sistemáticos.\nCurvas excesivamente flexibles: Si la curva suavizada presenta oscilaciones innecesarias, es señal de sobreajuste.\nBaja generalización: Evaluar el rendimiento del modelo en datos de prueba puede revelar problemas de sobreajuste.\n\n\n\n\n\n\nHastie, Trevor J. 2017. «Generalized additive models». En Statistical models in S, 249-307. Routledge.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Otros modelos de regresión: Modelos Aditivos Generalizados (GAMs)</span>"
    ]
  },
  {
    "objectID": "tema0.html",
    "href": "tema0.html",
    "title": "1  Introducción a los modelos de regresión",
    "section": "",
    "text": "1.1 Predecir vs. explicar\nEste tema inaugural tiene como misión construir el andamiaje conceptual y filosófico sobre el que se asienta el modelado estadístico moderno. A lo largo de estas páginas, contextualizaremos la regresión no solo como una técnica, sino como un marco de pensamiento indispensable en la ciencia de datos y en cualquier disciplina de investigación cuantitativa. Exploraremos en profundidad su propósito dual, desgranaremos sus componentes axiomáticos hasta el último detalle, y ofreceremos una visión panorámica, rica en matices, de la vasta familia de modelos de regresión. El objetivo es preparar al lector, con solidez y sin prisas, para las inmersiones técnicas que seguirán en los capítulos posteriores. Como lectura complementaria que comparte esta filosofía de aprendizaje profundo pero aplicado, recomendamos encarecidamente la obra de (James et al. 2021).\nEl modelado de regresión constituye una de las herramientas más potentes y flexibles del arsenal estadístico. Ofrece un marco metodológico riguroso para investigar y cuantificar las relaciones entre un conjunto de variables, y su aplicabilidad abarca un espectro extraordinariamente amplio de disciplinas: desde la física de partículas y la ingeniería aeroespacial, donde se usa para modelar sistemas complejos, hasta la econometría, la psicometría, la epidemiología o las finanzas, donde es fundamental para entender mercados y comportamientos.\nAunque en la práctica ambos objetivos a menudo se entrelazan, conceptualmente, el modelado estadístico se orienta hacia uno de dos polos, una dicotomía fundamental articulada brillantemente por (Shmueli 2010): la predicción o la inferencia (explicación). Comprender esta distinción es el primer paso para convertirse en un modelador eficaz.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción a los modelos de regresión</span>"
    ]
  },
  {
    "objectID": "tema0.html#sec-proposito",
    "href": "tema0.html#sec-proposito",
    "title": "1  Introducción a los modelos de regresión",
    "section": "",
    "text": "Predicción: El objetivo principal es la precisión. Se busca construir un modelo que pueda estimar con el menor error posible el valor de una variable de interés (la respuesta) basándose en la información proporcionada por otras variables (las predictoras). En este paradigma, el modelo puede ser tratado como una “caja negra” (black box). Su funcionamiento interno o la interpretabilidad de sus componentes son secundarios, siempre y cuando sus predicciones sean consistentemente fiables y robustas en datos no observados previamente.\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nUna entidad financiera quiere predecir la probabilidad de que un cliente incurra en impago de un crédito. Utilizan variables como la edad, ingresos, nivel de estudios y historial crediticio. El banco no necesita necesariamente entender la “causa” exacta del impago; su principal interés es tener un modelo que clasifique correctamente a los futuros solicitantes como de alto o bajo riesgo para minimizar pérdidas.\n\n\n\n\nInferencia: El foco se desplaza radicalmente hacia la comprensión y la interpretación. El objetivo no es solo predecir, sino dilucidar la naturaleza de las interdependencias entre las variables. Se busca cuantificar cómo un cambio en una variable predictora influye, ya sea de forma causal o asociativa, en la variable de respuesta. Aquí, la interpretabilidad del modelo es primordial. El interés reside en la magnitud, el signo y, crucialmente, la incertidumbre estadística (expresada mediante errores estándar, intervalos de confianza y p-valores) de los parámetros estimados.\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nUna epidemióloga investiga los factores de riesgo de una enfermedad cardíaca. Modela la presión arterial en función de variables como el índice de masa corporal (IMC), el consumo diario de sal y las horas de ejercicio semanales. Su objetivo no es solo predecir la presión arterial de un paciente, sino entender y cuantificar la relación: “¿En cuántos mmHg aumenta la presión arterial, en promedio, por cada gramo adicional de sal consumido al día, manteniendo constantes el IMC y el ejercicio?”. La respuesta a esta pregunta tiene implicaciones directas para la salud pública y las recomendaciones dietéticas.\n\n\n\n\n\n\n\n\n\nUna relación simbiótica\n\n\n\nAunque conceptualmente distintos, ambos objetivos no son mutuamente excluyentes; a menudo se benefician el uno del otro. Un modelo con una base inferencial sólida, que captura relaciones causales o asociativas verdaderas, suele tener un buen rendimiento predictivo. A la inversa, un modelo que demuestra una alta precisión predictiva en datos nuevos nos da confianza en que las relaciones que ha aprendido no son meras casualidades del conjunto de datos de entrenamiento, sino que probablemente reflejen patrones reales y generalizables. La tensión entre interpretabilidad y precisión es uno de los debates más fascinantes en la ciencia de datos moderna.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción a los modelos de regresión</span>"
    ]
  },
  {
    "objectID": "tema0.html#sec-componentes",
    "href": "tema0.html#sec-componentes",
    "title": "1  Introducción a los modelos de regresión",
    "section": "1.2 Anatomía de un modelo de regresión: los componentes axiomáticos",
    "text": "1.2 Anatomía de un modelo de regresión: los componentes axiomáticos\nTodo modelo de regresión, desde el más simple hasta el más sofisticado, se construye sobre tres pilares fundamentales. Estos componentes, definidos en textos clásicos como el de (Kutner et al. 2005), son los ladrillos con los que edificaremos todo nuestro conocimiento.\n\n1.2.1 La variable de respuesta\nTambién designada como variable dependiente, variable de salida, target, variable objetivo o variable explicada. Representa el fenómeno o la característica principal cuyo comportamiento se busca modelar, comprender o predecir. La naturaleza de esta variable es, quizás, el factor más determinante a la hora de elegir el tipo de modelo de regresión. Puede ser:\n\nContinua: Una variable que puede tomar cualquier valor dentro de un rango. Ej: temperatura, altura, precio de una acción, concentración de un compuesto químico.\nDiscreta de Conteo: Una variable que representa un número de eventos. Ej: número de accidentes en una intersección, número de clientes que entran en una tienda, número de mutaciones en un gen.\nBinaria o Dicotómica: Una variable con solo dos resultados posibles. Ej: éxito/fracaso, enfermo/sano, compra/no compra, spam/no spam.\nCategórica: Una variable que representa grupos o categorías. Si no tiene orden, es nominal (ej: tipo de sangre, partido político); si tiene un orden intrínseco, es ordinal (ej: nivel de satisfacción “bajo/medio/alto”, estadio de una enfermedad “I/II/III/IV”).\n\n\n\n1.2.2 Las variables predictoras\nConocidas indistintamente como variables independientes, explicativas, regresoras, covariables o características (features). Son las magnitudes, atributos o factores que se postula que influyen o están asociados con el comportamiento de la variable de respuesta. Al igual que la variable de respuesta, pueden ser de diversa naturaleza (continuas, categóricas, etc.). La selección de estas variables es una de las fases más críticas del modelado, requiriendo una combinación de conocimiento del dominio, análisis exploratorio de datos y técnicas estadísticas formales.\n\n\n1.2.3 El término de error aleatorio\nEste componente, a menudo subestimado, es conceptualmente crucial. Simboliza la variabilidad intrínseca de la variable de respuesta que no es capturada o explicada por las variables predictoras incluidas explícitamente en el modelo. El término de error \\(\\epsilon\\) no es un simple “error” en el sentido de equivocación; es un componente estocástico que amalgama múltiples fuentes de variabilidad:\n\nVariables Omitidas: Ningún modelo es perfecto. Siempre habrá factores que influyen en \\(Y\\) pero que no han sido medidos o incluidos en el modelo (variables latentes).\nError de Medición: Las mediciones de \\(Y\\) (y también de \\(X\\)) pueden no ser perfectamente precisas.\nAleatoriedad Intrínseca: Muchos fenómenos naturales y sociales tienen un componente de variabilidad irreducible. Dos individuos con idénticos valores en todas las variables predictoras pueden, aun así, tener valores distintos en la variable de respuesta.\n\nFormalmente, la relación fundamental de la regresión se expresa como la descomposición de la variable de respuesta en una parte sistemática y una parte aleatoria:\n\\[Y = \\underbrace{f(X_1, \\ldots, X_k)}_{\\text{Componente Sistemática}} + \\underbrace{\\epsilon}_{\\text{Componente Aleatoria}}\\]\ndonde \\(f(\\cdot)\\) denota la componente sistemática (o determinística) del modelo, que representa el valor esperado de \\(Y\\) para unos valores dados de las \\(X\\). La función \\(f\\) es lo que intentamos estimar a partir de los datos. Por su parte, \\(\\epsilon\\) es la componente aleatoria, y gran parte del diagnóstico y la inferencia en regresión se basa en verificar los supuestos que hacemos sobre la distribución de este término (ej: que su media es cero, que su varianza es constante, etc.).\n\n\n\n\n\n\nLinealidad en los parámetros, no en las variables\n\n\n\n\n\nUna característica que define a los modelos de regresión lineal (y que se extiende a muchos otros tipos de modelos) es que la función \\(f(\\cdot)\\) mantiene una relación lineal con respecto a sus parámetros desconocidos (los coeficientes beta, \\(\\beta_j\\)). Es crucial enfatizar que esta “linealidad en los parámetros” no impone una restricción de linealidad en las variables predictoras mismas.\nPor el contrario, es común y metodológicamente válido incorporar transformaciones no lineales de los predictores o interacciones complejas entre ellos para capturar relaciones más sofisticadas. Por ejemplo, el siguiente modelo es un modelo de regresión lineal:\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_1^2 + \\beta_3 \\log(X_2) + \\beta_4 (X_1 \\cdot X_2) + \\epsilon\\]\nAunque la relación entre \\(Y\\) y las variables \\(X_1\\) y \\(X_2\\) es claramente no lineal (es cuadrática en \\(X_1\\), logarítmica en \\(X_2\\) e incluye una interacción), el modelo es lineal en los parámetros \\(\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4\\). La función \\(f\\) es una combinación lineal de estos coeficientes. Esta flexibilidad es una de las razones de la enorme potencia de los modelos lineales.\nEl siguiente bloque de código en R genera un ejemplo visual. Simulamos datos que siguen una relación cuadrática y luego ajustamos un modelo lineal que incluye un término cuadrático (\\(X^2\\)). Como se puede observar en la figura, la línea de regresión (azul) captura perfectamente la curvatura de los datos, demostrando que un modelo lineal en sus parámetros puede modelar relaciones no lineales en sus variables.\n\n# Cargar la librería necesaria para la visualización\nlibrary(ggplot2)\n\n# 1. Simulación de datos\nset.seed(42) # Para reproducibilidad\nn &lt;- 100 # Número de observaciones\nx &lt;- runif(n, -5, 5)\n# La relación verdadera es cuadrática: y = 1.5 + 0.5*x + 0.8*x^2 + error\ny &lt;- 1.5 + 0.5 * x + 0.8 * x^2 + rnorm(n, mean = 0, sd = 5)\ndatos &lt;- data.frame(x, y)\n\n# 2. Ajuste del modelo lineal\n# Usamos I(x^2) para indicar que tratamos x^2 como una variable\nmodelo_cuadratico &lt;- lm(y ~ x + I(x^2), data = datos)\n\n# 3. Visualización con ggplot2\nggplot(datos, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6, color = \"gray40\") + # Puntos de los datos originales\n  geom_smooth(method = \"lm\", formula = y ~ x + I(x^2), se = FALSE, color = \"#0072B2\", size = 1.2) + # Línea del modelo ajustado\n  labs(\n    title = \"Modelo Lineal con Término Cuadrático\",\n    subtitle = \"Relación no lineal en la variable (curva) ajustada con un modelo lineal en los parámetros\",\n    x = \"Variable Predictora (X)\",\n    y = \"Variable de Respuesta (Y)\"\n  ) +\n  theme_classic(base_size = 14)\n\n\n\n\n\n\n\nFigura 1.1: Ejemplo de un modelo lineal en los parámetros que captura una relación no lineal (cuadrática) en los datos.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción a los modelos de regresión</span>"
    ]
  },
  {
    "objectID": "tema0.html#sec-tipos-modelos",
    "href": "tema0.html#sec-tipos-modelos",
    "title": "1  Introducción a los modelos de regresión",
    "section": "1.3 Un viaje preliminar por el universo de los modelos de regresión",
    "text": "1.3 Un viaje preliminar por el universo de los modelos de regresión\nLa regresión lineal clásica, que será el objeto de estudio de los primeros capítulos, es el punto de partida y la piedra angular sobre la cual se erige una prolífica y fascinante gama de metodologías estadísticas avanzadas. Este volumen se dedicará a desentrañar con rigor las siguientes extensiones y especializaciones, que permiten al analista abordar una variedad casi infinita de problemas.\n\n1.3.1 Modelos lineales (LMs)\nConstituyen el paradigma fundamental, el alfabeto sobre el que se escribe el lenguaje del modelado estadístico. Son mucho más que una simple técnica para ajustar una recta a una nube de puntos; son el laboratorio donde se forjan y se comprenden los conceptos esenciales que nos acompañarán durante todo nuestro viaje. Es aquí donde aprenderemos a:\n\nEstimar parámetros e interpretar su significado en el contexto del problema.\nCuantificar la incertidumbre de nuestras estimaciones mediante errores estándar e intervalos de confianza.\nRealizar contrastes de hipótesis para evaluar si la relación entre nuestras variables es estadísticamente significativa o fruto del azar.\nDiagnosticar la “salud” de un modelo, examinando si los supuestos sobre los que se construye son razonables para nuestros datos.\n\nEn su forma más clásica, el modelo lineal asume que la variable de respuesta (y, por consecuencia, el término de error aleatorio) sigue una distribución Normal o Gaussiana. Esta asunción es la clave que desbloquea todo el elegante aparato de la inferencia estadística, permitiéndonos realizar pruebas exactas y derivar propiedades matemáticas bien conocidas. Técnicas tan ubicuas en la ciencia como el Análisis de la Varianza (ANOVA) o el Análisis de la Covarianza (ANCOVA) no son más que casos particulares de la gran familia de los modelos lineales, un hecho que unifica campos de la estadística que históricamente se estudiaban por separado. Dominar los LMs es, sencillamente, un requisito indispensable.\n\n\n1.3.2 Modelos lineales generalizados (GLMs)\nSi los LMs son el alfabeto, los GLMs son la gramática que nos permite construir frases complejas y con significado en una variedad de contextos mucho más amplia. Introducidos en el influyente y verdaderamente revolucionario trabajo de (Nelder y Wedderburn 1972), los GLMs representan un salto conceptual que expande de forma masiva el universo de problemas que podemos abordar. Suponen una generalización elegante que nos permite escapar de la “tiranía” de la distribución Normal y modelar respuestas con una variedad mucho más amplia de naturalezas y escalas.\nEsta flexibilidad se logra mediante la combinación de dos ingeniosos mecanismos que son el corazón de la teoría:\n\nLa familia exponencial de distribuciones: Los GLMs no funcionan con cualquier distribución, sino con aquellas que pertenecen a una “familia” matemática con propiedades muy convenientes: la familia exponencial. Este “club” de distribuciones es muy selecto, pero incluye a miembros tan importantes como la Normal, la Poisson (para datos de conteo), la Binomial (para datos de proporciones o binarios), la Gamma (para datos continuos positivos y asimétricos) o la Binomial Negativa. Su estructura matemática común permite desarrollar una teoría unificada para la estimación de parámetros, lo que es un logro teórico de primer orden.\nLa función de enlace (link function): Este es el verdadero golpe de genialidad. El predictor lineal de nuestro modelo, \\(\\boldsymbol{X\\beta}\\), puede tomar cualquier valor en la recta real, desde \\(-\\infty\\) hasta \\(+\\infty\\). Sin embargo, la media de nuestra variable de respuesta, \\(E[Y] = \\mu\\), a menudo está restringida. Por ejemplo, una probabilidad (\\(\\mu\\) en un modelo binomial) debe estar entre 0 y 1; un conteo (\\(\\mu\\) en un modelo de Poisson) debe ser positivo. La función de enlace, \\(g(\\cdot)\\), actúa como un “traductor” o un “puente” que conecta estos dos mundos. Transforma la media restringida de la respuesta para que pueda ser modelada por el predictor lineal no restringido. La relación fundamental es, por tanto, \\(g(E[Y]) = g(\\mu) = \\boldsymbol{X\\beta}\\).\n\nPara datos de conteo (Poisson), se usa un enlace logarítmico (\\(g(\\mu) = \\log(\\mu)\\)). Esto garantiza que, al invertir la función para obtener la media (\\(\\mu = \\exp(\\boldsymbol{X\\beta})\\)), el resultado será siempre positivo, como debe ser un conteo.\nPara datos binarios (Binomial), se usa un enlace logit (\\(g(\\mu) = \\log(\\frac{\\mu}{1-\\mu})\\)). Esta función toma una probabilidad \\(\\mu\\) en el rango (0, 1) y la proyecta sobre toda la recta real, permitiendo que sea modelada por \\(\\boldsymbol{X\\beta}\\).\n\n\nGracias a los GLMs, podemos usar el mismo marco conceptual de la regresión lineal para modelar una gama de fenómenos increíblemente diversa, desde predecir la cantidad de ciclistas en una ciudad (Poisson) hasta la probabilidad de que un paciente responda a un tratamiento (logística).\n\n\n1.3.3 Modelos de efectos mixtos (Mixed Models)\nSu desarrollo responde a la necesidad crítica de analizar datos que exhiben estructuras de dependencia o correlación, como agrupamientos, anidamientos o jerarquías. En datos estándar, asumimos que las observaciones son independientes, pero esta asunción se viola en casos como: * Medidas repetidas sobre los mismos sujetos (ej: medir la presión arterial de un paciente cada mes). * Datos longitudinales (un tipo de medida repetida a lo largo del tiempo). * Datos agrupados (ej: estudiantes anidados dentro de clases, que a su vez están anidadas dentro de colegios). Estos modelos, detallados en obras como la de (Pinheiro y Bates 2000), introducen explícitamente una estructura de correlación en el término de error mediante la incorporación de efectos aleatorios, que permiten capturar la variabilidad entre los diferentes grupos o individuos, además de los efectos fijos que representan a la población general.\n\n\n1.3.4 Modelos aditivos generalizados (GAMs)\nRepresentan una extensión natural y altamente flexible de los GLMs que relaja el supuesto de linealidad entre el predictor transformado y las covariables. Los GAMs, cuya implementación moderna se debe en gran parte al trabajo de (Wood 2017), permiten modelar estas relaciones mediante funciones suaves no paramétricas (como splines), manteniendo al mismo tiempo la estructura aditiva del modelo. La forma general es \\(g(\\mu) = \\alpha + f_1(x_1) + f_2(x_2) + \\ldots + f_p(x_p)\\), donde las \\(f_i(\\cdot)\\) son funciones suaves de los predictores estimadas a partir de los datos. Esto permite capturar patrones no lineales complejos sin necesidad de especificar una forma funcional paramétrica a priori, logrando un equilibrio excepcional entre flexibilidad e interpretabilidad.\n\n\n\n\n\n\nR como lenguaje del modelado estadístico\n\n\n\n\n\nEste compendio no es un texto puramente teórico. Fusiona intrínsecamente la exposición de los conceptos con su aplicación computacional directa a través del lenguaje y entorno estadístico R. R se ha consolidado como el estándar de facto en la investigación estadística y la ciencia de datos académica por su potencia, flexibilidad y el inmenso ecosistema de paquetes contribuidos por la comunidad científica. Se presupone en el lector una familiaridad operativa básica con R, y se fomenta activamente el desarrollo de una fluidez progresiva mediante la reproducción, modificación y experimentación con los numerosos ejemplos y fragmentos de código presentados.\nLa capacidad de ejecutar análisis en R es fundamental para todo el ciclo de vida del modelado: - La exploración de datos y la visualización inicial. - La estimación de parámetros y el ajuste de los modelos. - El diagnóstico riguroso de la adecuación del modelo y la validación de sus supuestos. - La producción de gráficos y tablas de alta calidad para comunicar los resultados.\nEn R, las herramientas fundamentales para la regresión lineal (lm()) y los modelos lineales generalizados (glm()) están incluidas en el paquete stats, que es uno de los paquetes base y se carga automáticamente con cada sesión. Por lo tanto, no necesitamos instalarlo ni cargarlo.\nA lo largo del libro, extenderemos esta funcionalidad base con paquetes especializados que sí requieren instalación y carga. Entre los más importantes que usaremos se encuentran:\n\nmgcv: La implementación de referencia para GAMs, mantenida por su creador, Simon Wood, y citada en (Wood 2017).\nlme4 y nlme: Los dos paquetes fundamentales para el ajuste de modelos de efectos mixtos, desarrollados por los pioneros en el campo (Pinheiro y Bates 2000; Bates et al. 2015).\nrms: Un paquete y una filosofía de trabajo para implementar estrategias de modelado de regresión robustas, como se detalla en la obra de (Harrell 2015).\ngamair: Contiene numerosos conjuntos de datos que acompañan al libro de (Wood 2017), ideales para practicar con GAMs.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción a los modelos de regresión</span>"
    ]
  },
  {
    "objectID": "tema0.html#sec-papel-r",
    "href": "tema0.html#sec-papel-r",
    "title": "1  Introducción a los modelos de regresión",
    "section": "1.4 R como lenguaje del modelado estadístico",
    "text": "1.4 R como lenguaje del modelado estadístico\nEste compendio no es un texto puramente teórico. Fusiona intrínsecamente la exposición de los conceptos con su aplicación computacional directa a través del lenguaje y entorno estadístico R. R se ha consolidado como el estándar de facto en la investigación estadística y la ciencia de datos académica por su potencia, flexibilidad y el inmenso ecosistema de paquetes contribuidos por la comunidad científica. Se presupone en el lector una familiaridad operativa básica con R, y se fomenta activamente el desarrollo de una fluidez progresiva mediante la reproducción, modificación y experimentación con los numerosos ejemplos y fragmentos de código presentados.\nLa capacidad de ejecutar análisis en R es fundamental para todo el ciclo de vida del modelado: - La exploración de datos y la visualización inicial. - La estimación de parámetros y el ajuste de los modelos. - El diagnóstico riguroso de la adecuación del modelo y la validación de sus supuestos. - La producción de gráficos y tablas de alta calidad para comunicar los resultados.\n\n\n\n\n\n\nFunciones y paquetes esenciales\n\n\n\n\n\nEn R, las herramientas fundamentales para la regresión lineal (lm()) y los modelos lineales generalizados (glm()) están incluidas en el paquete stats, que es uno de los paquetes base y se carga automáticamente con cada sesión. Por lo tanto, no necesitamos instalarlo ni cargarlo.\nA lo largo del libro, extenderemos esta funcionalidad base con paquetes especializados que sí requieren instalación y carga. Entre los más importantes que usaremos se encuentran:\n\nmgcv: La implementación de referencia para GAMs, mantenida por su creador, Simon Wood, y citada en Wood (2017).\nlme4 y nlme: Los dos paquetes fundamentales para el ajuste de modelos de efectos mixtos, desarrollados por los pioneros en el campo (Pinheiro y Bates 2000; Bates et al. 2015).\nrms: Un paquete y una filosofía de trabajo para implementar estrategias de modelado de regresión robustas, como se detalla en la obra de Harrell (2015).\ngamair: Contiene numerosos conjuntos de datos que acompañan al libro de Wood (2017), ideales para practicar con GAMs.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción a los modelos de regresión</span>"
    ]
  },
  {
    "objectID": "tema0.html#sec-perfil-lector",
    "href": "tema0.html#sec-perfil-lector",
    "title": "1  Introducción a los modelos de regresión",
    "section": "1.4 Filosofía pedagógica del volumen",
    "text": "1.4 Filosofía pedagógica del volumen\nLa filosofía que subyace a la obra es un enfoque “teórico-práctico” deliberado y sin concesiones. No nos conformamos con una mera aplicación de “recetas” o una guía de funciones de software. Buscamos fomentar una comprensión profunda del modus operandi de cada modelo y método. Perseguimos un equilibrio entre la técnica estadística y la estrategia de resolución de problemas, bajo la firme convicción de que la labor práctica se desarrolla con mayor fluidez, creatividad y éxito cuando se cimienta en una comprensión robusta de los principios matemáticos y estadísticos subyacentes, tal y como defiende Harrell (2015) en su influyente obra.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción a los modelos de regresión</span>"
    ]
  },
  {
    "objectID": "tema0.html#sec-historia",
    "href": "tema0.html#sec-historia",
    "title": "1  Introducción a los modelos de regresión",
    "section": "1.4 Una breve crónica del desarrollo de la regresión",
    "text": "1.4 Una breve crónica del desarrollo de la regresión\n\n1.4.1 Los orígenes: Galton y la “regresión a la mediocridad”\nLa gestación de la metodología de regresión se traza hasta las investigaciones pioneras de Sir Francis Galton, un polímata de la era victoriana. A finales del siglo XIX, estudiando la herencia de la estatura, Galton recopiló datos de padres e hijos y notó un fenómeno curioso: los padres muy altos tendían a tener hijos altos, pero, en promedio, no tan altos como ellos. Análogamente, los padres muy bajos tenían hijos bajos, pero no tan bajos como ellos. Acuñó el término “regresión a la mediocridad” (hoy diríamos “regresión a la media”) para describir esta tendencia de las características de la descendencia a “regresar” hacia la media de la población, en lugar de perpetuar los extremos de los progenitores (Galton 1886).\n\n\n\n\n\n\nEstudios de Galton sobre estatura\n\n\n\n\n\nDatos recopilados\n\nGalton recopiló datos sobre las estaturas de 928 hijos y sus respectivos padres.\nLas medidas fueron expresadas en pulgadas (1 pulgada = 2.54 cm).\n\nEn sus análisis, utilizó el promedio de las estaturas de ambos padres, conocido como estatura media parental, para compararlo con la estatura de los hijos.\n\nPrincipales hallazgos\n\nRelación lineal entre padres e hijos:\nGalton observó que existe una relación positiva entre la estatura de los padres y la de los hijos. Los padres altos tienden a tener hijos altos, y los padres bajos tienden a tener hijos bajos. Esta relación puede modelarse con una línea recta, lo que inspiró la formulación de la regresión lineal.\nRegresión a la media:\n\nAunque los hijos de padres altos son, en promedio, más altos que el promedio general de la población, también tienden a ser menos altos que sus padres.\n\nDe manera similar, los hijos de padres bajos son más bajos que el promedio general, pero suelen ser menos bajos que sus padres.\n\nEste fenómeno, que Galton llamó “regresión a la media”, ocurre porque las características extremas tienden a suavizarse en la siguiente generación debido a la influencia de múltiples factores genéticos y ambientales.\n\nEcuación de la recta de regresión:\nGalton ajustó una recta para describir la relación entre la estatura media parental (\\(X\\)) y la estatura de los hijos (\\(Y\\)): \\[\nY = \\beta_0 + \\beta_1 X\n\\] Donde:\n\n\\(\\beta_0\\): Intercepto, representa la estatura promedio de los hijos cuando la estatura parental es promedio.\n\\(\\beta_1\\): Pendiente, indica cómo cambia la estatura de los hijos por cada unidad de cambio en la estatura media parental.\n\n\nImportancia en la Estadística\n\nRegresión lineal:\nEste estudio introdujo el concepto de recta de regresión, que describe cómo varía la media de una variable dependiente en función de una variable independiente.\nCorrelación:\nGalton también estudió el grado de relación entre variables, precursor del concepto de coeficiente de correlación desarrollado posteriormente por Karl Pearson, un discípulo suyo.\nRegresión a la media:\nEl término y la idea detrás de “regresión a la media” surgieron de estos estudios y son hoy fundamentales en estadística y genética.\n\nEjemplo Gráfico\nGalton representó sus datos en gráficos de dispersión, mostrando cómo los puntos (pares de estatura media parental y estatura de los hijos) se agrupan alrededor de la recta de regresión, ilustrando la tendencia general de la relación.\n\n# Cargar los paquetes necesarios\nlibrary(ggplot2)\nlibrary(HistData)\n\n# Cargar los datos de Galton\ndata(\"GaltonFamilies\")\n\n# Crear el modelo de regresión lineal para obtener los coeficientes\nmodelo &lt;- lm(childHeight ~ midparentHeight, data = GaltonFamilies)\n\n# Crear la etiqueta para la ecuación de la recta de forma más limpia\n# Usamos sprintf() para un formato más controlado y legible\neq_label &lt;- sprintf(\"y = %.2f + %.2f * x\", coef(modelo)[1], coef(modelo)[2])\n\n# --- Gráfico Mejorado ---\n# Usamos un tema más limpio y colores más suaves para una apariencia profesional.\n# geom_jitter() es mejor que geom_point() para estos datos, ya que evita la superposición de puntos.\nggplot(GaltonFamilies, aes(x = midparentHeight, y = childHeight)) +\n  \n  # 1. Puntos de datos: Usamos geom_jitter para visualizar mejor los puntos superpuestos\n  #    y añadimos transparencia (alpha) para ver la densidad.\n  geom_jitter(alpha = 0.3, color = \"gray50\", width = 0.1, height = 0.1) +\n  \n  # 2. Línea de regresión: En un color azul profesional y más gruesa para que destaque.\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#0072B2\", size = 1.2) +\n  \n  # 3. Anotación: Añadimos la ecuación de la recta de forma elegante,\n  #    usando el mismo color que la línea para crear cohesión visual.\n  annotate(\n    \"text\",\n    x = 66, y = 74, # Posición ajustada para mejor visibilidad\n    label = eq_label,\n    color = \"#0072B2\", # Mismo color que la línea\n    size = 4.5, # Tamaño de la fuente\n    fontface = \"italic\" # Cursiva para la ecuación\n  ) +\n  \n  # 4. Títulos y etiquetas: Mejorados para mayor claridad y contexto.\n  #    Añadimos un subtítulo y una fuente.\n  labs(\n    title = \"Regresión de la Estatura de Hijos vs. Padres\",\n    subtitle = \"Datos históricos del estudio de Sir Francis Galton sobre la 'regresión a la media'\",\n    x = \"Promedio de Estatura de los Padres (pulgadas)\",\n    y = \"Estatura del Hijo/a (pulgadas)\",\n    caption = \"Fuente: Paquete HistData de R\"\n  ) +\n  \n  # 5. Tema: Usamos un tema limpio y profesional como base.\n  theme_classic(base_size = 14)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nDatos de Galton sobre la estatura de padres e hijos\n\n\n\n\n\n\n\n\n\n1.4.2 La formalización matemática: Legendre y Gauss\nAunque Galton sentó las bases conceptuales e introdujo el término, la formalización matemática de la estimación de parámetros en modelos lineales se atribuye a dos de los más grandes matemáticos de la historia. Adrien-Marie Legendre publicó en 1805 el “Método de los mínimos cuadrados” como un procedimiento numérico para ajustar observaciones astronómicas. Pocos años después, Carl Friedrich Gauss no solo publicó que había desarrollado el mismo método de forma independiente años antes, sino que lo dotó de una profundidad teórica mucho mayor, conectándolo con la teoría de la probabilidad y derivándolo bajo el supuesto de errores distribuidos normalmente, convirtiéndolo en la técnica fundamental para la estimación en modelos lineales que sigue siendo hoy.\n\n\n1.4.3 El desarrollo moderno: la revolución de los GLMs\nA lo largo del siglo XX, la regresión experimentó un desarrollo explosivo. Sin embargo, el hito que probablemente más ha influido en la práctica estadística moderna fue la publicación del artículo sobre Modelos Lineales Generalizados (GLMs) por John Nelder y Robert Wedderburn en 1972 (Nelder y Wedderburn 1972). Esta obra seminal fue revolucionaria porque unificó bajo un mismo paraguas conceptual y computacional diversas clases de modelos que hasta entonces se trataban por separado: la regresión lineal para datos normales, la regresión logística para datos binarios y la regresión de Poisson para datos de conteo. Esto estimuló enormemente el desarrollo de software y la aplicación del modelado estadístico a una nueva y vasta gama de problemas.\n\n\n1.4.4 La evolución contemporánea\nEste legado continúa evolucionando a un ritmo vertiginoso, con la inclusión de modelos jerárquicos y bayesianos, métodos no paramétricos y de machine learning como los árboles de regresión, y la adaptación de la regresión al análisis de datos masivos (big data). La regresión ha evolucionado desde una observación sobre la herencia biológica hasta convertirse en una de las herramientas más versátiles y poderosas del arsenal analítico moderno.\n\n\n\n\nBates, Douglas, Martin Mächler, Ben Bolker, y Steve Walker. 2015. «Fitting Linear Mixed-Effects Models Using lme4». Journal of Statistical Software 67 (1): 1-48.\n\n\nGalton, Francis. 1886. «Regression towards mediocrity in hereditary stature». The Journal of the Anthropological Institute of Great Britain and Ireland 15: 246-63.\n\n\nHarrell, Frank E., Jr. 2015. Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis. Second. Springer.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, y Robert Tibshirani. 2021. An Introduction to Statistical Learning with Applications in R. Second. Springer.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, y William Li. 2005. Applied linear statistical models. McGraw-hill.\n\n\nNelder, John Ashworth, y Robert WM Wedderburn. 1972. «Generalized linear models». Journal of the Royal Statistical Society Series A: Statistics in Society 135 (3): 370-84.\n\n\nPinheiro, José C., y Douglas M. Bates. 2000. Mixed-Effects Models in S and S-PLUS. New York: Springer.\n\n\nShmueli, Galit. 2010. «To Explain or to Predict?» Statistical Science 25 (3): 289-310.\n\n\nWood, Simon N. 2017. Generalized Additive Models: An Introduction with R. Second. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción a los modelos de regresión</span>"
    ]
  },
  {
    "objectID": "tema1.html#el-modelo-de-regresión-lineal-simple",
    "href": "tema1.html#el-modelo-de-regresión-lineal-simple",
    "title": "2  El modelo de regresión lineal simple",
    "section": "2.3 El modelo de regresión lineal simple",
    "text": "2.3 El modelo de regresión lineal simple\nPara poder modelar la relación entre dos variables, primero debemos definir el lenguaje matemático que usaremos. El modelo de regresión lineal simple, a pesar de su nombre, es una herramienta de una profundidad y elegancia notables. En esta sección, desgranaremos su estructura formal y, lo que es más importante, estableceremos las “reglas del juego”: los supuestos teóricos que deben cumplirse para que nuestras conclusiones sean válidas y fiables.\n\n2.3.1 Formulación matemática del modelo\nEn el corazón de la estadística yace la distinción entre la población, que representa la verdad teórica y completa que deseamos entender, y la muestra, que es el conjunto de datos finito con el que realmente trabajamos. El modelo de regresión lineal se formula en estos dos niveles.\nEl modelo poblacional postula que la relación verdadera entre una variable respuesta \\(Y\\) y una predictora \\(X\\) sigue una línea recta, aunque contaminada por cierta aleatoriedad. Para cualquier individuo \\(i\\) de la población, esta relación se describe como:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\n\\]\nEn esta ecuación, \\(\\beta_0\\) (el intercepto) y \\(\\beta_1\\) (la pendiente) son los parámetros poblacionales: valores fijos pero desconocidos que definen la “verdadera” recta que relaciona \\(X\\) e \\(Y\\). El término \\(\\varepsilon_i\\) es el error aleatorio, una variable aleatoria que captura todo aquello que el modelo no puede explicar, desde la influencia de otras variables hasta la variabilidad intrínseca del fenómeno.\nComo nunca observamos la población entera, nuestro trabajo consiste en usar una muestra para estimar esos parámetros desconocidos. Esto nos lleva al modelo muestral, que es la aproximación que construimos a partir de nuestros datos:\n\\[\n\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\n\\]\nAquí, los “gorros” (\\(\\hat{\\cdot}\\)) denotan estimaciones. \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) son nuestros mejores intentos de adivinar los verdaderos valores de \\(\\beta_0\\) y \\(\\beta_1\\). De igual modo, \\(\\hat{Y}_i\\) es el valor que nuestro modelo predice para la observación \\(i\\). La diferencia entre el valor real y el predicho, \\(e_i = Y_i - \\hat{Y}_i\\), se conoce como residuo, y es la manifestación observable en nuestra muestra del error teórico e inobservable \\(\\varepsilon_i\\).\n\n\n2.3.2 Los supuestos del modelo lineal clásico\nPara que el puente entre nuestro modelo muestral y la realidad poblacional sea sólido, debemos asumir que los errores teóricos \\(\\varepsilon_i\\) se comportan de una manera predecible y ordenada. Estas “reglas de comportamiento” son los supuestos del modelo lineal clásico, también conocidos como los supuestos de Gauss-Markov. Son la base que legitima todo el proceso de estimación e inferencia.\nEl primer supuesto es la linealidad. Asumimos que la relación entre la variable predictora \\(X\\) y el valor esperado de la respuesta \\(Y\\) es, en promedio, una línea recta. Esto no exige que cada punto se alinee perfectamente, sino que la tendencia central de los datos siga un patrón lineal. Si la verdadera relación es curva, un modelo lineal estará sistemáticamente equivocado, como intentar describir una parábola con una regla.\nEl segundo supuesto es la independencia de los errores. Esto significa que el error de una observación, \\(\\varepsilon_i\\), no está correlacionado con el error de ninguna otra, \\(\\varepsilon_j\\). Cada observación debe aportar información nueva e independiente. Este supuesto es fácil de violar en datos con estructura, como las series temporales, donde un valor de hoy a menudo depende del de ayer. Si los errores están correlacionados, nuestros cálculos de incertidumbre (errores estándar, p-valores) serán incorrectos, llevándonos a una falsa sensación de precisión y a detectar relaciones significativas donde solo hay ruido estructurado.\nEl tercer supuesto es la homocedasticidad, o varianza constante de los errores. Intuitivamente, esto implica que la dispersión de los datos alrededor de la recta de regresión es uniforme a lo largo de toda la recta. La “nube” de puntos no debe ensancharse ni estrecharse en un patrón de embudo. Esta condición es fundamental para la eficiencia de nuestras estimaciones. Cuando se viola y aparece la heterocedasticidad (varianza no constante), los estimadores de los coeficientes siguen siendo insesgados, pero dejan de ser los “mejores” en el sentido de tener la mínima varianza. Más grave aún, las fórmulas estándar para los errores estándar se vuelven inválidas, comprometiendo toda la inferencia estadística.\nFinalmente, para que nuestras pruebas de hipótesis e intervalos de confianza sean exactos, especialmente en muestras pequeñas, añadimos el supuesto de normalidad de los errores. Asumimos que los errores \\(\\varepsilon_i\\) siguen una distribución Normal con media cero y una varianza constante \\(\\sigma^2\\). Es crucial entender que este supuesto es para la inferencia, no para la estimación. El Teorema de Gauss-Markov, que garantiza las buenas propiedades de nuestros estimadores, no depende de la normalidad. Sin embargo, sin ella, no podemos estar seguros de que nuestros estadísticos de prueba sigan las distribuciones teóricas (t-Student, F) que usamos para calcular p-valores. Afortunadamente, gracias al Teorema del Límite Central, para muestras grandes este supuesto es menos crítico.\n\n\n\n\n\n\nEn resumen: la imagen completa\n\n\n\nEl modelo de regresión lineal simple postula que, para cada valor de \\(X\\), existe una subpoblación de valores de \\(Y\\). La media de cada una de estas subpoblaciones se encuentra sobre la recta de regresión poblacional \\(E[Y] = \\beta_0 + \\beta_1 X\\). Además, cada una de estas subpoblaciones sigue una distribución Normal y todas ellas comparten la misma varianza \\(\\sigma^2\\).\n\n\nEntendido. Mis disculpas. Aquí tienes el código Markdown en crudo, sin ningún tipo de formato ni compilación, tal y como lo pides.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema1.html#estimación-de-los-parámetros-el-método-de-mínimos-cuadrados-ordinarios-mco",
    "href": "tema1.html#estimación-de-los-parámetros-el-método-de-mínimos-cuadrados-ordinarios-mco",
    "title": "2  El modelo de regresión lineal simple",
    "section": "2.4 Estimación de los parámetros: el método de mínimos cuadrados ordinarios (MCO)",
    "text": "2.4 Estimación de los parámetros: el método de mínimos cuadrados ordinarios (MCO)\nUna vez hemos postulado que la relación entre nuestras variables \\(X\\) e \\(Y\\) puede ser representada por una recta poblacional, nos enfrentamos a la pregunta fundamental: de las infinitas rectas que podemos dibujar a través de nuestra nube de puntos muestral, ¿cuál es la “mejor”? ¿Qué criterio objetivo nos permite seleccionar una única recta que represente de la forma más fiel posible los datos que hemos observado? El Método de Mínimos Cuadrados Ordinarios (MCO), o Ordinary Least Squares (OLS) en inglés, nos da una respuesta elegante y matemáticamente robusta a esta pregunta.\n\n2.4.1 La intuición: minimizar la suma de los errores al cuadrado\nImaginemos una recta candidata cualquiera que atraviesa nuestros datos. Para cada punto \\((x_i, y_i)\\), podemos medir la distancia vertical entre el valor observado \\(y_i\\) y el valor que la recta predice para \\(x_i\\), que denotamos como \\(\\hat{y}_i\\). Esta diferencia, \\(e_i = y_i - \\hat{y}_i\\), es lo que llamamos residuo.\nAlgunos de estos residuos serán positivos (el punto está por encima de la recta) y otros negativos (el punto está por debajo). Una idea ingenua sería buscar la recta que haga la suma de todos los residuos igual a cero, pero existen infinitas rectas que cumplen esto. Para evitar que los errores positivos y negativos se cancelen, MCO propone una solución más inteligente: elevar cada residuo al cuadrado y buscar la recta que minimice la Suma de los Cuadrados de los Errores (SCE).\n\\[\n\\text{SCE}(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} e_i^2 = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_i))^2\n\\]\nGeométricamente, estamos buscando la recta que hace que el área total de todos los cuadrados construidos sobre los residuos sea lo más pequeña posible.\n\n\n2.4.2 La derivación matemática\nPara encontrar los valores de \\(\\beta_0\\) y \\(\\beta_1\\) que minimizan esta función SCE, recurrimos al cálculo. Tratamos la SCE como una función de dos variables (\\(\\beta_0\\) y \\(\\beta_1\\)) y calculamos sus derivadas parciales, igualándolas a cero para encontrar el mínimo.\n\\[\n\\frac{\\partial \\text{SCE}}{\\partial \\beta_0} = -2 \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i) = 0\n\\]\n\\[\n\\frac{\\partial \\text{SCE}}{\\partial \\beta_1} = -2 \\sum_{i=1}^{n} x_i (y_i - \\beta_0 - \\beta_1 x_i) = 0\n\\]\nLa resolución de este sistema de dos ecuaciones (conocidas como las ecuaciones normales) nos proporciona las fórmulas para los estimadores de MCO, que denotamos con un “gorro” (\\(\\hat{\\cdot}\\)):\n\\[\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)}\n\\]\n\\[\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n\\]\nEs importante destacar que, bajo los supuestos del modelo lineal clásico, el Teorema de Gauss-Markov demuestra que estos estimadores MCO son los Mejores Estimadores Lineales Insesgados (MELI), o Best Linear Unbiased Estimators (BLUE) en inglés. Esto significa que, de entre todos los posibles estimadores insesgados que son una combinación lineal de los datos, los de MCO son los que tienen la menor varianza.\n\n\n2.4.3 Aplicación en R\nEn la práctica, no necesitamos calcular estas fórmulas a mano. R nos proporciona la función lm() (linear model) para estimar los parámetros de forma instantánea.\n\n\n\n\n\n\nEstimación con R\n\n\n\nContinuando con nuestro ejemplo de las calificaciones y las horas de estudio, así estimaríamos los coeficientes del modelo:\n\n# Suponiendo que el dataframe 'datos' ya está creado\n# Ajustamos el modelo lineal\nmodelo_estudio &lt;- lm(Calificaciones ~ Tiempo_Estudio, data = datos)\n\n# Extraemos los coeficientes estimados\ncoeficientes &lt;- coef(modelo_estudio)\n\n# Imprimimos los resultados\nprint(coeficientes)\n\n   (Intercept) Tiempo_Estudio \n    5.00117598     0.09874923 \n\n# Interpretación:\n# El intercepto (β̂₀) es aprox. 5.04.\n# La pendiente (β̂₁) es aprox. 0.10.\n# La recta de regresión muestral es: Calificaciones = 5.04 + 0.10 * Tiempo_Estudio\n\n\n\n\n\n\n\nDraper, NR. 1998. Applied regression analysis. McGraw-Hill. Inc.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema1.html#exploración-inicial-visualización-y-correlación",
    "href": "tema1.html#exploración-inicial-visualización-y-correlación",
    "title": "2  El modelo de regresión lineal simple",
    "section": "",
    "text": "Ejemplo práctico: Horas de estudio vs. Calificaciones\n\n\n\nVamos a plantear un problema que nos acompañará durante todo el capítulo: queremos saber si el tiempo de estudio semanal influye en las calificaciones finales.\nPrimero, simulamos los datos y los visualizamos con un gráfico de dispersión. Luego, calculamos su correlación.\n\nlibrary(ggplot2)\nset.seed(123) # Para reproducibilidad\n\n# Simulación de datos\ndatos &lt;- data.frame(\n  Tiempo_Estudio = round(runif(100, min = 5, max = 40), 1)\n)\ndatos$Calificaciones &lt;- round(5 + 0.1 * datos$Tiempo_Estudio + rnorm(100, mean = 0, sd = 0.5), 2)\n\n# Visualización con ggplot2\nggplot(datos, aes(x = Tiempo_Estudio, y = Calificaciones)) +\n  geom_point(color = \"#0072B2\", alpha = 0.7) +\n  labs(\n    title = \"Relación entre Tiempo de Estudio y Calificaciones\",\n    x = \"Tiempo de Estudio (horas/semana)\",\n    y = \"Calificaciones (promedio)\"\n  ) +\n  theme_minimal(base_size = 14)\n\n# Cálculo de la correlación\ncorrelacion &lt;- cor(datos$Tiempo_Estudio, datos$Calificaciones)\nprint(paste(\"El coeficiente de correlación de Pearson es:\", round(correlacion, 4)))\n\n[1] \"El coeficiente de correlación de Pearson es: 0.8983\"\n\n\n\n\n\n\n\n\nFigura 2.1: Relación entre tiempo de estudio y calificaciones.\n\n\n\n\n\nEl gráfico muestra una clara tendencia lineal positiva, y la correlación de 0.9 confirma que la asociación es fuerte. Esta evidencia visual y numérica nos da luz verde para proponer un modelo de regresión lineal.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema1.html#formulación-y-supuestos-del-modelo",
    "href": "tema1.html#formulación-y-supuestos-del-modelo",
    "title": "2  El modelo de regresión lineal simple",
    "section": "2.2 Formulación y supuestos del modelo",
    "text": "2.2 Formulación y supuestos del modelo\nUna vez que la exploración inicial sugiere una relación lineal, el siguiente paso es formalizarla matemáticamente. Aquí es donde definimos la estructura teórica del modelo y los supuestos bajo los cuales operará.\n\n2.2.1 Formulación matemática del modelo\nEn el corazón de la estadística yace la distinción entre la población (la verdad teórica que deseamos entender) y la muestra (el conjunto de datos con el que trabajamos). El modelo de regresión se formula en estos dos niveles.\nEl modelo poblacional postula que la relación verdadera entre una variable respuesta \\(Y\\) y una predictora \\(X\\) sigue una línea recta, aunque contaminada por cierta aleatoriedad. Para cualquier individuo \\(i\\) de la población, esta relación se describe como:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\n\\]\nEn esta ecuación, \\(\\beta_0\\) y \\(\\beta_1\\) son los parámetros poblacionales (el intercepto y la pendiente verdaderos pero desconocidos), y \\(\\varepsilon_i\\) es el error aleatorio, que captura todo aquello que el modelo no puede explicar.\nComo nunca observamos la población entera, nuestro trabajo consiste en usar una muestra para estimar el modelo muestral:\n\\[\n\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\n\\]\nAquí, los “gorros” (\\(\\hat{\\cdot}\\)) denotan estimaciones calculadas a partir de la muestra. La diferencia entre el valor real y el predicho, \\(e_i = Y_i - \\hat{Y}_i\\), se conoce como residuo.\n\n\n2.2.2 Los supuestos del modelo lineal clásico\nPara que el puente entre nuestro modelo muestral y la realidad poblacional sea sólido, debemos asumir que los errores teóricos \\(\\varepsilon_i\\) se comportan de una manera predecible y ordenada. Estas “reglas de comportamiento” son los supuestos del modelo lineal clásico (o de Gauss-Markov).\n\nLinealidad: La relación entre \\(X\\) y el valor esperado de \\(Y\\) es, en promedio, una línea recta.\nIndependencia de los errores: El error de una observación no está correlacionado con el error de ninguna otra.\nHomocedasticidad: La varianza del error es constante para todos los valores de \\(X\\).\nNormalidad de los errores: Para la inferencia, se asume que los errores siguen una distribución Normal con media cero y varianza constante.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema1.html#estimación-de-los-parámetros-el-método-de-mínimos-cuadrados",
    "href": "tema1.html#estimación-de-los-parámetros-el-método-de-mínimos-cuadrados",
    "title": "2  El modelo de regresión lineal simple",
    "section": "2.3 Estimación de los parámetros: el método de mínimos cuadrados",
    "text": "2.3 Estimación de los parámetros: el método de mínimos cuadrados\nAhora que hemos visto una relación lineal en los datos y hemos definido un modelo matemático para describirla, necesitamos un método para encontrar la “mejor” recta de ajuste. El Método de Mínimos Cuadrados Ordinarios (MCO) nos proporciona este criterio.\n\n2.3.1 La intuición: minimizar la suma de los errores al cuadrado\nLa idea es encontrar la recta que minimice la suma de las distancias verticales al cuadrado entre cada punto y la propia recta. Estas distancias son los residuos (\\(e_i\\)). Al elevarlos al cuadrado, nos aseguramos de que los errores positivos y negativos no se cancelen y penalizamos más los errores grandes. Buscamos los valores de \\(\\beta_0\\) y \\(\\beta_1\\) que minimizan la Suma de los Cuadrados de los Errores (SCE):\n\\[\n\\text{SCE}(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} e_i^2 = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_i))^2\n\\]\n\n\n2.3.2 La derivación matemática y propiedades\nUtilizando cálculo diferencial, se puede demostrar que los valores de \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) que minimizan esta función son:\n\\[\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)} \\quad \\quad \\quad \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n\\]\nEs importante destacar que, bajo los supuestos del modelo, el Teorema de Gauss-Markov demuestra que estos estimadores son los Mejores Estimadores Lineales Insesgados (MELI / BLUE), es decir, los más eficientes.\n\n\n2.3.3 Aplicación en R\nEn la práctica, R calcula estos estimadores por nosotros con la función lm().\n\n\n\n\n\n\nEstimación con R\n\n\n\nAjustamos el modelo a nuestros datos de estudio:\n\n# Ajustamos el modelo lineal\nmodelo_estudio &lt;- lm(Calificaciones ~ Tiempo_Estudio, data = datos)\n\n# Extraemos los coeficientes estimados\nprint(coef(modelo_estudio))\n\n   (Intercept) Tiempo_Estudio \n    5.00117598     0.09874923 \n\n\nLa recta de regresión muestral es: Calificaciones = 5 + 0.1 * Tiempo_Estudio.\n\n\n\n\n\n\nDraper, NR. 1998. Applied regression analysis. McGraw-Hill. Inc.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema1.html#exploración-inicial-visualización-y-cuantificación-de-la-relación-lineal",
    "href": "tema1.html#exploración-inicial-visualización-y-cuantificación-de-la-relación-lineal",
    "title": "2  El modelo de regresión lineal simple",
    "section": "",
    "text": "2.1.1 Visualización: el gráfico de dispersión\nLa herramienta más potente para examinar la relación entre dos variables continuas es el gráfico de dispersión (scatterplot). Nos permite intuir visualmente la forma (¿es lineal, curva, sin patrón?), la dirección (¿positiva o negativa?) y la fuerza (¿los puntos están muy agrupados o muy dispersos?) de la relación. Una inspección visual es siempre el punto de partida.\n\n\n2.1.2 Cuantificación de la asociación: covarianza y correlación\nUna vez que la visualización sugiere una tendencia, necesitamos métricas para cuantificarla.\n\n2.1.2.1 Covarianza\nLa covarianza es una medida de la variabilidad conjunta de dos variables. Nos indica la dirección de la relación lineal. La covarianza muestral se calcula como:\n\\[\n\\text{Cov}(X, Y) = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\n\\]\n\nSi la covarianza es positiva, significa que cuando \\(X\\) está por encima de su media, \\(Y\\) también tiende a estarlo. La relación es directa.\nSi es negativa, cuando \\(X\\) está por encima de su media, \\(Y\\) tiende a estar por debajo. La relación es inversa.\nSi es cercana a cero, no hay una tendencia lineal clara.\n\nEl principal inconveniente de la covarianza es que su magnitud depende de las unidades de las variables, lo que la hace difícil de interpretar. Una covarianza de 100 puede ser muy grande o muy pequeña dependiendo de si medimos en metros o en milímetros.\n\n\n2.1.2.2 Coeficiente de correlación de Pearson\nPara solucionar el problema de la escala, estandarizamos la covarianza, dividiéndola por el producto de las desviaciones típicas de cada variable. El resultado es el coeficiente de correlación de Pearson (\\(r\\)):\n\\[\nr(X, Y) = \\frac{\\text{Cov}(X, Y)}{s_X \\cdot s_Y}\n\\]\nEste coeficiente es adimensional y siempre varía entre -1 y 1, lo que permite una interpretación universal:\n\nUn valor cercano a 1 indica una relación lineal positiva fuerte.\nUn valor cercano a -1 indica una relación lineal negativa fuerte.\nUn valor cercano a 0 indica una ausencia de relación lineal.\n\nEs crucial recordar que la correlación solo mide la fuerza de la asociación lineal. Dos variables pueden tener una relación perfecta no lineal (por ejemplo, una parábola) y aun así tener una correlación de Pearson cercana a cero.\n\n\n\n\n\n\nEjemplo práctico: Horas de estudio vs. Calificaciones\n\n\n\nVamos a plantear un problema que nos acompañará durante todo el capítulo: queremos saber si el tiempo de estudio semanal influye en las calificaciones finales.\nPrimero, simulamos los datos y los visualizamos. Luego, calculamos su covarianza y correlación para cuantificar la relación.\n\nlibrary(ggplot2)\nset.seed(123) # Para reproducibilidad\n\n# Simulación de datos\ndatos &lt;- data.frame(\n  Tiempo_Estudio = round(runif(100, min = 5, max = 40), 1)\n)\ndatos$Calificaciones &lt;- round(5 + 0.1 * datos$Tiempo_Estudio + rnorm(100, mean = 0, sd = 0.5), 2)\n\n# Visualización con ggplot2\nggplot(datos, aes(x = Tiempo_Estudio, y = Calificaciones)) +\n  geom_point(color = \"#0072B2\", alpha = 0.7) +\n  labs(\n    title = \"Relación entre Tiempo de Estudio y Calificaciones\",\n    x = \"Tiempo de Estudio (horas/semana)\",\n    y = \"Calificaciones (promedio)\"\n  ) +\n  theme_minimal(base_size = 14)\n\n# Cálculo de la covarianza y correlación\ncovarianza &lt;- cov(datos$Tiempo_Estudio, datos$Calificaciones)\ncorrelacion &lt;- cor(datos$Tiempo_Estudio, datos$Calificaciones)\n\nprint(paste(\"La covarianza es:\", round(covarianza, 4)))\n\n[1] \"La covarianza es: 9.8215\"\n\nprint(paste(\"El coeficiente de correlación de Pearson es:\", round(correlacion, 4)))\n\n[1] \"El coeficiente de correlación de Pearson es: 0.8983\"\n\n\n\n\n\n\n\n\nFigura 2.1: Relación entre tiempo de estudio y calificaciones.\n\n\n\n\n\nEl gráfico muestra una clara tendencia lineal positiva. La covarianza positiva confirma esta dirección, y el coeficiente de correlación de 0.9 nos dice que esta asociación lineal es, además, muy fuerte. Esta evidencia visual y numérica nos da una base sólida para proponer un modelo de regresión lineal.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema1.html#el-proceso-de-modelización-estadística",
    "href": "tema1.html#el-proceso-de-modelización-estadística",
    "title": "2  El modelo de regresión lineal simple",
    "section": "",
    "text": "2.1.1 1. Contextualización del problema\nEl primer paso es siempre definir el problema, establecer objetivos claros y determinar las variables involucradas.\n\n\n\n\n\n\nEjemplo: Horas de estudio vs. Calificaciones\n\n\n\n\nProblema: Investigar si existe una relación positiva entre el tiempo dedicado al estudio semanal y el promedio de calificaciones de los estudiantes.\nVariables:\n\nVariable explicativa (independiente): Tiempo_Estudio (en horas semanales).\nVariable respuesta (dependiente): Calificaciones (promedio de 0 a 10).\n\nObjetivo: Determinar si los estudiantes que dedican más horas al estudio obtienen, en promedio, mejores calificaciones.\n\nPara explorar este problema, simularemos un conjunto de datos en R.\n\nset.seed(123) # Para reproducibilidad\ndatos &lt;- data.frame(\n  Tiempo_Estudio = round(runif(100, min = 5, max = 40), 1)\n)\n# La relación \"verdadera\" que queremos descubrir es: Calificaciones = 5 + 0.1*Tiempo_Estudio + Ruido\ndatos$Calificaciones &lt;- round(5 + 0.1 * datos$Tiempo_Estudio + rnorm(100, mean = 0, sd = 0.5), 2)\n\nknitr::kable(head(datos), caption = \"Muestra de datos generados.\")\n\n\n\n\nMuestra de datos generados.\n\n\nTiempo_Estudio\nCalificaciones\n\n\n\n\n15.1\n6.64\n\n\n32.6\n8.25\n\n\n19.3\n6.91\n\n\n35.9\n9.27\n\n\n37.9\n8.68\n\n\n6.6\n6.42\n\n\n\n\n\nFigura 2.1: Primeros registros de los datos simulados.\n\n\n\n\n\n\n\n\n2.1.2 2. Inspección gráfica e identificación de tendencias\nAntes de ajustar cualquier modelo, es crucial visualizar los datos. Un gráfico de dispersión nos permite observar la forma, dirección y fuerza de la relación.\n\n\n\n\n\n\nEjemplo: Visualización\n\n\n\n\nlibrary(ggplot2)\n\nggplot(datos, aes(x = Tiempo_Estudio, y = Calificaciones)) +\n  geom_point(color = \"#0072B2\", alpha = 0.7) +\n  labs(\n    title = \"Relación entre Tiempo de Estudio y Calificaciones\",\n    x = \"Tiempo de Estudio (horas/semana)\",\n    y = \"Calificaciones (promedio)\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\nFigura 2.2: Gráfico de dispersión de la relación entre tiempo de estudio y calificaciones.\n\n\n\n\n\nLa inspección visual sugiere una tendencia lineal positiva.\n\n\n\n\n2.1.3 3. Cuantificación de la relación: correlación\nPara medir la fuerza de esta tendencia lineal, calculamos el coeficiente de correlación de Pearson (\\(r\\)).\n\\[\nr(X, Y) = \\frac{\\text{Cov}(X, Y)}{s_X \\cdot s_Y} = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^{n}(X_i - \\bar{X})^2 \\sum_{i=1}^{n}(Y_i - \\bar{Y})^2}}\n\\]\nEste coeficiente adimensional varía entre -1 y 1. Un valor cercano a |1| indica una relación lineal fuerte, mientras que un valor cercano a 0 sugiere una relación lineal débil o inexistente.\n\n\n\n\n\n\nEjemplo: Cálculo de la Correlación\n\n\n\n\ncorrelacion &lt;- cor(datos$Tiempo_Estudio, datos$Calificaciones)\nprint(paste(\"El coeficiente de correlación de Pearson es:\", round(correlacion, 4)))\n\n[1] \"El coeficiente de correlación de Pearson es: 0.8983\"\n\n\nEl resultado confirma una fuerte asociación lineal positiva.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema1.html#el-modelo-de-regresión-lineal-simple-formulación-y-supuestos",
    "href": "tema1.html#el-modelo-de-regresión-lineal-simple-formulación-y-supuestos",
    "title": "2  El modelo de regresión lineal simple",
    "section": "2.2 El modelo de regresión lineal simple: formulación y supuestos",
    "text": "2.2 El modelo de regresión lineal simple: formulación y supuestos\nCon la evidencia de una relación lineal, podemos formalizarla con un modelo matemático.\n\n2.2.1 Formulación matemática del modelo\nEl modelo postula que la variable respuesta \\(Y\\) es una función lineal de la predictora \\(X\\), más un error aleatorio \\(\\varepsilon\\).\n\nModelo Poblacional (Verdad Teórica): \\[\n  Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\n  \\] Donde \\(\\beta_0\\) y \\(\\beta_1\\) son los parámetros poblacionales (intercepto y pendiente) que deseamos estimar.\nModelo Muestral (Recta Ajustada): \\[\n  \\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\n  \\] Donde \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) son las estimaciones de los parámetros obtenidas a partir de la muestra.\n\n\n\n2.2.2 Los supuestos del modelo lineal clásico\nPara que las inferencias del modelo sean válidas, se requieren ciertos supuestos sobre el término de error \\(\\varepsilon_i\\):\n\nLinealidad: La relación entre \\(X\\) y el valor esperado de \\(Y\\) es lineal.\nIndependencia: Los errores \\(\\varepsilon_i\\) son independientes entre sí.\nHomocedasticidad: La varianza de los errores es constante (\\(\\sigma^2\\)) para todos los valores de \\(X\\).\nNormalidad: Los errores siguen una distribución Normal con media cero y varianza \\(\\sigma^2\\). Este supuesto es crucial para la inferencia.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema1.html#estimación-de-los-parámetros-del-modelo",
    "href": "tema1.html#estimación-de-los-parámetros-del-modelo",
    "title": "2  El modelo de regresión lineal simple",
    "section": "2.3 Estimación de los parámetros del modelo",
    "text": "2.3 Estimación de los parámetros del modelo\nPara encontrar los “mejores” valores de \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\), utilizamos el método de mínimos cuadrados ordinarios (MCO).\n\n2.3.1 El criterio de mínimos cuadrados\nMCO busca la recta que minimiza la Suma de los Cuadrados de los Errores (SCE), es decir, la suma de las distancias verticales al cuadrado entre los puntos observados y la recta de regresión:\n\\[\n\\text{SCE} = \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2 = \\sum_{i=1}^{n} (Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_i))^2\n\\]\nMediante cálculo diferencial, se obtienen los estimadores que minimizan esta función:\n\\[\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)} \\quad \\quad \\quad \\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{X}\n\\]\n\n\n2.3.2 Ajuste del modelo en R\nLa función lm() en R realiza estos cálculos por nosotros.\n\n\n\n\n\n\nEjemplo: Ajuste y Resumen del Modelo\n\n\n\n\n# Ajustar el modelo lineal\nmodelo_estudio &lt;- lm(Calificaciones ~ Tiempo_Estudio, data = datos)\n\n# Visualizar el ajuste\nggplot(datos, aes(x = Tiempo_Estudio, y = Calificaciones)) +\n  geom_point(color = \"#0072B2\", alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Ajuste del Modelo de Regresión Lineal\",\n    x = \"Tiempo de Estudio (horas/semana)\",\n    y = \"Calificaciones (promedio)\"\n  ) +\n  theme_minimal(base_size = 14)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n# Ver un resumen completo de los resultados del modelo\nsummary(modelo_estudio)\n\n\nCall:\nlm(formula = Calificaciones ~ Tiempo_Estudio, data = datos)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.11465 -0.30262 -0.00942  0.29509  1.10533 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     5.00118    0.11977   41.76   &lt;2e-16 ***\nTiempo_Estudio  0.09875    0.00488   20.23   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4842 on 98 degrees of freedom\nMultiple R-squared:  0.8069,    Adjusted R-squared:  0.8049 \nF-statistic: 409.5 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nFigura 2.3: Recta de regresión ajustada a los datos.\n\n\n\n\n\nInterpretación del summary: * Coefficients: Nos da las estimaciones \\(\\hat{\\beta}_0\\) (5) y \\(\\hat{\\beta}_1\\) (0.1). También nos proporciona sus errores estándar, el estadístico t y el p-valor para el contraste de hipótesis \\(H_0: \\beta_j = 0\\). * R-squared: El valor de \\(R^2\\) (0.81) nos indica que el 81% de la variabilidad en las calificaciones es explicada por el tiempo de estudio. * F-statistic: El p-valor del estadístico F (98) evalúa la significancia global del modelo. Un p-valor muy pequeño, como en este caso, confirma que el modelo es significativo.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema1.html#diagnóstico-del-modelo",
    "href": "tema1.html#diagnóstico-del-modelo",
    "title": "2  El modelo de regresión lineal simple",
    "section": "2.6 Diagnóstico del Modelo",
    "text": "2.6 Diagnóstico del Modelo\nUna vez que hemos ajustado un modelo y evaluado su significancia, el trabajo no ha terminado. Un paso crucial, a menudo subestimado, es el diagnóstico del modelo. Este proceso consiste en verificar si se cumplen los supuestos del modelo de regresión lineal clásico. La fiabilidad de nuestras inferencias (los p-valores de los contrastes t y F, y los intervalos de confianza) depende directamente de la validez de estos supuestos.\nEl diagnóstico se realiza principalmente a través del análisis de los residuos del modelo (\\(e_i = y_i - \\hat{y}_i\\)). Los residuos son nuestra mejor aproximación empírica de los errores teóricos no observables (\\(\\varepsilon_i\\)). A continuación, se detalla cómo verificar cada uno de los supuestos clave.\n\n2.6.1 Linealidad\nEste supuesto establece que la relación entre la variable predictora \\(X\\) y el valor esperado de la variable respuesta \\(Y\\) es, en promedio, una línea recta: \\(E[Y | X] = \\beta_0 + \\beta_1 X\\).\nLa herramienta fundamental para diagnosticar la linealidad es el gráfico de residuos (\\(e_i\\)) frente a los valores ajustados por el modelo (\\(\\hat{y}_i\\)). La lógica de este gráfico es sencilla pero potente: si el modelo lineal es adecuado, los errores que comete (los residuos) deberían ser completamente aleatorios, sin guardar relación alguna con la magnitud de las predicciones. En esencia, buscamos confirmar que no queda ninguna información sistemática en los errores que el modelo no haya capturado.\nEn un escenario ideal, este gráfico debería parecer una nube de puntos distribuida horizontalmente y sin estructura aparente, centrada en la línea del cero. Esto nos indica que los errores son, en promedio, nulos para todos los niveles de predicción, cumpliendo así el supuesto de linealidad. La línea roja que R superpone en este gráfico, que suaviza la tendencia de los puntos, debería ser prácticamente plana y pegada al cero, confirmando la ausencia de patrones.\n\n\n\n\n\n\nEjemplo de un Modelo Válido (Nuestro Caso)\n\n\n\nPara nuestro modelo_estudio, podemos generar específicamente el primer gráfico de diagnóstico, que es el de Residuos vs. Valores Ajustados.\n\n# Usamos which = 1 para seleccionar solo el primer gráfico de diagnóstico\nplot(modelo_estudio, which = 1)\n\n\n\n\n\n\n\nFigura 2.3: Gráfico de Residuos vs. Valores Ajustados para el modelo de estudio. No se observan patrones.\n\n\n\n\n\nComo se puede observar, los puntos se distribuyen de forma aleatoria alrededor de la línea horizontal en cero. La línea roja, que suaviza la tendencia de los residuos, es prácticamente plana. Esto es un claro indicativo de que el supuesto de linealidad se cumple en nuestro modelo.\n\n\nPor el contrario, la aparición de un patrón sistemático en los residuos es la señal de alarma de que algo anda mal. En lo que respecta al supuesto de linealidad, la evidencia más clara de una violación es una tendencia curvilínea (como una “U” o una parábola). Este patrón nos dice que el modelo es estructuralmente incapaz de capturar la forma de los datos y, por lo tanto, comete errores predecibles. Por ejemplo, puede subestimar la respuesta en los extremos (generando residuos positivos) y sobreestimarla en el centro (residuos negativos), lo que invalida el modelo lineal.\n\n\n\n\n\n\nContraejemplo: Violación del Supuesto de Linealidad\n\n\n\nAhora, vamos a simular a propósito unos datos que siguen una relación cuadrática (curva) y ajustaremos incorrectamente un modelo lineal para ver cómo se manifiesta el problema en el gráfico de diagnóstico.\n\n# 1. Simulación de datos no lineales\nset.seed(42) # Nueva semilla para este ejemplo\nx_no_lineal &lt;- runif(100, 0, 10)\n# La relación verdadera es cuadrática (y = 10 - (x-5)^2) más un error\ny_no_lineal &lt;- 10 - (x_no_lineal - 5)^2 + rnorm(100, 0, 4)\ndatos_no_lineal &lt;- data.frame(x = x_no_lineal, y = y_no_lineal)\n\n# 2. Ajuste de un modelo lineal (incorrecto)\nmodelo_no_lineal &lt;- lm(y ~ x, data = datos_no_lineal)\n\n# 3. Gráfico de Residuos vs. Valores Ajustados\nplot(modelo_no_lineal, which = 1)\n\n\n\n\n\n\n\nFigura 2.4: Patrón curvo evidente en los residuos, violando el supuesto de linealidad.\n\n\n\n\n\nEl gráfico de diagnóstico es inequívoco. A diferencia del ejemplo anterior, donde los puntos formaban una nube aleatoria, aquí los residuos dibujan un patrón parabólico perfecto (una “U” invertida). La línea roja de tendencia, en lugar de ser plana, sigue fielmente esta curva.\n\n\n\n\n2.6.2 Homocedasticidad\nEl supuesto de homocedasticidad establece que la varianza de los errores del modelo debe ser constante para todos los niveles de la variable predictora. Es decir, la dispersión de los datos alrededor de la línea de regresión es la misma en todo su recorrido (\\(Var(\\varepsilon_i | X_i) = \\sigma^2\\)). La violación de este supuesto se conoce como heteroscedasticidad, y es un problema común en el modelado.\n¿Por qué es tan importante? Si un modelo es heteroscedástico, los errores estándar de los coeficientes (\\(\\beta_0, \\beta_1\\)) estarán calculados de forma incorrecta. Como consecuencia, los intervalos de confianza y los contrastes de hipótesis (p-valores) no serán fiables, pudiendo llevarnos a conclusiones erróneas sobre la significancia de nuestras variables.\n\n\n\n\n\n\n¿Qué son los Residuos Estandarizados y Estudentizados?\n\n\n\nAntes de analizar los gráficos, es útil definir los residuos estandarizados. Un residuo simple (\\(e_i = y_i - \\hat{y}_i\\)) nos dice cuán lejos está una observación de la línea de regresión. Sin embargo, no todos los residuos son directamente comparables, ya que su varianza puede depender de los valores de \\(X\\) (especialmente de su apalancamiento o leverage).\nPara solucionar esto, se estandarizan los residuos, dividiéndolos por una estimación de su desviación estándar. Esto los pone a todos en una escala común (similar a una puntuación Z), haciendo más fácil la identificación de patrones y valores atípicos. R, en sus gráficos de diagnóstico, utiliza una versión aún más refinada llamada residuos estudentizados, que es más precisa para detectar outliers. A efectos prácticos, la interpretación es la misma.\n\n\nLa heteroscedasticidad se detecta principalmente buscando patrones en la dispersión de los residuos.\n\nGráfico de Residuos vs. Valores Ajustados: Como en la prueba de linealidad, este gráfico es nuestra primera herramienta. Aquí no buscamos patrones en la media de los residuos (que debe ser cero), sino en su dispersión. La señal de alarma inequívoca de heteroscedasticidad es una forma de embudo o megáfono, donde la dispersión de los residuos aumenta o disminuye a medida que cambian los valores ajustados.\nGráfico Scale-Location: Este gráfico está diseñado específicamente para detectar heteroscedasticidad. Muestra la raíz cuadrada de los residuos estandarizados en el eje Y (sqrt(|Standardized residuals|)) frente a los valores ajustados en el eje X. Al usar la raíz cuadrada, se suaviza la distribución de los residuos, haciendo los patrones de varianza más fáciles de ver. Si la varianza es constante (homocedasticidad), deberíamos ver una nube de puntos aleatoria con una línea de tendencia roja aproximadamente plana. Una pendiente en esta línea roja indica que la varianza cambia con el nivel de la respuesta.\nPrueba de Breusch-Pagan: Es el contraste de hipótesis formal. Su lógica es ingeniosa: realiza una regresión auxiliar donde intenta predecir los residuos al cuadrado a partir de las variables predictoras originales. Si las variables predictoras ayudan a explicar la magnitud de los residuos al cuadrado, significa que la varianza del error depende de los predictores, y por tanto, hay heteroscedasticidad.\n\nHipótesis Nula (\\(H_0\\)): El modelo es homocedástico.\nDecisión: Un p-valor pequeño (p. ej., &lt; 0.05) es evidencia en contra de la homocedasticidad.\n\n\n\n\n\n\n\n\nEjemplo de un Modelo Válido (Nuestro Caso)\n\n\n\nAnalicemos nuestro modelo_estudio. Nos centraremos en el gráfico Scale-Location (which = 3) y en la prueba de Breusch-Pagan.\n\n# Usamos which = 3 para seleccionar el gráfico Scale-Location\nplot(modelo_estudio, which = 3)\n\n# Prueba de Breusch-Pagan\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nbptest(modelo_estudio)\n\n\n    studentized Breusch-Pagan test\n\ndata:  modelo_estudio\nBP = 0.019638, df = 1, p-value = 0.8886\n\n\n\n\n\n\n\n\nFigura 2.5: Gráfico Scale-Location para el modelo de estudio. La línea de tendencia es casi plana.\n\n\n\n\n\nEl diagnóstico es positivo. En el gráfico Scale-Location, la línea roja es casi horizontal, lo que indica que la varianza de los residuos es estable a lo largo de los valores ajustados. Esto se confirma con la prueba de Breusch-Pagan, que arroja un p-valor alto, por lo que no tenemos evidencia para rechazar la hipótesis nula de homocedasticidad. Nuestro modelo cumple el supuesto.\n\n\n\n\n\n\n\n\nContraejemplo: Violación del Supuesto de Homocedasticidad\n\n\n\nAhora, simularemos datos donde el error aumenta a medida que x crece, un caso clásico de heteroscedasticidad.\n\n# Esta línea le dice a R que prepare una rejilla de 1 fila y 2 columnas para los gráficos\npar(mfrow = c(1, 2)) \n\n# 1. Simulación de datos (la misma que antes)\nset.seed(101)\nx_hetero &lt;- 1:100\ny_hetero &lt;- 10 + 2 * x_hetero + rnorm(100, mean = 0, sd = 0.4 * x_hetero)\ndatos_hetero &lt;- data.frame(x = x_hetero, y = y_hetero)\nmodelo_hetero &lt;- lm(y ~ x, data = datos_hetero)\n\n# 2. Gráficos de diagnóstico\n# El primer plot irá a la izquierda\nplot(modelo_hetero, which = 1) \n\n# El segundo plot irá a la derecha\nplot(modelo_hetero, which = 3)\n\n# 3. Prueba de Breusch-Pagan\n# La prueba no genera un gráfico, por lo que no afecta al layout\nlibrary(lmtest)\ntest_values &lt;- bptest(modelo_hetero)\n\n\n\n\n\n\n\nFigura 2.6: Diagnóstico de heteroscedasticidad. Izquierda: Gráfico de Residuos vs. Ajustados (patrón de embudo). Derecha: Gráfico Scale-Location (tendencia ascendente).\n\n\n\n\n\nLos resultados son un libro de texto sobre la heteroscedasticidad.\n\nEl gráfico de Residuos vs. Valores Ajustados (izquierda) tiene una forma de embudo inconfundible: la dispersión de los puntos aumenta drásticamente de izquierda a derecha.\nEl gráfico Scale-Location (derecha) confirma el problema, mostrando una línea roja con una clara pendiente ascendente.\nLa prueba de Breusch-Pagan arroja un p-valor 7.43e-07, dándonos una fuerte evidencia estadística para rechazar la hipótesis nula de homocedasticidad.\n\nEste modelo viola claramente el supuesto, y las inferencias basadas en él (como el p-valor del coeficiente de x) no serían fiables.\n\n\n\n\n2.6.3 3. Normalidad de los Errores\nEste supuesto postula que los errores del modelo (\\(\\varepsilon_i\\)) siguen una distribución normal: \\(\\varepsilon_i \\sim N(0, \\sigma^2)\\). Es especialmente importante para la validez de los intervalos de confianza y los contrastes de hipótesis cuando el tamaño de la muestra es pequeño.\n\n2.6.3.1 Evaluación Visual\n\nGráfico Normal Q-Q (Normal Q-Q Plot): Compara los cuantiles de los residuos estandarizados con los cuantiles de una distribución normal teórica.\n\nQué buscar: Los puntos deben caer muy cerca de la línea diagonal de 45 grados.\n\nHistograma de los Residuos: Un simple histograma de los residuos debe mostrar una forma aproximada de campana de Gauss.\n\n\n\n2.6.3.2 Prueba Analítica\n\nPrueba de Shapiro-Wilk: Es uno de los contrastes más potentes para la normalidad.\n\nHipótesis Nula (\\(H_0\\)): Los residuos provienen de una distribución normal.\nDecisión: Un p-valor pequeño (&lt; 0.05) sugiere rechazar \\(H_0\\).\n\n\n\n\n\n\n2.6.4 4. Independencia de los Errores\nEste supuesto afirma que el error de una observación no está correlacionado con el de ninguna otra: \\(Cov(\\varepsilon_i, \\varepsilon_j) = 0\\) para \\(i \\neq j\\). La violación, conocida como autocorrelación, es común en datos de series temporales.\n\n2.6.4.1 Evaluación\n\nPrueba de Durbin-Watson: Es el contraste clásico para la autocorrelación de primer orden. Su estadístico se calcula como: \\[ DW = \\frac{\\sum_{i=2}^{n}(e_i - e_{i-1})^2}{\\sum_{i=1}^{n}e_i^2} \\] El estadístico varía entre 0 y 4. Un valor cercano a 2 sugiere no autocorrelación. Valores cercanos a 0 indican autocorrelación positiva, y cercanos a 4, autocorrelación negativa.\n\n\n\n\n\n2.6.5 5. Identificación de Observaciones Influyentes y Atípicas\nAlgunos puntos pueden tener una influencia desproporcionada en el modelo. Es crucial identificarlos usando tres métricas clave:\n\nApalancamiento (Leverage, \\(h_{ii}\\)): Mide cuán atípico es el valor de la variable predictora \\(X_i\\) de una observación. Un apalancamiento alto significa que el punto tiene el potencial de ser muy influyente. En regresión simple, se calcula como: \\[ h_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n}(x_j - \\bar{x})^2} \\] Una regla común es considerar un apalancamiento alto si \\(h_{ii} &gt; \\frac{2(k+1)}{n}\\), donde \\(k\\) es el número de predictores (1 en regresión simple).\nResiduos Estudentizados: Son una versión de los residuos estandarizados que son más efectivos para detectar outliers. Un residuo estudentizado (\\(r_i\\)) sigue una distribución t de Student con \\(n-k-2\\) grados de libertad. Valores \\(|r_i| &gt; 2\\) o \\(3\\) a menudo se consideran atípicos.\nDistancia de Cook (\\(D_i\\)): Mide la influencia global de una observación, combinando su apalancamiento y su residuo. Representa cuánto cambian los coeficientes del modelo si la i-ésima observación es eliminada. \\[ D_i = \\frac{r_i^2}{k+1} \\cdot \\frac{h_{ii}}{1-h_{ii}} \\] Se considera que un punto es influyente si su distancia de Cook es grande, por ejemplo, si \\(D_i &gt; 1\\) o \\(D_i &gt; 4/(n-k-1)\\).\n\nEl gráfico Residuals vs. Leverage es la herramienta visual que combina estas tres métricas, facilitando la identificación de puntos problemáticos.\n\n\n2.6.6 Aplicación Práctica en R\nA continuación, se realizan las pruebas de diagnóstico para nuestro modelo_estudio.\n\n# Gráficos de diagnóstico visual estándar\npar(mfrow = c(2, 2))\nplot(modelo_estudio)\npar(mfrow = c(1, 1))\n\n# Para las pruebas analíticas, usamos el paquete 'lmtest'\n# Si no está instalado, ejecuta: install.packages(\"lmtest\")\nlibrary(lmtest)\n\n# 1. Prueba de Breusch-Pagan para homocedasticidad\nbptest(modelo_estudio)\n\n\n    studentized Breusch-Pagan test\n\ndata:  modelo_estudio\nBP = 0.019638, df = 1, p-value = 0.8886\n\n# 2. Prueba de Shapiro-Wilk para normalidad (sobre los residuos)\nshapiro.test(residuals(modelo_estudio))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(modelo_estudio)\nW = 0.99008, p-value = 0.671\n\n# 3. Prueba de Durbin-Watson para independencia\ndwtest(modelo_estudio)\n\n\n    Durbin-Watson test\n\ndata:  modelo_estudio\nDW = 2.0565, p-value = 0.6104\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\n\n\n\n\n\nFigura 2.7: Gráficos de diagnóstico estándar generados por R.\n\n\n\n\n\n\n\n\n\nDraper, NR. 1998. Applied regression analysis. McGraw-Hill. Inc.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema1.html#exploración-inicial-visualización-y-cuantificación-de-la-relación",
    "href": "tema1.html#exploración-inicial-visualización-y-cuantificación-de-la-relación",
    "title": "2  El modelo de regresión lineal simple",
    "section": "",
    "text": "2.1.1 Visualización: el gráfico de dispersión\nLa herramienta más potente para examinar la relación entre dos variables continuas es el gráfico de dispersión (scatterplot). Nos permite intuir visualmente la forma, la dirección y la fuerza de la relación. Una inspección visual es siempre el punto de partida.\n\n\n2.1.2 Cuantificación de la asociación: covarianza y correlación\nUna vez que la visualización sugiere una tendencia, necesitamos métricas para cuantificarla.\n\n2.1.2.1 Covarianza\nLa covarianza es una medida de la variabilidad conjunta de dos variables aleatorias, \\(X\\) e \\(Y\\). Nos indica la dirección de la relación lineal. La covarianza muestral, calculada a partir de nuestras observaciones \\((x_i, y_i)\\), es:\n\\[\n\\text{Cov}(x, y) = s_{xy} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{n-1}\n\\]\nEl principal inconveniente de la covarianza es que su magnitud depende de las unidades de las variables, lo que la hace difícil de interpretar.\n\n\n2.1.2.2 Coeficiente de correlación de Pearson\nPara solucionar el problema de la escala, estandarizamos la covarianza, dividiéndola por el producto de las desviaciones típicas de cada variable. El resultado es el coeficiente de correlación de Pearson (\\(r\\)):\n\\[\nr = r_{xy} = \\frac{s_{xy}}{s_x s_y}\n\\]\nEste coeficiente es adimensional y siempre varía entre -1 y 1, lo que permite una interpretación universal de la fuerza de la asociación lineal.\n\n\n\n\n\n\nEjemplo práctico: Horas de estudio vs. Calificaciones\n\n\n\nVamos a plantear un problema que nos acompañará durante todo el capítulo: queremos saber si el tiempo de estudio semanal influye en las calificaciones finales.\n\nlibrary(ggplot2)\nset.seed(123) # Para reproducibilidad\n\n# Simulación de datos\ndatos &lt;- data.frame(\n  Tiempo_Estudio = round(runif(100, min = 5, max = 40), 1)\n)\ndatos$Calificaciones &lt;- round(5 + 0.1 * datos$Tiempo_Estudio + rnorm(100, mean = 0, sd = 0.5), 2)\n\n# Visualización\nggplot(datos, aes(x = Tiempo_Estudio, y = Calificaciones)) +\n  geom_point(color = \"#0072B2\", alpha = 0.7) +\n  labs(\n    title = \"Relación entre Tiempo de Estudio y Calificaciones\",\n    x = \"Tiempo de Estudio (horas/semana)\",\n    y = \"Calificaciones (promedio)\"\n  ) +\n  theme_classic(base_size = 14)\n\n# Cuantificación (los objetos se guardan para usarlos en el texto)\ncovarianza &lt;- cov(datos$Tiempo_Estudio, datos$Calificaciones)\ncorrelacion &lt;- cor(datos$Tiempo_Estudio, datos$Calificaciones)\n\n\n\n\n\n\n\nFigura 2.1: Relación entre tiempo de estudio y calificaciones.\n\n\n\n\n\nEl gráfico muestra una clara tendencia lineal positiva. La covarianza toma un valor de 9.82, y el coeficiente de correlación de Pearson es de 0.9. Ambos valores confirman que la asociación lineal es, además de positiva, muy fuerte. Esta evidencia visual y numérica nos da una base sólida para proponer un modelo de regresión lineal.\n\n\n\n\n\n\n\n\n¡Correlación no implica causalidad!\n\n\n\nEl haber encontrado una fuerte correlación positiva entre el tiempo de estudio y las calificaciones (0.9) no nos autoriza a concluir que una cosa causa la otra. La regresión lineal puede demostrar que las variables se mueven juntas y nos permite predecir una a partir de la otra, pero no explica el porqué de la relación.\nPodría existir una tercera variable oculta (p. ej., el interés del alumno en la materia) que influya tanto en las horas de estudio como en las calificaciones. Establecer causalidad requiere un diseño experimental riguroso (asignando aleatoriamente a los estudiantes a diferentes tiempos de estudio), no solo un análisis observacional.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema1.html#formulación-teórica-del-modelo",
    "href": "tema1.html#formulación-teórica-del-modelo",
    "title": "2  El modelo de regresión lineal simple",
    "section": "2.2 Formulación teórica del modelo",
    "text": "2.2 Formulación teórica del modelo\nUna vez que la exploración sugiere una relación lineal, el siguiente paso es formalizarla matemáticamente. Aquí es donde definimos la estructura teórica del modelo y los supuestos bajo los cuales operará.\n\n2.2.1 El modelo poblacional y sus componentes\nEl modelo poblacional postula que la relación verdadera entre la variable respuesta \\(Y\\) y la predictora \\(X\\) sigue una línea recta, aunque contaminada por cierta aleatoriedad. Para cualquier individuo \\(i\\) de la población, esta relación se describe como:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\n\\]\nEn esta ecuación, \\(\\beta_0\\) y \\(\\beta_1\\) son los parámetros poblacionales (el intercepto y la pendiente verdaderos pero desconocidos), y \\(\\varepsilon_i\\) es el error aleatorio, un componente fundamental que captura todas las fuentes de variabilidad que el modelo no puede explicar por sí solo. Específicamente, este término incluye:\n\nVariables omitidas: Factores que también afectan a las calificaciones (como la calidad del sueño, la motivación del estudiante o su conocimiento previo) y que no están en el modelo.\nError de medida: Pequeñas imprecisiones al medir las variables (p. ej., un estudiante podría reportar 20 horas de estudio cuando en realidad fueron 19.5).\nAleatoriedad inherente: La variabilidad puramente estocástica o impredecible en el comportamiento humano.\n\nComo nunca observamos la población entera, nuestro trabajo consiste en usar una muestra para estimar el modelo muestral:\n\\[\n\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\n\\]\nAquí, los “gorros” (\\(\\hat{\\cdot}\\)) denotan estimaciones calculadas a partir de la muestra. La diferencia entre el valor real y el predicho, \\(e_i = y_i - \\hat{y}_i\\), se conoce como residuo.\n\n\n2.2.2 Los supuestos del modelo lineal clásico (Gauss-Markov)\nPara que el puente entre nuestro modelo muestral y la realidad poblacional sea sólido, debemos asumir que los errores teóricos \\(\\varepsilon_i\\) se comportan de una manera predecible y ordenada.\n\nLinealidad: La relación entre \\(X\\) y el valor esperado de \\(Y\\) es, en promedio, una línea recta: \\(E[Y_i | X_i] = \\beta_0 + \\beta_1 X_i\\).\nIndependencia de los errores: El error de una observación no está correlacionado con el error de ninguna otra: \\(\\text{Cov}(\\varepsilon_i, \\varepsilon_j) = 0\\) para \\(i \\neq j\\).\nHomocedasticidad: La varianza del error es constante (\\(\\sigma^2\\)) para todos los valores de \\(X\\): \\(Var(\\varepsilon_i | X_i) = \\sigma^2\\). Esto significa que la dispersión de los datos alrededor de la línea de regresión es la misma a lo largo de todos los valores de la variable predictora. La violación de este supuesto se conoce como heterocedasticidad, donde la dispersión de los errores cambia (p. ej., aumenta a medida que \\(X\\) crece).\n\nCuando el objetivo no es sólo estimar la recta, sino inferir con ella, entonces se asume una hipótesis más: la normalidad de la variable respuesta, o lo que es lo mismo, del error aleatorio:\n\nNormalidad de los errores: Para la inferencia, se asume que los errores siguen una distribución Normal con media cero y varianza \\(\\sigma^2\\): \\(\\varepsilon_i \\sim N(0, \\sigma^2)\\).\n\nEstos supuestos son esenciales para garantizar la validez de las estimaciones y conclusiones derivadas del modelo.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema1.html#estimación-de-los-parámetros-por-mínimos-cuadrados-ordinarios-mco",
    "href": "tema1.html#estimación-de-los-parámetros-por-mínimos-cuadrados-ordinarios-mco",
    "title": "2  El modelo de regresión lineal simple",
    "section": "2.3 Estimación de los parámetros por Mínimos Cuadrados Ordinarios (MCO)",
    "text": "2.3 Estimación de los parámetros por Mínimos Cuadrados Ordinarios (MCO)\nNecesitamos un método para encontrar la “mejor” recta de ajuste. El Método de Mínimos Cuadrados Ordinarios (MCO) nos proporciona este criterio.\n\n2.3.1 El criterio de mínimos cuadrados\nMCO busca la recta que minimice la Suma de los Cuadrados del Error (SSE), es decir, la suma de las distancias verticales al cuadrado entre los puntos observados y la recta de regresión:\n\\[\n\\text{SSE}(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} e_i^2 = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_i))^2\n\\]\n\n\n2.3.2 Derivación matemática de los estimadores\nPara encontrar los valores de \\(\\beta_0\\) y \\(\\beta_1\\) que minimizan esta función, recurrimos al cálculo. Tratamos la SSE como una función de dos variables y calculamos sus derivadas parciales, igualándolas a cero para encontrar el mínimo.\n\\[\n\\frac{\\partial \\text{SSE}}{\\partial \\beta_0} = -2 \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i) = 0\n\\]\n\\[\n\\frac{\\partial \\text{SSE}}{\\partial \\beta_1} = -2 \\sum_{i=1}^{n} x_i (y_i - \\beta_0 - \\beta_1 x_i) = 0\n\\]\nLa resolución de este sistema de dos ecuaciones (conocidas como las ecuaciones normales) nos proporciona las fórmulas para los estimadores de MCO:\n\\[\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} = \\frac{s_{xy}}{s_{xx}} \\quad \\quad \\quad \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n\\]\nBajo los supuestos del modelo, el Teorema de Gauss-Markov demuestra que estos estimadores son los Mejores Estimadores Lineales Insesgados (MELI / BLUE).\n\n\n2.3.3 Implementación en R\nLa función lm() en R realiza estos cálculos por nosotros.\n\n\n\n\n\n\nEjemplo: Ajuste del Modelo\n\n\n\n\n# Ajustamos el modelo lineal\nmodelo_estudio &lt;- lm(Calificaciones ~ Tiempo_Estudio, data = datos)\n\n# Extraemos los coeficientes estimados\nprint(coef(modelo_estudio))\n\n   (Intercept) Tiempo_Estudio \n    5.00117598     0.09874923 \n\n\nLa recta de regresión muestral es: Calificaciones = 5 + 0.1 * Tiempo_Estudio.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema1.html#inferencia-y-bondad-de-ajuste",
    "href": "tema1.html#inferencia-y-bondad-de-ajuste",
    "title": "2  El modelo de regresión lineal simple",
    "section": "2.4 Inferencia y bondad de ajuste",
    "text": "2.4 Inferencia y bondad de ajuste\nUna vez hemos estimado los parámetros del modelo, nuestro trabajo apenas ha comenzado. Ahora debemos pasar de la descripción a la inferencia. Necesitamos un conjunto de herramientas que nos permitan responder a preguntas cruciales: ¿Son nuestros coeficientes estimados, \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\), meras casualidades de nuestra muestra o reflejan una relación real en la población? ¿Qué tan bueno es nuestro modelo para explicar la variabilidad de la variable respuesta? Esta sección se dedica a responder estas preguntas.\n\n2.4.1 Propiedades de los estimadores de MCO\nAntes de realizar inferencias, es fundamental entender las propiedades teóricas de los estimadores que hemos calculado.\n\nInsesgadez: Los estimadores de MCO son insesgados. Esto significa que si pudiéramos repetir nuestro muestreo muchísimas veces y calcular los estimadores en cada muestra, el promedio de todas nuestras estimaciones de \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) convergería a los verdaderos valores poblacionales \\(\\beta_0\\) y \\(\\beta_1\\). Matemáticamente: \\[\n  E[\\hat{\\beta}_0] = \\beta_0 \\quad \\text{y} \\quad E[\\hat{\\beta}_1] = \\beta_1\n  \\]\nVarianza de los estimadores: Las fórmulas para la varianza de nuestros estimadores cuantifican su precisión. Una varianza pequeña implica que el estimador es más estable a través de diferentes muestras. \\[\n  Var(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} = \\frac{\\sigma^2}{S_{xx}}\n  \\] \\[\n  Var(\\hat{\\beta}_0) = \\sigma^2 \\left[ \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} \\right]\n  \\] Donde \\(\\sigma^2\\) es la varianza (desconocida) del término de error \\(\\varepsilon\\).\nTeorema de Gauss-Markov: Este es uno de los resultados más importantes de la teoría de la regresión. Establece que, bajo los supuestos de linealidad, independencia y homocedasticidad (no se requiere normalidad), los estimadores de MCO son los Mejores Estimadores Lineales Insesgados (MELI, o BLUE en inglés). Esto significa que, de entre toda la clase de estimadores que son lineales e insesgados, los de MCO son los que tienen la menor varianza posible.\n\n\n\n\n\n\n\nPropiedades adicionales para las predicciones y para los residuos\n\n\n\n\n\n\nLa suma de los residuos es cero: \\[\n  \\sum_{i=1}^n e_i=\\sum_{i=1}^n(y_i-\\hat{y_i})=0\n  \\]\nLa suma de los valores observados es igual a la suma de los valores ajustados: \\[\n  \\sum_{i=1}^n y_i=\\sum_{i=1}^n \\hat{y_i}\n  \\]\nLa suma de los residuos ponderados por los regresores es cero: \\[\n  \\sum_{i=1}^n x_ie_i=0\n  \\]\nLa suma de los residuos ponderados por las predicciones es cero: \\[\n  \\sum_{i=1}^n \\hat{y_i}e_i=0\n  \\]\nLa recta de regresión contiene el punto \\((\\bar{x},\\bar{y})\\):\n\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nPara los datos de calificaciones y tiempo de estudio, estos son los estimadores de los parámetros del modelo de regresión:\n\n# 1. Ajustamos el modelo lineal\nmodelo_estudio &lt;- lm(Calificaciones ~ Tiempo_Estudio, data = datos)\n\n# 2. Obtenemos el resumen completo del modelo\nsummary(modelo_estudio)\n\n\nCall:\nlm(formula = Calificaciones ~ Tiempo_Estudio, data = datos)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.11465 -0.30262 -0.00942  0.29509  1.10533 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     5.00118    0.11977   41.76   &lt;2e-16 ***\nTiempo_Estudio  0.09875    0.00488   20.23   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4842 on 98 degrees of freedom\nMultiple R-squared:  0.8069,    Adjusted R-squared:  0.8049 \nF-statistic: 409.5 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n2.4.2 Estimación de la varianza del error\nLas fórmulas de la varianza de los estimadores dependen de \\(\\sigma^2\\), la varianza del error poblacional, que es desconocida. Por lo tanto, necesitamos estimarla a partir de nuestros datos. Un estimador insesgado de \\(\\sigma^2\\) es la Media Cuadrática del Error (MSE):\n\\[\n\\hat{\\sigma}^2 = \\text{MSE} = \\frac{\\text{SSE}}{n-2} = \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n-2}\n\\]\nDividimos por \\(n-2\\), los grados de libertad del error, porque hemos “gastado” dos grados de libertad de nuestros datos para estimar los dos parámetros, \\(\\beta_0\\) y \\(\\beta_1\\). La raíz cuadrada de la MSE, \\(\\hat{\\sigma}\\), se conoce como el error estándar de los residuos y es una medida de la dispersión promedio de los puntos alrededor de la recta de regresión.\n\n2.4.2.1 El error estándar de los residuos y el RMSE\nLa raíz cuadrada de la MSE, \\(\\hat{\\sigma}\\), se conoce formalmente como el error estándar de los residuos (Residual Standard Error). Este valor es nuestra estimación de la desviación estándar del error poblacional, \\(\\sigma\\), y es una medida de la dispersión promedio de los puntos alrededor de la recta de regresión.\n\\[\n\\hat{\\sigma} = \\sqrt{\\text{MSE}}\n\\]\nEn el campo del modelado predictivo y el machine learning, esta misma cantidad se conoce como la Raíz del Error Cuadrático Medio o RMSE (Root Mean Squared Error). Aunque la fórmula es idéntica, la interpretación del RMSE se centra en la evaluación del rendimiento predictivo del modelo. El RMSE nos dice, en promedio, cuál es la magnitud del error de predicción de nuestro modelo, y tiene la ventaja de estar en las mismas unidades que la variable respuesta \\(Y\\). Por ejemplo, si estamos prediciendo precios de viviendas en euros, un RMSE de 5000 significa que nuestras predicciones se desvían, en promedio, unos 5000 € de los precios reales.\n\n\n\n2.4.3 Análisis de la Varianza (ANOVA) para la significancia de la regresión\nUna vez hemos estimado los coeficientes, necesitamos una prueba formal para determinar si el modelo en su conjunto es útil. Es decir, ¿la variable predictora \\(X\\) explica una porción de la variabilidad de la variable respuesta \\(Y\\) que sea estadísticamente significativa, o la relación que observamos podría deberse simplemente al azar? El Análisis de la Varianza (ANOVA) nos proporciona la herramienta para responder a esta pregunta a través del contraste F de significancia global.\nLas hipótesis de este contraste son:\n\n\\(H_0: \\beta_1 = 0\\): La hipótesis nula postula que no existe una relación lineal entre \\(X\\) e \\(Y\\). El modelo no tiene poder explicativo y no es mejor que usar simplemente la media, \\(\\bar{y}\\), como predicción para cualquier valor de \\(x\\).\n\\(H_1: \\beta_1 \\neq 0\\): La hipótesis alternativa sostiene que sí existe una relación lineal significativa.\n\n\n\n\n\n\n\nRepaso\n\n\n\nEs conveniente repasar el tema de Análisis de la Varianza estudiado en la asignatura de Inferencia, ya que los conceptos son directamente aplicables aquí.\n\n\nLa idea fundamental del ANOVA es comparar la variabilidad que nuestro modelo explica con la variabilidad que no puede explicar (el error residual). Para ello, se descompone la variabilidad total de nuestras observaciones (\\(y_i\\)) en dos partes ortogonales.\n\nLa Suma Total de Cuadrados (SST) mide la variabilidad total de los datos alrededor de su media. Es nuestra referencia base de la dispersión total que hay que explicar. \\[\n\\text{SST} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n\\]\nEsta variabilidad se descompone en:\n\nSuma de Cuadrados de la Regresión (SSR): Mide la parte de la variabilidad total que es explicada por nuestro modelo. Cuantifica cuánto se desvían las predicciones del modelo (\\(\\hat{y}_i\\)) de la media general (\\(\\bar{y}\\)). \\[\n  \\text{SSR} = \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2\n  \\]\nSuma de Cuadrados del Error (SSE): Mide la variabilidad residual, es decir, la parte que el modelo no puede capturar. Cuantifica la dispersión de los puntos reales (\\(y_i\\)) alrededor de la recta de regresión (\\(\\hat{y}_i\\)). \\[\n  \\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n  \\]\n\n\nLa descomposición fundamental de la varianza es, por tanto: \\(\\text{SST} = \\text{SSR} + \\text{SSE}\\).\nPara poder comparar estas sumas de cuadrados de forma justa, las estandarizamos dividiéndolas por sus respectivos grados de libertad, obteniendo así las Medias Cuadráticas (MS):\n\\[\n\\text{MSR} = \\frac{\\text{SSR}}{1} \\quad \\quad \\quad \\text{MSE} = \\frac{\\text{SSE}}{n-2}\n\\]\nFinalmente, el estadístico F se construye como el cociente entre la variabilidad explicada por el modelo y la variabilidad no explicada:\n\\[\nF = \\frac{\\text{MSR}}{\\text{MSE}}\n\\]\nIntuitivamente, el estadístico F actúa como una ratio de señal a ruido. La MSR (la “señal”) representa la variabilidad que nuestro modelo captura sistemáticamente, mientras que la MSE (el “ruido”) representa la variabilidad aleatoria o residual. Un valor de F grande nos dice que la señal es mucho más fuerte que el ruido, lo que apoya la hipótesis de que la relación que hemos modelado es real y no fruto del azar.\nToda esta información se organiza de forma estándar en la tabla ANOVA:\n\n\n\nFuente\n\\(df\\)\n\\(SS\\)\n\\(MS = SS/df\\)\nEstadístico \\(F\\)\n\n\n\n\nRegresión\n1\n\\(SSR\\)\n\\(MSR\\)\n\\(F = MSR/MSE\\)\n\n\nError\n\\(n-2\\)\n\\(SSE\\)\n\\(MSE\\)\n\n\n\nTotal\n\\(n-1\\)\n\\(SST\\)\n\n\n\n\n\nBajo la hipótesis nula (\\(H_0: \\beta_1 = 0\\)), el estadístico \\(F\\) sigue una distribución \\(F\\) con 1 y \\(n-2\\) grados de libertad. Si el p-valor asociado a nuestro estadístico F es suficientemente pequeño (\\(p &lt; \\alpha\\)), rechazamos \\(H_0\\) y concluimos que nuestro modelo tiene un poder explicativo estadísticamente significativo.\n\n\n2.4.4 Bondad del ajuste: coeficiente de determinación\nEl coeficiente de determinación (\\(R^2\\)) es una medida clave que cuantifica qué proporción de la variabilidad total observada en la muestra (\\(y_i\\)) es explicada por la relación lineal con \\(X\\) a través del modelo. Su fórmula se deriva de la descomposición de la varianza:\n\\[\nR^2 = \\frac{\\text{SSR}}{\\text{SST}} = 1 - \\frac{\\text{SSE}}{\\text{SST}}\n\\]\nDonde las sumas de cuadrados se calculan a partir de los datos muestrales:\n\n\\(\\text{SST} = \\sum_{i=1}^n (y_i - \\bar{y})^2\\): Suma Total de Cuadrados, mide la variabilidad total de las observaciones.\n\\(\\text{SSR} = \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2\\): Suma de Cuadrados de la Regresión, mide la variabilidad explicada por el modelo.\n\\(\\text{SSE} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\): Suma de Cuadrados del Error, mide la variabilidad no explicada (residual).\n\nUn \\(R^2\\) cercano a 1 indica que el modelo ajusta bien los datos, mientras que un \\(R^2\\) cercano a 0 indica un ajuste pobre.\n\n\n\n\n\n\nRelación entre R² y el coeficiente de correlación\n\n\n\n\n\nEn el caso específico del modelo de regresión lineal simple, existe una relación directa y simple: el coeficiente de determinación \\(R^2\\) es literalmente el cuadrado del coeficiente de correlación de Pearson (\\(r\\)) entre \\(X\\) e \\(Y\\).\n\\[ R^2 = (r_{xy})^2 \\]\nEsto refuerza la idea de que ambos miden la fuerza de la asociación lineal, aunque \\(R^2\\) lo hace desde la perspectiva de la varianza explicada por el modelo.\n\n\n\n\n\n\n\n\n\nInterpretación de R²\n\n\n\nEl coeficiente de determinación, \\(R^2\\), es una métrica muy popular, pero su interpretación requiere cautela. Un valor alto no garantiza un buen modelo, y un valor bajo no siempre implica un modelo inútil. Es fundamental tener en cuenta las siguientes observaciones:\n\n\\(R^2\\) no mide la linealidad de la relación. Un modelo puede tener un \\(R^2\\) muy alto incluso si la relación subyacente entre las variables \\(X\\) e \\(Y\\) no es lineal. Por ello, un \\(R^2\\) elevado nunca debe sustituir a un análisis gráfico de los residuos para verificar el supuesto de linealidad.\n\\(R^2\\) es sensible al rango de la variable predictora \\(X\\). Si el modelo de regresión es adecuado, la magnitud de \\(R^2\\) aumentará si aumenta la dispersión de las observaciones \\(x_i\\) (es decir, si \\(S_{xx}\\) crece). Esto se debe a que un mayor rango en \\(X\\) tiende a aumentar la Suma Total de Cuadrados (SST), lo que puede inflar el valor de \\(R^2\\) sin que la precisión del modelo (medida por la MSE) haya mejorado.\nUn rango restringido en \\(X\\) puede producir un \\(R^2\\) artificialmente bajo. Como consecuencia del punto anterior, si los datos se han recogido en un rango muy estrecho de la variable \\(X\\), el \\(R^2\\) puede ser muy pequeño, aunque exista una relación fuerte y significativa entre las variables. Esto podría llevar a la conclusión errónea de que el predictor no es útil.\n\n\n\n\n\n2.4.5 Inferencia sobre los coeficientes\nAdemás de la prueba F global, podemos realizar inferencias sobre cada parámetro individualmente. Para ello, necesitamos el supuesto de normalidad de los errores.\n\n2.4.5.1 Distribución de los estimadores\nBajo el supuesto de normalidad, se puede demostrar que los estimadores también siguen una distribución Normal: \\[\n\\hat{\\beta}_1 \\sim N\\left(\\beta_1, \\frac{\\sigma^2}{S_{xx}}\\right) \\quad \\quad \\quad \\hat{\\beta}_0 \\sim N\\left(\\beta_0, \\sigma^2 \\left[ \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} \\right]\\right)\n\\]\nAl estandarizar y reemplazar la desconocida \\(\\sigma^2\\) por su estimador \\(\\hat{\\sigma}^2 = \\text{MSE}\\), obtenemos un estadístico que sigue una distribución t-Student con \\(n-2\\) grados de libertad: \\[\nt = \\frac{\\hat{\\beta}_1 - \\beta_1}{\\text{SE}(\\hat{\\beta}_1)} \\sim t_{n-2}\n\\] donde \\(\\text{SE}(\\hat{\\beta}_1) = \\sqrt{\\frac{\\text{MSE}}{S_{xx}}}\\) es el error estándar del estimador \\(\\hat{\\beta}_1\\).\n\n\n2.4.5.2 Contraste de hipótesis para la pendiente\nEl contraste más común es el de la significancia de la pendiente: * \\(H_0: \\beta_1 = 0\\) * \\(H_1: \\beta_1 \\neq 0\\)\nBajo \\(H_0\\), el estadístico de contraste es: \\[\nt_0 = \\frac{\\hat{\\beta}_1}{\\text{SE}(\\hat{\\beta}_1)}\n\\] Rechazamos \\(H_0\\) si \\(|t_0| &gt; t_{\\alpha/2, n-2}\\) o, equivalentemente, si el p-valor asociado es menor que \\(\\alpha\\).\n\n\n\n\n\n\nRelación entre el contraste F y el contraste t\n\n\n\nEn el contexto de la regresión lineal simple (y solo en este caso), el contraste F para la significancia global del modelo es matemáticamente equivalente al contraste t para la significancia del coeficiente \\(\\beta_1\\). Se puede demostrar que \\(F = t^2\\), y el p-valor de ambos contrastes será idéntico.\n\n\n\n\n2.4.5.3 Intervalo de confianza para la pendiente\nA partir de la distribución t, podemos construir un intervalo de confianza al \\(100(1-\\alpha)\\%\\) para el verdadero valor de la pendiente \\(\\beta_1\\): \\[\n\\hat{\\beta}_1 \\pm t_{\\alpha/2, n-2} \\cdot \\text{SE}(\\hat{\\beta}_1)\n\\] Este intervalo nos da un rango de valores plausibles para el efecto de \\(X\\) sobre \\(Y\\). Si el intervalo no contiene el cero, es equivalente a rechazar la hipótesis nula \\(H_0: \\beta_1 = 0\\).\n\n\n\n\n\n\nPara recordar\n\n\n\nEn los programas estadísticos se suele proporcionar el p-valor del contraste. Puedes repasar el significado de p-valor proporcionado en la asignatura de Inferencia.\n\n\n\n\n\n\n\n\nEjemplo: Interpretación del summary\n\n\n\n\n\nLa función summary() en R nos proporciona toda esta información.\n\nsummary(modelo_estudio)\n\n\nCall:\nlm(formula = Calificaciones ~ Tiempo_Estudio, data = datos)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.11465 -0.30262 -0.00942  0.29509  1.10533 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     5.00118    0.11977   41.76   &lt;2e-16 ***\nTiempo_Estudio  0.09875    0.00488   20.23   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4842 on 98 degrees of freedom\nMultiple R-squared:  0.8069,    Adjusted R-squared:  0.8049 \nF-statistic: 409.5 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretación:\n\nCoefficients: El p-valor para Tiempo_Estudio (&lt;0.001) es muy pequeño, por lo que rechazamos \\(H_0\\) y concluimos que la variable es un predictor significativo.\nR-squared: El valor de \\(R^2\\) (0.81) nos indica que el 81% de la variabilidad en las calificaciones es explicada por el tiempo de estudio.\nF-statistic: El p-valor del estadístico F (98) confirma que el modelo en su conjunto es estadísticamente significativo.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "index.html#filosofía-pedagógica-del-volumen",
    "href": "index.html#filosofía-pedagógica-del-volumen",
    "title": "Modelos Estadísticos para la Predicción",
    "section": "Filosofía pedagógica del volumen",
    "text": "Filosofía pedagógica del volumen\nLa filosofía que subyace a la obra es un enfoque “teórico-práctico” deliberado y sin concesiones. No nos conformamos con una mera aplicación de “recetas” o una guía de funciones de software. Buscamos fomentar una comprensión profunda del modus operandi de cada modelo y método. Perseguimos un equilibrio entre la técnica estadística y la estrategia de resolución de problemas, bajo la firme convicción de que la labor práctica se desarrolla con mayor fluidez, creatividad y éxito cuando se cimienta en una comprensión robusta de los principios matemáticos y estadísticos subyacentes, tal y como defiende Harrell (2015) en su influyente obra.\n\n¿Qué aprenderás con este libro?\nAl completar este recorrido, habrás desarrollado habilidades clave para:\n\nModelar la dependencia entre una variable respuesta y múltiples predictores en conjuntos de datos complejos.\nResolver problemas con iniciativa y creatividad, eligiendo las técnicas estadísticas más adecuadas para cada caso.\nEvaluar de forma crítica las ventajas e inconvenientes de diferentes alternativas metodológicas.\nImplementar estos modelos utilizando software estadístico profesional como R.\nInterpretar correctamente los resultados, proponer mejoras y tomar decisiones basadas en datos.\nAdquirir las competencias y la autonomía necesarias para emprender con éxito estudios de posgrado o proyectos profesionales en ciencia de datos.\n\nAgradecemos a los profesores y colegas que han contribuido al desarrollo de esta asignatura y a la elaboración de este libro. Su dedicación y conocimiento han sido fundamentales para la creación de este recurso.\nEsperamos que esta guía te resulte útil y enriquecedora.\n¡Comenzamos!\n\n\n\n\n\n\nGrado en Matemáticas\n\n\n\nEste libro presenta el material de la asignatura de Modelos Estadísticos para la Predicción del Grado en Matemáticas de la Universidad Rey Juan Carlos. Su contenido está fuertemente relacionado con las asignaturas de Inferencia Estadística, Análisis de Datos y Métodos Estadísticos.\n\n\n\n\n\n\n\n\nConocimientos previos\n\n\n\nEs altamente recomendable que los alumnos que cursen esta materia manejen con soltura los conocimientos adquiridos en las asignaturas de Probabilidad y Estadística Matemática, así como herramientas de cálculo univariante, multivariante y álgebra lineal.\n\n\n\n\n\n\n\n\nSobre los autores\n\n\n\nVíctor Aceña Gil es graduado en Matemáticas por la UNED, máster en Tratamiento Estadístico y Computacional de la Información por la UCM y la UPM, doctor en Tecnologías de la Información y las Comunicaciones por la URJC y profesor del departamento de Informática y Estadística de la URJC. Miembro del grupo de investigación de alto rendimiento en Fundamentos y Aplicaciones de la Ciencia de Datos, DSLAB, de la URJC. Pertenece al grupo de innovación docente, DSLAB-TI.\nIsaac Martín de Diego es diplomado en Estadística por la Universidad de Valladolid (UVA), licenciado en Ciencias y Técnicas Estadísticas por la Universidad Carlos III de Madrid (UC3M), doctor en Ingeniería Matemática por la UC3M, catedrático de Ciencias de la Computación e Inteligencia Artificial del departamento de Informática y Estadística de la URJC. Es fundador y coordinador del DSLAB y del DSLAB-TI.\n\n\nEsta obra está bajo una licencia de Creative Commons Atribución-CompartirIgual 4.0 Internacional.\n\n\n\n\nHarrell, Frank E., Jr. 2015. Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis. Second. Springer.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "tema1.html#estimación-de-los-parámetros",
    "href": "tema1.html#estimación-de-los-parámetros",
    "title": "2  El modelo de regresión lineal simple",
    "section": "2.3 Estimación de los parámetros",
    "text": "2.3 Estimación de los parámetros\nNecesitamos un método para encontrar la “mejor” recta de ajuste. El Método de Mínimos Cuadrados Ordinarios (MCO/OLS) nos proporciona este criterio.\n\n2.3.1 El criterio de mínimos cuadrados\nMCO busca la recta que minimice la Suma de los Cuadrados del Error (SSE), es decir, la suma de las distancias verticales al cuadrado entre los puntos observados y la recta de regresión:\n\\[\n\\text{SSE}(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} e_i^2 = \\sum_{i=1}^{n} (y_i-\\hat{y})^2 = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_i))^2\n\\]\n\n\n2.3.2 Derivación matemática de los estimadores\nPara encontrar los valores de \\(\\beta_0\\) y \\(\\beta_1\\) que minimizan esta función, recurrimos al cálculo. Tratamos la SSE como una función de dos variables y calculamos sus derivadas parciales, igualándolas a cero para encontrar el mínimo.\n\\[\n\\frac{\\partial \\text{SSE}}{\\partial \\beta_0} = -2 \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i) = 0\n\\]\n\\[\n\\frac{\\partial \\text{SSE}}{\\partial \\beta_1} = -2 \\sum_{i=1}^{n} x_i (y_i - \\beta_0 - \\beta_1 x_i) = 0\n\\]\nLa resolución de este sistema de dos ecuaciones (conocidas como las ecuaciones normales) nos proporciona las fórmulas para los estimadores de MCO:\n\\[\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} = \\frac{s_{xy}}{s_{xx}}\n\\] \\[ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n\\]\n\n2.3.2.1 Interpretación práctica de los coeficientes\nUna vez estimados, los coeficientes tienen una interpretación muy concreta y útil:\n\nPendiente (\\(\\hat{\\beta}_1\\)): Representa el cambio promedio estimado en la variable respuesta \\(Y\\) por cada aumento de una unidad en la variable predictora \\(X\\). En nuestro ejemplo, sería el número de puntos que se espera que aumente la calificación final por cada hora adicional de estudio semanal.\nIntercepto (\\(\\hat{\\beta}_0\\)): Es el valor promedio estimado de la variable respuesta \\(Y\\) cuando la variable predictora \\(X\\) es igual a cero. La interpretación del intercepto solo tiene sentido práctico si \\(X=0\\) es un valor plausible y se encuentra dentro del rango de nuestros datos. De lo contrario (como en nuestro ejemplo, donde nadie estudia 0 horas), a menudo se considera simplemente un ancla matemática para la recta de regresión.\n\n\n\n\n\n\n\nMinimización de SSE\n\n\n\n\n\nLa obtención de los estimadores de mínimos cuadrados para la regresión lineal simple se basa en minimizar la suma de los cuadrados de los residuos (\\(SSE\\)). Aquí está el proceso paso a paso:\nPara minimizar \\(SSE\\), derivamos parcialmente con respecto a \\(\\beta_0\\) y \\(\\beta_1\\) y resolvemos el sistema de ecuaciones.\n\nPrimera derivada con respecto a \\(\\beta_0\\):\n\n\\[\n    \\frac{\\partial SSE}{\\partial \\beta_0} = -2\\sum_{i=1}^n\n    \\left(y_i - (\\beta_0 + \\beta_1 x_i)\\right).\n  \\]\nIgualando a cero: \n\\[\n    \\sum_{i=1}^n \\left(y_i - \\beta_0 - \\beta_1\n    x_i\\right) = 0.\n  \\]\nReordenando: \n\\[\n    n\\beta_0 + \\beta_1 \\sum_{i=1}^n x_i = \\sum_{i=1}^n y_i. \\tag{1}\n  \\]\n\nPrimera derivada con respecto a \\(\\beta_1\\):\n\n\\[\n    \\frac{\\partial SSE}{\\partial \\beta_1} = -2\\sum_{i=1}^n x_i\n    \\left(y_i - (\\beta_0 + \\beta_1 x_i)\\right).\n    \\]\nIgualando a cero: \n\\[\n    \\sum_{i=1}^n x_i \\left(y_i - \\beta_0 -\n    \\beta_1 x_i\\right) = 0.\n   \\]\nReordenando: \n\\[\n    \\beta_0 \\sum_{i=1}^n x_i + \\beta_1 \\sum_{i=1}^n x_i^2 = \\sum_{i=1}^n x_i y_i. \\tag{2}\n   \\]\nResolución del Sistema de Ecuaciones\nEl sistema está dado por las ecuaciones (1) y (2):\n\n\\(n\\beta_0 + \\beta_1 \\sum_{i=1}^n x_i = \\sum_{i=1}^n y_i.\\)\n\n\\(\\beta_0 \\sum_{i=1}^n x_i + \\beta_1 \\sum_{i=1}^n x_i^2 = \\sum_{i=1}^n x_i y_i.\\)\n\nResolviendo para \\(\\beta_0\\) y \\(\\beta_1\\):\n\nDe la primera ecuación, despejamos \\(\\beta_0\\):\n\\[\n\\beta_0 = \\frac{\\sum_{i=1}^n y_i - \\beta_1 \\sum_{i=1}^n x_i}{n}. \\tag{3}\n\\]\nSustituimos \\(\\beta_0\\) en la segunda ecuación:\n\\[\n\\frac{\\sum_{i=1}^n y_i - \\beta_1 \\sum_{i=1}^n x_i}{n} \\sum_{i=1}^n x_i + \\beta_1 \\sum_{i=1}^n x_i^2 = \\sum_{i=1}^n x_i y_i.\n\\]\nSimplificando:\n\\[\n\\beta_1 \\left(\\sum_{i=1}^n x_i^2 - \\frac{(\\sum_{i=1}^n x_i)^2}{n}\\right) = \\sum_{i=1}^n x_i y_i - \\frac{\\sum_{i=1}^n x_i \\sum_{i=1}^n y_i}{n}.\n\\]\nExpresamos \\(\\beta_1\\):\n\\[\n\\beta_1 = \\frac{\\sum_{i=1}^n x_i y_i - \\frac{\\sum_{i=1}^n x_i \\sum_{i=1}^n y_i}{n}}{\\sum_{i=1}^n x_i^2 - \\frac{(\\sum_{i=1}^n x_i)^2}{n}}.\n\\] Esta es la fórmula para \\(\\beta_1\\), que puede reescribirse como:\n\\[\n\\beta_1 = \\frac{\\text{Cov}(x, y)}{\\text{Var}(x)},\n\\] donde \\(\\text{Cov}(x, y)\\) y \\(\\text{Var}(x)\\) son la covarianza y la varianza muestral de \\(x\\) y \\(y\\).\nFinalmente, sustituimos \\(\\beta_1\\) en la ecuación (3) para obtener \\(\\beta_0\\):\n\\[\n\\beta_0 = \\bar{y} - \\beta_1 \\bar{x},\n\\] donde \\(\\bar{x}\\) y \\(\\bar{y}\\) son las medias de \\(x\\) y \\(y\\).\n\n\n\n\nBajo los supuestos del modelo, el Teorema de Gauss-Markov demuestra que estos estimadores son los Mejores Estimadores Lineales Insesgados (MELI / BLUE).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema1.html#ajuste-inferencia-y-bondad-de-ajuste-del-modelo",
    "href": "tema1.html#ajuste-inferencia-y-bondad-de-ajuste-del-modelo",
    "title": "2  El modelo de regresión lineal simple",
    "section": "2.4 Ajuste, Inferencia y Bondad de Ajuste del Modelo",
    "text": "2.4 Ajuste, Inferencia y Bondad de Ajuste del Modelo\nAhora que hemos definido el modelo y el método de estimación, es el momento de aplicar todo el proceso en R y, lo que es más importante, de interpretar los resultados para evaluar la significancia y la utilidad de nuestro modelo.\n\n2.4.1 Implementación e Interpretación del summary en R\nLa función lm() ajusta el modelo, y la función summary() nos proporciona una radiografía completa del mismo, incluyendo la estimación de los coeficientes, las pruebas de hipótesis y las medidas de bondad de ajuste.\n\n\n\n\n\n\nEjemplo: Análisis completo del modelo de estudio\n\n\n\nAjustamos el modelo a nuestros datos y examinamos su resumen completo.\n\n# 1. Ajustamos el modelo lineal\nmodelo_estudio &lt;- lm(Calificaciones ~ Tiempo_Estudio, data = datos)\n\n# 2. Obtenemos el resumen completo del modelo\nsummary(modelo_estudio)\n\n\nCall:\nlm(formula = Calificaciones ~ Tiempo_Estudio, data = datos)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.11465 -0.30262 -0.00942  0.29509  1.10533 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     5.00118    0.11977   41.76   &lt;2e-16 ***\nTiempo_Estudio  0.09875    0.00488   20.23   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4842 on 98 degrees of freedom\nMultiple R-squared:  0.8069,    Adjusted R-squared:  0.8049 \nF-statistic: 409.5 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretación detallada de la salida:\n\nCoefficients: Esta tabla es el corazón de la inferencia sobre los parámetros.\n\nEstimate: Contiene los valores de \\(\\hat{\\beta}_0\\) (5) y \\(\\hat{\\beta}_1\\) (0.1). La recta de regresión muestral es: Calificaciones = 5 + 0.1 * Tiempo_Estudio.\nStd. Error: Es el error estándar de cada estimador, \\(\\text{SE}(\\hat{\\beta}_j)\\), que mide su variabilidad muestral.\nt value: Es el valor del estadístico t para el contraste de hipótesis \\(H_0: \\beta_j = 0\\).\nPr(&gt;|t|): Es el p-valor asociado. El p-valor para Tiempo_Estudio es extremadamente pequeño (&lt;0.001), lo que nos da una fuerte evidencia para rechazar \\(H_0\\) y concluir que la variable es un predictor estadísticamente significativo.\n\nResidual standard error: Es el valor de \\(\\hat{\\sigma}\\) (0.48), la estimación de la desviación estándar del error. Nos dice que, en promedio, las predicciones del modelo se desvían unos 0.48 puntos de las calificaciones reales.\nR-squared: El coeficiente de determinación \\(R^2\\) es 0.81. Esto significa que el 81% de la variabilidad total en las calificaciones es explicada por el tiempo de estudio.\nF-statistic: Proporciona el resultado del test F de significancia global. El p-valor (98) confirma la conclusión del test t: el modelo en su conjunto es estadísticamente significativo.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema1.html#predicción-de-nuevas-observaciones",
    "href": "tema1.html#predicción-de-nuevas-observaciones",
    "title": "2  El modelo de regresión lineal simple",
    "section": "2.5 Predicción de nuevas observaciones",
    "text": "2.5 Predicción de nuevas observaciones\nUna vez que hemos ajustado y validado un modelo de regresión, uno de sus propósitos más importantes es utilizarlo para hacer predicciones. Sin embargo, es fundamental distinguir entre dos tipos de predicción:\n\nEstimar la respuesta media para un valor dado de \\(X\\). Por ejemplo: “¿Cuál es la calificación promedio que esperamos para todos los estudiantes que estudian 25 horas semanales?”.\nPredecir una respuesta individual para un valor dado de \\(X\\). Por ejemplo: “Si un estudiante concreto estudia 25 horas semanales, ¿entre qué valores esperamos que se encuentre su calificación?”.\n\nEstos dos objetivos, aunque parecidos, responden a preguntas distintas y manejan diferentes fuentes de incertidumbre, lo que da lugar a dos tipos de intervalos.\n\n2.5.1 Intervalo de confianza para la respuesta media\nEste intervalo estima el valor esperado de \\(Y\\) para un valor concreto del regresor, \\(x_0\\). Su objetivo es acotar dónde se encuentra la línea de regresión poblacional verdadera para ese punto \\(x_0\\). La estimación puntual es \\(\\hat{y}_0 = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0\\).\nEl intervalo de confianza al \\(100(1-\\alpha)\\%\\) para la respuesta media \\(E[Y|X=x_0]\\) viene dado por:\n\\[\n\\hat{y}_0 \\pm t_{\\alpha/2, n-2} \\cdot \\sqrt{\\text{MSE} \\left( \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{S_{xx}} \\right)}\n\\]\nLa anchura de este intervalo depende de dos fuentes de error: la incertidumbre en la estimación de la recta y la distancia del punto \\(x_0\\) a la media \\(\\bar{x}\\). El intervalo es más estrecho cerca del centro de los datos y más ancho en los extremos.\n\n\n2.5.2 Intervalo de predicción para una respuesta individual\nEste intervalo es el que debemos usar cuando queremos predecir el valor para una única observación futura, no para la media. Como indicas, este intervalo debe tener en cuenta dos fuentes de variabilidad:\n\nLa incertidumbre sobre la localización de la verdadera recta de regresión (la misma que en el intervalo de confianza).\nLa variabilidad inherente de una observación individual alrededor de la recta de regresión (el error aleatorio \\(\\varepsilon_i\\), cuya varianza estimamos con la MSE).\n\nPor esta razón, el intervalo de predicción siempre será más ancho que el intervalo de confianza para la respuesta media. El intervalo de predicción al \\(100(1-\\alpha)\\%\\) para una observación futura \\(y_0\\) en el punto \\(x_0\\) es:\n\\[\n\\hat{y}_0 \\pm t_{\\alpha/2, n-2} \\cdot \\sqrt{\\text{MSE} \\left( 1 + \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{S_{xx}} \\right)}\n\\]\nLa única diferencia matemática es el “+1” dentro de la raíz cuadrada, que representa la varianza \\(\\sigma^2\\) del error de una sola observación.\n\n2.5.2.1 Predicción para la media de m observaciones futuras\nSi se desea un intervalo de predicción para la media de m futuras observaciones en un valor \\(x_0\\), la fórmula se modifica ligeramente. Este intervalo será más estrecho que el de una sola observación pero más ancho que el de la respuesta media:\n\\[\n\\hat{y}_0 \\pm t_{\\alpha/2, n-2} \\cdot \\sqrt{\\text{MSE} \\left( \\frac{1}{m} + \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{S_{xx}} \\right)}\n\\]\n\n\n\n\n\n\nEjemplo práctico: Predicción de calificaciones\n\n\n\nVamos a calcular y visualizar los intervalos para nuestro modelo de estudio. Usaremos la función predict() de R, que calcula estos intervalos de forma automática.\n\n# 1. Crear una secuencia de nuevos valores de X para predecir\nnuevos_datos &lt;- data.frame(\n  Tiempo_Estudio = seq(min(datos$Tiempo_Estudio), max(datos$Tiempo_Estudio), length.out = 100)\n)\n\n# 2. Calcular el intervalo de confianza para la RESPUESTA MEDIA\nconf_interval &lt;- predict(\n  modelo_estudio, \n  newdata = nuevos_datos, \n  interval = \"confidence\", \n  level = 0.95\n)\n\n# 3. Calcular el intervalo de predicción para una OBSERVACIÓN INDIVIDUAL\npred_interval &lt;- predict(\n  modelo_estudio, \n  newdata = nuevos_datos, \n  interval = \"prediction\", \n  level = 0.95\n)\n\n# 4. Unir todo para graficar con ggplot2\nplot_data &lt;- cbind(nuevos_datos, as.data.frame(conf_interval), pred_pred = as.data.frame(pred_interval))\ncolnames(plot_data) &lt;- c(\"Tiempo_Estudio\", \"fit_conf\", \"lwr_conf\", \"upr_conf\", \"fit_pred\", \"lwr_pred\", \"upr_pred\")\n\n# 5. Visualización\nggplot() +\n  # Capa 1: Puntos originales del dataframe 'datos'\n  geom_point(data = datos, aes(x = Tiempo_Estudio, y = Calificaciones), color = \"#0072B2\", alpha = 0.7) +\n  \n  # Capa 2: Línea de regresión del dataframe 'plot_data'\n  geom_line(data = plot_data, aes(x = Tiempo_Estudio, y = fit_conf), color = \"black\", linewidth = 1) +\n  \n  # Capa 3: Banda de predicción (roja) del dataframe 'plot_data'\n  geom_ribbon(data = plot_data, aes(x = Tiempo_Estudio, ymin = lwr_pred, ymax = upr_pred), fill = \"red\", alpha = 0.2) +\n  \n  # Capa 4: Banda de confianza (azul) del dataframe 'plot_data'\n  geom_ribbon(data = plot_data, aes(x = Tiempo_Estudio, ymin = lwr_conf, ymax = upr_conf), fill = \"blue\", alpha = 0.3) +\n  \n  # Etiquetas y tema\n  labs(\n    title = \"Intervalos de Confianza y Predicción\",\n    x = \"Tiempo de Estudio (horas/semana)\",\n    y = \"Calificaciones (promedio)\",\n    caption = \"La banda azul (más estrecha) es el IC del 95% para la media.\\nLa banda roja (más ancha) es el IP del 95% para una nueva observación.\"\n  ) +\n  theme_classic(base_size = 14)\n\n\n\n\n\n\n\nFigura 2.2: Comparación visual del intervalo de confianza (azul, más estrecho) y el intervalo de predicción (rojo, más ancho).\n\n\n\n\n\nEl gráfico muestra claramente que la incertidumbre al predecir una calificación individual es mucho mayor que la incertidumbre al estimar la calificación promedio. Ambas bandas se ensanchan al alejarse del centro de los datos.\nSi quisiéramos una predicción para un estudiante que estudia 25 horas:\n\ndato_nuevo &lt;- data.frame(Tiempo_Estudio = 25)\n\n# Guardamos la predicción para la media en un objeto\npred_media &lt;- predict(modelo_estudio, newdata = dato_nuevo, interval = \"confidence\")\n\n# Guardamos la predicción para un individuo en un objeto\npred_indiv &lt;- predict(modelo_estudio, newdata = dato_nuevo, interval = \"prediction\")\n\nInterpretación:\n\nCon un 95% de confianza, la calificación promedio de los estudiantes que estudian 25 horas está entre 7.37 y 7.57.\nCon un 95% de confianza, la calificación de un estudiante concreto que estudia 25 horas estará entre 6.5 y 8.44.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "tema1.html#diagnóstico-del-modelo-verificación-de-los-supuestos",
    "href": "tema1.html#diagnóstico-del-modelo-verificación-de-los-supuestos",
    "title": "2  El modelo de regresión lineal simple",
    "section": "2.6 Diagnóstico del Modelo: Verificación de los Supuestos",
    "text": "2.6 Diagnóstico del Modelo: Verificación de los Supuestos\nUna vez que hemos ajustado un modelo y evaluado su significancia, el trabajo no ha terminado. Un paso crucial, a menudo subestimado, es el diagnóstico del modelo. Este proceso consiste en verificar si se cumplen los supuestos del modelo de regresión lineal clásico. La fiabilidad de nuestras inferencias (los p-valores de los contrastes t y F, y los intervalos de confianza) depende directamente de la validez de estos supuestos.\nEl diagnóstico se realiza principalmente a través del análisis de los residuos del modelo (\\(e_i = y_i - \\hat{y}_i\\)). Los residuos son nuestra mejor aproximación empírica de los errores teóricos no observables (\\(\\varepsilon_i\\)). A continuación, se detalla cómo verificar cada uno de los supuestos clave.\n\n\n2.6.1 1. Linealidad\nEste supuesto establece que la relación entre la variable predictora \\(X\\) y el valor esperado de la variable respuesta \\(Y\\) es, en promedio, una línea recta: \\(E[Y | X] = \\beta_0 + \\beta_1 X\\).\n\n2.6.1.1 Evaluación Visual\n\nGráfico de Residuos vs. Valores Ajustados (Residuals vs. Fitted): Este es el gráfico más importante para evaluar la linealidad.\n\nQué buscar: Una nube de puntos distribuida de manera aleatoria y sin patrones, centrada horizontalmente alrededor de la línea cero. Esto indica que los residuos son independientes de los valores predichos, lo cual es consistente con una relación lineal.\nSeñal de violación: Si observas un patrón claro, como una curva (forma de “U” o “U” invertida), significa que el modelo lineal no está capturando adecuadamente la relación y un modelo no lineal podría ser más apropiado.\n\n\n\n\n\n\n2.6.2 2. Homocedasticidad (Varianza Constante)\nLa homocedasticidad implica que la varianza de los errores es constante para todos los niveles de la variable predictora \\(X\\), es decir, \\(Var(\\varepsilon_i | X_i) = \\sigma^2\\). La violación de este supuesto se denomina heteroscedasticidad.\n\n2.6.2.1 Evaluación Visual\n\nGráfico de Residuos vs. Valores Ajustados: Este gráfico también es clave aquí.\n\nQué buscar: La dispersión vertical de los puntos debe ser constante a lo largo del eje horizontal.\nSeñal de violación: Una forma de embudo o megáfono, donde la dispersión de los residuos aumenta o disminuye a medida que los valores ajustados cambian.\n\nGráfico de Scale-Location: Este gráfico muestra la raíz cuadrada de los residuos estandarizados (\\(\\sqrt{|e_i standardized|}\\)) frente a los valores ajustados.\n\nQué buscar: Una línea de tendencia roja que sea aproximadamente horizontal y plana.\n\n\n\n\n2.6.2.2 Prueba Analítica\n\nPrueba de Breusch-Pagan: Es un contraste formal para la homocedasticidad. Regresiona los residuos al cuadrado sobre las variables predictoras y comprueba si estas últimas explican la variabilidad de los residuos.\n\nHipótesis Nula (\\(H_0\\)): El modelo es homocedástico.\nDecisión: Un p-valor pequeño (p. ej., &lt; 0.05) sugiere rechazar \\(H_0\\) y concluir que existe heteroscedasticidad.\n\n\n\n\n\n\n2.6.3 3. Normalidad de los Errores\nEste supuesto postula que los errores del modelo (\\(\\varepsilon_i\\)) siguen una distribución normal: \\(\\varepsilon_i \\sim N(0, \\sigma^2)\\). Es especialmente importante para la validez de los intervalos de confianza y los contrastes de hipótesis cuando el tamaño de la muestra es pequeño.\n\n2.6.3.1 Evaluación Visual\n\nGráfico Normal Q-Q (Normal Q-Q Plot): Compara los cuantiles de los residuos estandarizados con los cuantiles de una distribución normal teórica.\n\nQué buscar: Los puntos deben caer muy cerca de la línea diagonal de 45 grados.\n\nHistograma de los Residuos: Un simple histograma de los residuos debe mostrar una forma aproximada de campana de Gauss.\n\n\n\n2.6.3.2 Prueba Analítica\n\nPrueba de Shapiro-Wilk: Es uno de los contrastes más potentes para la normalidad.\n\nHipótesis Nula (\\(H_0\\)): Los residuos provienen de una distribución normal.\nDecisión: Un p-valor pequeño (&lt; 0.05) sugiere rechazar \\(H_0\\).\n\n\n\n\n\n\n2.6.4 4. Independencia de los Errores\nEste supuesto afirma que el error de una observación no está correlacionado con el de ninguna otra: \\(Cov(\\varepsilon_i, \\varepsilon_j) = 0\\) para \\(i \\neq j\\). La violación, conocida como autocorrelación, es común en datos de series temporales.\n\n2.6.4.1 Evaluación\n\nPrueba de Durbin-Watson: Es el contraste clásico para la autocorrelación de primer orden. Su estadístico se calcula como: \\[ DW = \\frac{\\sum_{i=2}^{n}(e_i - e_{i-1})^2}{\\sum_{i=1}^{n}e_i^2} \\] El estadístico varía entre 0 y 4. Un valor cercano a 2 sugiere no autocorrelación. Valores cercanos a 0 indican autocorrelación positiva, y cercanos a 4, autocorrelación negativa.\n\n\n\n\n\n2.6.5 5. Identificación de Observaciones Influyentes y Atípicas\nAlgunos puntos pueden tener una influencia desproporcionada en el modelo. Es crucial identificarlos usando tres métricas clave:\n\nApalancamiento (Leverage, \\(h_{ii}\\)): Mide cuán atípico es el valor de la variable predictora \\(X_i\\) de una observación. Un apalancamiento alto significa que el punto tiene el potencial de ser muy influyente. En regresión simple, se calcula como: \\[ h_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n}(x_j - \\bar{x})^2} \\] Una regla común es considerar un apalancamiento alto si \\(h_{ii} &gt; \\frac{2(k+1)}{n}\\), donde \\(k\\) es el número de predictores (1 en regresión simple).\nResiduos Estudentizados: Son una versión de los residuos estandarizados que son más efectivos para detectar outliers. Un residuo estudentizado (\\(r_i\\)) sigue una distribución t de Student con \\(n-k-2\\) grados de libertad. Valores \\(|r_i| &gt; 2\\) o \\(3\\) a menudo se consideran atípicos.\nDistancia de Cook (\\(D_i\\)): Mide la influencia global de una observación, combinando su apalancamiento y su residuo. Representa cuánto cambian los coeficientes del modelo si la i-ésima observación es eliminada. \\[ D_i = \\frac{r_i^2}{k+1} \\cdot \\frac{h_{ii}}{1-h_{ii}} \\] Se considera que un punto es influyente si su distancia de Cook es grande, por ejemplo, si \\(D_i &gt; 1\\) o \\(D_i &gt; 4/(n-k-1)\\).\n\nEl gráfico Residuals vs. Leverage es la herramienta visual que combina estas tres métricas, facilitando la identificación de puntos problemáticos.\n\n\n2.6.6 Aplicación Práctica en R\nA continuación, se realizan las pruebas de diagnóstico para nuestro modelo_estudio.\n\n# Gráficos de diagnóstico visual estándar\npar(mfrow = c(2, 2))\nplot(modelo_estudio)\npar(mfrow = c(1, 1))\n\n# Para las pruebas analíticas, usamos el paquete 'lmtest'\n# Si no está instalado, ejecuta: install.packages(\"lmtest\")\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n# 1. Prueba de Breusch-Pagan para homocedasticidad\nbptest(modelo_estudio)\n\n\n    studentized Breusch-Pagan test\n\ndata:  modelo_estudio\nBP = 0.019638, df = 1, p-value = 0.8886\n\n# 2. Prueba de Shapiro-Wilk para normalidad (sobre los residuos)\nshapiro.test(residuals(modelo_estudio))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(modelo_estudio)\nW = 0.99008, p-value = 0.671\n\n# 3. Prueba de Durbin-Watson para independencia\ndwtest(modelo_estudio)\n\n\n    Durbin-Watson test\n\ndata:  modelo_estudio\nDW = 2.0565, p-value = 0.6104\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\n\n\n\n\n\nFigura 2.3: Gráficos de diagnóstico estándar generados por R.\n\n\n\n\n\n\n\n\n\nDraper, NR. 1998. Applied regression analysis. McGraw-Hill. Inc.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>El modelo de regresión lineal simple</span>"
    ]
  }
]