<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Métodos de selección de variables y problemas de regularización – Modelos Estadísticos para la Predicción</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./tema3.html" rel="next">
<link href="./tema1.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-1b90310288666db1f83ae9fc236520cd.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Sin resultados",
    "search-matching-documents-text": "documentos encontrados",
    "search-copy-link-title": "Copiar el enlace en la búsqueda",
    "search-hide-matches-text": "Ocultar resultados adicionales",
    "search-more-match-text": "resultado adicional en este documento",
    "search-more-matches-text": "resultados adicionales en este documento",
    "search-clear-button-title": "Borrar",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Enviar",
    "search-label": "Buscar"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./tema2.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Métodos de selección de variables y problemas de regularización</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Buscar" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Modelos Estadísticos para la Predicción</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Buscar"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefacio</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tema0.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introducción a los modelos de regresión</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tema1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">El modelo de regresión lineal simple</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tema2.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Métodos de selección de variables y problemas de regularización</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tema3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Modelos no lineales. Transformación de variables. Ingeniería de características.</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tema4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Modelos de regresión generalizada</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tema5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Otros modelos de regresión: Modelos Aditivos Generalizados (GAMs)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./conclusiones.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Conclusiones</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliografía</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Tabla de contenidos</h2>
   
  <ul>
  <li><a href="#proceso-de-construcción-del-modelo-de-regresión" id="toc-proceso-de-construcción-del-modelo-de-regresión" class="nav-link active" data-scroll-target="#proceso-de-construcción-del-modelo-de-regresión"><span class="header-section-number">3.1</span> Proceso de construcción del modelo de regresión</a></li>
  <li><a href="#reducción-de-variables" id="toc-reducción-de-variables" class="nav-link" data-scroll-target="#reducción-de-variables"><span class="header-section-number">3.2</span> Reducción de variables</a>
  <ul class="collapse">
  <li><a href="#motivaciones-para-reducir-variables" id="toc-motivaciones-para-reducir-variables" class="nav-link" data-scroll-target="#motivaciones-para-reducir-variables"><span class="header-section-number">3.2.1</span> Motivaciones para reducir variables</a></li>
  <li><a href="#métodos-de-reducción-de-variables" id="toc-métodos-de-reducción-de-variables" class="nav-link" data-scroll-target="#métodos-de-reducción-de-variables"><span class="header-section-number">3.2.2</span> Métodos de reducción de variables</a></li>
  </ul></li>
  <li><a href="#selección-de-variables" id="toc-selección-de-variables" class="nav-link" data-scroll-target="#selección-de-variables"><span class="header-section-number">3.3</span> Selección de variables</a></li>
  <li><a href="#métodos-de-selección-directa" id="toc-métodos-de-selección-directa" class="nav-link" data-scroll-target="#métodos-de-selección-directa"><span class="header-section-number">3.4</span> Métodos de selección directa</a></li>
  <li><a href="#métodos-automáticos" id="toc-métodos-automáticos" class="nav-link" data-scroll-target="#métodos-automáticos"><span class="header-section-number">3.5</span> Métodos automáticos</a></li>
  <li><a href="#métodos-basados-en-regularización" id="toc-métodos-basados-en-regularización" class="nav-link" data-scroll-target="#métodos-basados-en-regularización"><span class="header-section-number">3.6</span> Métodos basados en regularización</a>
  <ul class="collapse">
  <li><a href="#ridge-regression" id="toc-ridge-regression" class="nav-link" data-scroll-target="#ridge-regression"><span class="header-section-number">3.6.1</span> Ridge regression</a></li>
  <li><a href="#regresión-lasso" id="toc-regresión-lasso" class="nav-link" data-scroll-target="#regresión-lasso"><span class="header-section-number">3.6.2</span> Regresión Lasso</a></li>
  <li><a href="#elastic-net" id="toc-elastic-net" class="nav-link" data-scroll-target="#elastic-net"><span class="header-section-number">3.6.3</span> Elastic Net</a></li>
  <li><a href="#comparación-de-los-métodos-de-regularización" id="toc-comparación-de-los-métodos-de-regularización" class="nav-link" data-scroll-target="#comparación-de-los-métodos-de-regularización"><span class="header-section-number">3.6.4</span> Comparación de los métodos de Regularización</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-tema2" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Métodos de selección de variables y problemas de regularización</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>En los modelos de regresión, especialmente cuando se trabaja con conjuntos de datos que incluyen un gran número de variables predictoras, es común enfrentarse al desafío de identificar qué variables son realmente relevantes para explicar la variable respuesta. La inclusión de demasiadas variables en un modelo puede llevar a problemas como el sobreajuste, pérdida de interpretabilidad y complejidad innecesaria, mientras que la exclusión de variables importantes puede resultar en modelos subóptimos.</p>
<section id="proceso-de-construcción-del-modelo-de-regresión" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="proceso-de-construcción-del-modelo-de-regresión"><span class="header-section-number">3.1</span> Proceso de construcción del modelo de regresión</h2>
<p>La construcción de un modelo de regresión múltiple es un proceso sistemático que busca explicar la relación entre una variable respuesta (<span class="math inline">\(Y\)</span>) y múltiples variables predictoras (<span class="math inline">\(X_1, X_2, \dots, X_k\)</span>). Este proceso consta de varias etapas clave <span class="citation" data-cites="kutner2005applied">(<a href="references.html#ref-kutner2005applied" role="doc-biblioref">Kutner et&nbsp;al. 2005</a>)</span>:</p>
<ol type="1">
<li><strong>Definición del problema y variables de interés:</strong>
<ul>
<li>Identificar claramente el objetivo del análisis, ya sea realizar predicciones, evaluar relaciones o controlar por efectos de variables confusoras.</li>
<li>Seleccionar las variables predictoras potenciales en función de su relevancia teórica, conocimiento previo o exploración inicial de los datos.</li>
</ul></li>
<li><strong>Recogida de datos:</strong></li>
</ol>
<ul>
<li>La calidad de los datos recogidos influye directamente en la validez de los resultados y conclusiones obtenidas. El proceso de recogida de datos consiste en recopilar información de manera organizada y sistemática para responder a las preguntas de investigación planteadas. Dependiendo del diseño del estudio y los objetivos del análisis, se pueden emplear diferentes tipos de experimentos o métodos de recogida de datos.</li>
<li>Debemos asegurar las siguientes características sobre los datos.
<ul>
<li><strong>Fiabilidad:</strong> Asegurar que los datos sean consistentes y puedan reproducirse bajo condiciones similares.</li>
<li><strong>Validez:</strong> Garantizar que los datos recojan realmente la información necesaria para responder a las preguntas de investigación.</li>
<li><strong>Ética:</strong> Asegurar la privacidad y el consentimiento informado de los participantes.</li>
<li><strong>Control de Sesgos:</strong> Diseñar el estudio de manera que se minimicen los sesgos que puedan distorsionar los resultados.</li>
</ul></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled" title="Tipos de experimentos">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tipos de experimentos
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>La elección del tipo de experimento o método de recogida de datos dependerá de la naturaleza del problema a investigar, los recursos disponibles y las limitaciones del estudio. Una correcta planificación y ejecución de esta etapa sienta las bases para un análisis robusto y confiable.</p>
<ol type="1">
<li><strong>Experimentos controlados:</strong>
<ul>
<li>Los experimentos controlados son diseñados de manera que los investigadores manipulan deliberadamente una o más variables independientes (llamadas factores o variables controladas) para observar su efecto en la variable dependiente.</li>
<li>Incluyen la aleatorización de sujetos entre grupos (por ejemplo, grupos de control y tratamiento) para minimizar sesgos y asegurar comparabilidad.</li>
<li>En muchas ocasiones la información suplementaria no se puede incorporar en el diseño del experimento. A esas variables, no controladas, se les suel llamar covariables.</li>
<li><strong>Ejemplo:</strong> Un estudio clínico donde se prueba un nuevo medicamento y se compara su efecto con un placebo.</li>
</ul></li>
<li><strong>Estudios observacionales exploratorios:</strong>
<ul>
<li>En este enfoque, los datos se recogen sin intervenir ni manipular las condiciones. Los investigadores observan y registran los fenómenos tal como ocurren en la naturaleza.</li>
<li>Pueden clasificarse en:
<ul>
<li><strong>Estudios transversales:</strong> Los datos se recogen en un único punto temporal.</li>
<li><strong>Estudios longitudinales:</strong> Los datos se recogen durante un periodo para analizar cambios a lo largo del tiempo.</li>
</ul></li>
<li><strong>Ejemplo:</strong> Investigar los hábitos alimenticios y su asociación con enfermedades cardiovasculares en una población.</li>
</ul></li>
<li><strong>Estudios observacionales confirmatorios:</strong>
<ul>
<li>En este enfoque, los datos se recogen para testear (confirmar o no) hipótesis derivadas de estudios previos o de ideas que pueden tener los investigadores.</li>
<li>En este contexto, las variables que aparecen involucradas en la hipótesis que se quiere confirmar se denominan variables primarias, y las variables explicativas que se sabe inluyen en la respuesta se llaman variables de control (en Epidemiología nos referimos a ellas como factores de riesgo)</li>
<li><strong>Ejemplo:</strong> Un equipo de investigadores, basándose en estudios previos, plantea la hipótesis de que existe una relación positiva entre el hábito de fumar (variable explicativa principal) y la incidencia de cáncer de pulmón (variable respuesta). Para confirmar esta hipótesis, realizan un estudio observacional en el que recopilan datos de una población durante un periodo determinado. Dado que no es ético inducir a las personas a fumar para realizar un experimento controlado, este estudio se realiza de forma observacional. Los datos se analizan para evaluar la asociación entre las variables, permitiendo confirmar (o refutar) la hipótesis planteada con un diseño adecuado y controlando los posibles factores de confusión.</li>
</ul></li>
<li><strong>Encuestas y cuestionarios:</strong>
<ul>
<li>Las encuestas son una técnica común para recoger datos de manera estructurada sobre actitudes, opiniones, comportamientos o características demográficas.</li>
<li>Pueden aplicarse en formato presencial, en línea, por teléfono o mediante correo.</li>
<li><strong>Ejemplo:</strong> Una encuesta para medir el grado de satisfacción de los clientes con un servicio.</li>
</ul></li>
<li><strong>Experimentos naturales:</strong>
<ul>
<li>Se producen cuando un fenómeno natural o social actúa como una intervención en un entorno sin que los investigadores tengan control sobre el experimento.</li>
<li>Este tipo de estudio aprovecha eventos únicos para analizar sus impactos.</li>
<li><strong>Ejemplo:</strong> Estudiar los efectos económicos de una nueva política fiscal aplicada en una región específica.</li>
</ul></li>
<li><strong>Estudios de simulación:</strong>
<ul>
<li>Los datos se generan a través de modelos matemáticos o computacionales que representan un sistema real o hipotético.</li>
<li>Este método se usa cuando es difícil o costoso realizar experimentos reales.</li>
<li><strong>Ejemplo:</strong> Simular el comportamiento de un mercado financiero bajo diferentes escenarios económicos.</li>
</ul></li>
<li><strong>Recogida de datos secundarios:</strong>
<ul>
<li>En lugar de recoger datos nuevos, se utilizan datos ya existentes recopilados por terceros, como censos, registros administrativos o bases de datos públicas.</li>
<li>Aunque es eficiente en tiempo y costos, el investigador tiene menor control sobre la calidad y las características de los datos.</li>
<li><strong>Ejemplo:</strong> Analizar datos de encuestas nacionales para estudiar tendencias sociales.</li>
</ul></li>
</ol>
</div>
</div>
</div>
<ol start="3" type="1">
<li><strong>Análisis Exploratorio de Datos (EDA):</strong>
<ul>
<li>Inspeccionar los datos mediante análisis descriptivo y visual para identificar posibles problemas como valores atípicos, datos faltantes y multicolinealidad.</li>
<li>Escalar o transformar las variables si es necesario, especialmente si están en diferentes escalas o presentan distribuciones no lineales.</li>
</ul></li>
<li><strong>Ajuste del modelo:</strong>
<ul>
<li>Especificar el modelo de regresión múltiple en su forma general:<br>
<span class="math display">\[
Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p + \varepsilon,
\]</span> donde <span class="math inline">\(\varepsilon\)</span> representa los errores aleatorios.</li>
<li>Estimar los coeficientes del modelo (<span class="math inline">\(\beta_0, \beta_1, \dots, \beta_p\)</span>) utilizando el método de mínimos cuadrados, que minimiza la suma de los errores al cuadrado.</li>
</ul></li>
<li><strong>Evaluación del modelo:</strong>
<ul>
<li>Analizar el ajuste general del modelo utilizando métricas como <span class="math inline">\(R^2\)</span> y <span class="math inline">\(R^2\)</span> ajustado, que miden la proporción de la variabilidad explicada.</li>
<li>Examinar la tabla ANOVA para evaluar la significancia global del modelo.</li>
<li>Realizar pruebas de hipótesis para los coeficientes individuales, verificando si las variables predictoras tienen un efecto significativo en la variable respuesta.</li>
</ul></li>
<li><strong>Diagnóstico del modelo:</strong>
<ul>
<li>Examinar los residuos para evaluar supuestos como la linealidad, homocedasticidad, normalidad de los errores y ausencia de autocorrelación.</li>
<li>Identificar observaciones atípicas, leverage y puntos de influencia utilizando herramientas como la distancia de Cook, DFBETAS y DFFITS.</li>
</ul></li>
<li><strong>Reducción de variables:</strong>
<ul>
<li>En análisis de regresión, especialmente cuando se trabaja con conjuntos de datos de alta dimensionalidad, es común enfrentar situaciones en las que el número de variables explicativas es muy grande. Esto puede llevar a problemas como el sobreajuste, dificultades en la interpretación del modelo y una mayor complejidad computacional. Por ello, reducir el número de variables explicativas, sin perder información relevante, se convierte en un paso crucial para construir modelos más eficientes y robustos.</li>
</ul></li>
<li><strong>Validación del modelo:</strong>
<ul>
<li>Evaluar el desempeño del modelo con datos de validación o mediante técnicas como validación cruzada para garantizar su capacidad predictiva en nuevos conjuntos de datos.</li>
</ul></li>
</ol>
<p>El objetivo principal de este tema es presentar las técnicas más relevantes para la selección de variables y regularización, entender sus fundamentos teóricos, y aplicarlas a casos prácticos. Esto no solo permitirá construir modelos más robustos y eficientes, sino que también ayudará a obtener insights más claros y útiles a partir de los datos.</p>
</section>
<section id="reducción-de-variables" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="reducción-de-variables"><span class="header-section-number">3.2</span> Reducción de variables</h2>
<p>En análisis de regresión, especialmente cuando se trabaja con conjuntos de datos de alta dimensionalidad, es común enfrentar situaciones en las que el número de variables explicativas es muy grande. Esto puede llevar a problemas como el sobreajuste, dificultades en la interpretación del modelo y una mayor complejidad computacional. Por ello, reducir el número de variables explicativas, sin perder información relevante, se convierte en un paso crucial para construir modelos más eficientes y robustos.</p>
<p>Es especialmente importante reducir el número de variables explicativas en los estudios observacionales exploratorios, ya que en otros tipos de estudios, como los diseñados previamente, las variables incluidas suelen estar seleccionadas de antemano porque se conoce su relación con la variable respuesta o porque han sido identificadas como relevantes en investigaciones previas.</p>
<p>La <strong>reducción de variables explicativas</strong> busca simplificar el modelo al seleccionar un subconjunto de predictores que capturen la mayor parte de la información relevante de los datos. Este proceso puede realizarse a través de diferentes enfoques, dependiendo del contexto y de las características del conjunto de datos.</p>
<section id="motivaciones-para-reducir-variables" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="motivaciones-para-reducir-variables"><span class="header-section-number">3.2.1</span> Motivaciones para reducir variables</h3>
<p>Al limitar el número de predictores, no solo se simplifica el modelo, sino que también se optimizan diversos aspectos fundamentales en el análisis.</p>
<ol type="1">
<li><strong>Evitar el sobreajuste:</strong>
<ul>
<li>Cuando hay demasiadas variables en relación al número de observaciones, el modelo puede ajustarse demasiado a los datos de entrenamiento y perder capacidad predictiva en nuevos conjuntos de datos.</li>
</ul></li>
<li><strong>Mejorar la interpretabilidad:</strong>
<ul>
<li>Un modelo con menos variables es más fácil de interpretar, lo que resulta fundamental en aplicaciones como ciencias sociales, biomedicina o economía.</li>
</ul></li>
<li><strong>Reducción de complejidad computacional:</strong>
<ul>
<li>Al disminuir el número de variables, se reducen los costos de tiempo y memoria en el ajuste y evaluación del modelo.</li>
</ul></li>
<li><strong>Manejo de multicolinealidad:</strong>
<ul>
<li>La reducción puede eliminar variables redundantes que presentan una alta correlación entre sí, estabilizando las estimaciones del modelo.</li>
</ul></li>
</ol>
</section>
<section id="métodos-de-reducción-de-variables" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="métodos-de-reducción-de-variables"><span class="header-section-number">3.2.2</span> Métodos de reducción de variables</h3>
<p>Algunas de las ideas más comunes para tratar de reducir el número de variables de un modelo son:</p>
<ol type="1">
<li><strong>Selección de variables:</strong>
<ul>
<li>Utiliza estrategias como selección directa, métodos automáticos (forward, backward o stepwise), o técnicas basadas en regularización (Lasso, Elastic Net) para seleccionar las variables más relevantes.</li>
</ul></li>
<li><strong>Técnicas de transformación de datos:</strong>
<ul>
<li>Se proyectan las variables explicativas en un nuevo espacio de menor dimensionalidad, manteniendo la mayor cantidad posible de información. Algunas de estas técnicas se estudian en la asignatura de Aprendizaje Automático:
<ul>
<li><strong>Análisis de Componentes Principales (PCA):</strong> Reduce las variables explicativas a un conjunto de componentes ortogonales que explican la mayor parte de la varianza.</li>
<li><strong>Análisis de Factores:</strong> Agrupa variables relacionadas en factores latentes que capturan la esencia de la información.</li>
</ul></li>
</ul></li>
<li><strong>Filtrado basado en información:</strong>
<ul>
<li>Identifica y descarta variables con baja variabilidad o poca relación con la variable respuesta, utilizando métricas como la correlación o importancia estadística.</li>
</ul></li>
<li><strong>Métodos de selección basados en modelos:</strong>
<ul>
<li>Ajusta modelos iterativamente para evaluar la contribución de cada variable explicativa y descartar aquellas con menor relevancia según criterios como el <span class="math inline">\(p\)</span>-valor, AIC o BIC.</li>
</ul></li>
</ol>
<div class="callout callout-style-default callout-important callout-titled" title="Consideraciones importantes">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Consideraciones importantes
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>La reducción de variables debe realizarse cuidadosamente para evitar la pérdida de información clave que pueda comprometer la calidad del modelo.</li>
<li>Es fundamental validar el modelo resultante, asegurándose de que mantenga su capacidad predictiva mediante técnicas como validación cruzada.</li>
<li>En algunos casos, la selección o transformación de variables puede implicar compromisos entre simplicidad e interpretabilidad.</li>
</ul>
</div>
</div>
</section>
</section>
<section id="selección-de-variables" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="selección-de-variables"><span class="header-section-number">3.3</span> Selección de variables</h2>
<p>Simplificar un modelo eliminando variables irrelevantes es fundamental para mejorar su parsimonia y evitar el sobreajuste. Este objetivo puede lograrse mediante métodos de selección de variables, ya sean directos, automáticos o basados en regularización. Estas técnicas permiten identificar subconjuntos óptimos de predictores, optimizando tanto la simplicidad como la precisión del modelo. En particular, los métodos de regularización, como Ridge, Lasso y Elastic Net, introducen penalizaciones al modelo para controlar la complejidad y prevenir el sobreajuste, convirtiéndose en herramientas clave en el análisis de datos modernos.</p>
<p>Cuando se dispone de <span class="math inline">\(p\)</span> variables explicativas, es posible construir hasta <span class="math inline">\(2^p\)</span> modelos diferentes considerando todas las combinaciones posibles de estas variables. Sin embargo, explorar de manera exhaustiva todos estos modelos puede ser inviable, especialmente si <span class="math inline">\(p\)</span> es grande. Por ejemplo, con solo 10 variables regresoras, se generarían <span class="math inline">\(2^{10} = 1024\)</span> modelos posibles. Aunque la tecnología actual permite ajustar todos estos modelos, evaluar cada uno en términos de bondad de ajuste, gráficos de residuos, detección de observaciones influyentes y otros diagnósticos sería extremadamente complejo y costoso.</p>
<p>Para superar este desafío, se han desarrollado criterios específicos de selección de variables que ayudan a los analistas a identificar un pequeño subconjunto de modelos que cumplan con los estándares de calidad deseados. Este enfoque permite centrar el análisis en un grupo reducido de modelos “buenos”, generalmente entre 4 y 6, y realizar un estudio más profundo y detallado de ellos. Esta estrategia facilita tanto la interpretación como la eficiencia del proceso analítico, optimizando el uso de recursos computacionales y asegurando que los resultados sean robustos y fiables.</p>
</section>
<section id="métodos-de-selección-directa" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="métodos-de-selección-directa"><span class="header-section-number">3.4</span> Métodos de selección directa</h2>
<p>Los métodos de selección directa son un enfoque fundamental en la búsqueda de un subconjunto óptimo de variables predictoras en modelos de regresión. Este enfoque evalúa de manera sistemática diferentes combinaciones de variables para identificar cuál de ellas proporciona el mejor ajuste al modelo en función de un criterio predefinido, como el coeficiente de determinación ajustado (<span class="math inline">\(R^2\)</span> ajustado), el error cuadrático medio (ECM) o criterios de información como AIC o BIC.</p>
<p>A diferencia de los métodos automáticos, los métodos de selección directa no dependen de un proceso iterativo de adición o eliminación de variables. En cambio, buscan exhaustivamente (o mediante aproximaciones computacionalmente más eficientes) entre todas las posibles combinaciones de variables, lo que garantiza un análisis completo de las interacciones y relevancias potenciales.</p>
<p>Estos métodos son especialmente útiles cuando el número de predictores no es demasiado grande, ya que el esfuerzo computacional crece exponencialmente con el número de variables. Aunque el costo computacional puede ser elevado en datasets amplios, los métodos de selección directa proporcionan una referencia sólida y transparente para evaluar qué variables son fundamentales en el modelo.</p>
<p>En esta sección, analizaremos los métodos de selección directa más comunes, su implementación práctica y las métricas utilizadas para comparar modelos, destacando sus ventajas y limitaciones.</p>
</section>
<section id="métodos-automáticos" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="métodos-automáticos"><span class="header-section-number">3.5</span> Métodos automáticos</h2>
<p>Los métodos automáticos de selección de variables son herramientas prácticas y eficientes diseñadas para identificar subconjuntos relevantes de predictores en un modelo de regresión. A diferencia de los métodos de selección directa, que exploran exhaustivamente todas las combinaciones posibles de variables, los métodos automáticos siguen un enfoque iterativo que simplifica el proceso de selección. Estos métodos son especialmente útiles en situaciones donde el número de predictores es elevado, ya que reducen significativamente el esfuerzo computacional.</p>
<p>El principio clave detrás de los métodos automáticos es el ajuste dinámico del conjunto de variables en función de criterios estadísticos, como <span class="math inline">\(p\)</span>-valores, coeficientes de determinación ajustados (<span class="math inline">\(R^2\)</span>) ajustado), o criterios de información como AIC y BIC. Entre las estrategias más comunes se encuentran:</p>
<ul>
<li><p><strong>Método Forward (selección progresiva):</strong> Parte de un modelo vacío e incorpora variables de manera secuencial, añadiendo en cada paso la variable que mejora más el modelo.</p></li>
<li><p><strong>Método Backward (eliminación regresiva):</strong> Comienza con todas las variables en el modelo y elimina iterativamente aquellas que tienen menor impacto.</p></li>
<li><p><strong>Método Stepwise:</strong> Combina las estrategias forward y backward, permitiendo tanto la inclusión como la exclusión de variables en cada iteración.</p></li>
</ul>
<p>Estos métodos ofrecen una manera estructurada y ágil de seleccionar variables, aunque no garantizan encontrar el mejor modelo global debido a su naturaleza secuencial. A lo largo de esta sección, examinaremos cada uno de estos métodos, sus ventajas, limitaciones y aplicaciones en diferentes contextos de análisis.</p>
</section>
<section id="métodos-basados-en-regularización" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="métodos-basados-en-regularización"><span class="header-section-number">3.6</span> Métodos basados en regularización</h2>
<p>En los modelos de regresión, especialmente cuando se trabaja con un gran número de variables predictoras o con datos multicolineales, los métodos tradicionales de selección de variables pueden resultar ineficaces o inestables. En estos casos, los métodos basados en regularización surgen como una alternativa poderosa que no solo selecciona variables, sino que también mejora la estabilidad y la precisión del modelo.</p>
<p>La regularización consiste en introducir una penalización en la función de ajuste del modelo, lo que tiene dos efectos principales: controlar el sobreajuste al reducir la complejidad del modelo y forzar la selección de un subconjunto más parsimonioso de predictores. Estas penalizaciones ajustan los coeficientes de las variables predictoras, favoreciendo soluciones más simples y robustas <span class="citation" data-cites="james2013introduction">(<a href="references.html#ref-james2013introduction" role="doc-biblioref">James et&nbsp;al. 2013</a>)</span>.</p>
<p>Entre los métodos de regularización más destacados se encuentran:</p>
<ul>
<li><strong>Ridge Regression:</strong> Aplica una penalización proporcional al cuadrado de los coeficientes, lo que permite manejar problemas de multicolinealidad pero no conduce a la eliminación completa de variables.</li>
<li><strong>Lasso (Least Absolute Shrinkage and Selection Operator):</strong> Introduce una penalización basada en el valor absoluto de los coeficientes, lo que no solo reduce su magnitud, sino que también puede anularlos completamente, realizando una selección automática de variables.</li>
<li><strong>Elastic Net:</strong> Combina las penalizaciones de Ridge y Lasso, ofreciendo mayor flexibilidad en situaciones donde hay una gran correlación entre los predictores.</li>
</ul>
<p>Estos métodos son especialmente útiles en problemas donde el número de variables predictoras excede el número de observaciones, o cuando se desea un modelo más interpretable. En esta sección, exploraremos en detalle los fundamentos teóricos, la implementación práctica y las aplicaciones de cada uno de estos métodos, destacando sus ventajas en escenarios complejos y desafiantes.</p>
<section id="ridge-regression" class="level3" data-number="3.6.1">
<h3 data-number="3.6.1" class="anchored" data-anchor-id="ridge-regression"><span class="header-section-number">3.6.1</span> Ridge regression</h3>
<p>La regresión Ridge introduce una penalización en la estimación de los coeficientes de regresión, lo que ayuda a reducir la varianza del modelo y mejora su capacidad predictiva en presencia de datos altamente correlacionados o con muchas variables <span class="citation" data-cites="marquardt1975ridge">(<a href="references.html#ref-marquardt1975ridge" role="doc-biblioref">Marquardt y Snee 1975</a>)</span>. El modelo de regresión Ridge es una extensión de la regresión lineal estándar. Dado un conjunto de datos con <span class="math inline">\(n\)</span> observaciones y <span class="math inline">\(p\)</span> predictores, expresamos el modelo de regresión lineal múltiple como:</p>
<p><span class="math display">\[
\mathbf{Y}= \mathbf{X} \beta + \boldsymbol{\varepsilon}
\]</span></p>
<p>donde:</p>
<ul>
<li><span class="math inline">\(\mathbf{Y}\)</span> es el vector de respuesta de dimensión <span class="math inline">\(n \times 1\)</span>.</li>
<li><span class="math inline">\(\mathbf{X}\)</span> es la matriz de diseño de dimensión <span class="math inline">\(n \times p\)</span>.</li>
<li><span class="math inline">\(\beta\)</span> es el vector de coeficientes de regresión de dimensión <span class="math inline">\(p \times 1\)</span>.</li>
<li><span class="math inline">\(\boldsymbol{\varepsilon}\)</span> es el vector de errores aleatorios.</li>
</ul>
<p>En mínimos cuadrados ordinarios (OLS), los coeficientes se estiman minimizando la suma de los errores al cuadrado:</p>
<p><span class="math display">\[
SSE = \sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2 = \| \mathbf{Y} - \mathbf{X} \beta \|^2.
\]</span></p>
<p>Sin embargo, cuando hay multicolinealidad, la matriz <span class="math inline">\(X^T X\)</span> puede ser casi singular, generando coeficientes inestables. Para evitar esto, la regresión Ridge añade un <strong>término de penalización</strong> <span class="math inline">\(\lambda\)</span>, de la siguiente manera:</p>
<p><span class="math display">\[
SSE_{ridge} = \| \mathbf{Y} - \mathbf{X} \beta \|^2 + \lambda \sum_{j=1}^{p} \beta_j^2.
\]</span></p>
<p>Este término adicional, es un <strong>término de penalización</strong> (<span class="math inline">\(L_2=\sum \beta_j^2\)</span>) impone una restricción sobre los coeficientes, evitando que tomen valores excesivamente grandes. La estimación de <span class="math inline">\(\beta\)</span> en Ridge se obtiene resolviendo:</p>
<p><span class="math display">\[
\hat{\beta}_{ridge} = (\mathbf{X}^T \mathbf{X} + \lambda I)^{-1} \mathbf{X}^T \mathbf{Y}.
\]</span></p>
<p>donde <span class="math inline">\(I\)</span> es la matriz identidad y <span class="math inline">\(\lambda \geq 0\)</span> es un hiperparámetro que controla la cantidad de penalización aplicada.</p>
<p><strong>Interpretación del parámetro</strong> <span class="math inline">\(\lambda\)</span></p>
<ul>
<li>Si <span class="math inline">\(\lambda = 0\)</span>, el modelo Ridge es equivalente a la regresión lineal tradicional (OLS).</li>
<li>A medida que <span class="math inline">\(\lambda\)</span> aumenta, los coeficientes <span class="math inline">\(\beta_j\)</span> se reducen en magnitud, lo que ayuda a controlar la varianza del modelo y a prevenir el sobreajuste.</li>
<li>Si <span class="math inline">\(\lambda\)</span> es demasiado grande, los coeficientes se acercan a cero y el modelo puede perder interpretabilidad.</li>
</ul>
<p>La elección óptima de <span class="math inline">\(\lambda\)</span> se determina generalmente mediante <strong>validación cruzada</strong>.</p>
<div class="callout callout-style-default callout-caution callout-titled" title="Aviso">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Aviso
</div>
</div>
<div class="callout-body-container callout-body">
<p>Los detalles de la validación cruzada son tratados en la asignatura de Aprendizaje Automático.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Propiedades Clave">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Propiedades Clave
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>Manejo de la multicolinealidad:</strong> La regularización reduce la sensibilidad del modelo cuando los predictores están altamente correlacionados.</p></li>
<li><p><strong>Menor varianza en las predicciones:</strong> El modelo Ridge tiende a ser más estable en comparación con OLS, lo que mejora la capacidad de generalización en conjuntos de datos nuevos.</p></li>
<li><p><strong>No realiza selección de variables:</strong> A diferencia de Lasso, Ridge <strong>no anula coeficientes</strong>, sino que reduce su magnitud. Esto es útil cuando se sospecha que todas las variables tienen algún grado de importancia en el modelo.</p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="Ejemplo">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Ejemplo
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Cargar librerías</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: Matrix</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Loaded glmnet 4.1-8</code></pre>
</div>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Datos simulados</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">100</span> <span class="sc">*</span> <span class="dv">10</span>), <span class="dv">100</span>, <span class="dv">10</span>)  <span class="co"># 100 observaciones, 10 predictores</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> <span class="fu">rnorm</span>(<span class="dv">10</span>) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)  <span class="co"># Variable de respuesta con ruido</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajustar modelo Ridge</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>modelo_ridge <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, Y, <span class="at">alpha =</span> <span class="dv">0</span>)  <span class="co"># alpha = 0 indica regresión Ridge</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Seleccionar lambda óptimo con validación cruzada</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>cv_ridge <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X, Y, <span class="at">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>lambda_optimo <span class="ot">&lt;-</span> cv_ridge<span class="sc">$</span>lambda.min  <span class="co"># Mejor valor de lambda</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(lambda_optimo)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.2583753</code></pre>
</div>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajustar modelo final con lambda óptimo</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>modelo_ridge_final <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, Y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> lambda_optimo)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>modelo_ridge_final</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:  glmnet(x = X, y = Y, alpha = 0, lambda = lambda_optimo) 

  Df  %Dev Lambda
1 10 93.55 0.2584</code></pre>
</div>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Comparación modelo clásico</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>modelo_lm <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y<span class="sc">~</span>X)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Mostrar coeficientes</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>output<span class="ot">=</span><span class="fu">cbind</span>(<span class="fu">round</span>(<span class="fu">coef</span>(modelo_ridge_final),<span class="dv">3</span>),</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>            <span class="fu">round</span>(<span class="fu">coef</span>(modelo_lm),<span class="dv">3</span>))</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(output)<span class="ot">=</span><span class="fu">c</span>(<span class="st">"RIDGE"</span>,<span class="st">"OLS"</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>output</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>11 x 2 sparse Matrix of class "dgCMatrix"
             RIDGE    OLS
(Intercept)  0.118  0.132
V1          -0.874 -0.995
V2          -1.019 -1.131
V3           0.040  0.039
V4           0.002  0.001
V5          -2.500 -2.703
V6           1.001  1.104
V7           0.247  0.274
V8           2.125  2.244
V9           0.635  0.658
V10         -0.390 -0.427</code></pre>
</div>
</div>
</div>
</div>
</div>
<hr>
<p>La regresión Ridge es una técnica poderosa para mejorar la estabilidad de los modelos de regresión en presencia de multicolinealidad. A diferencia de OLS, que puede generar coeficientes inestables, Ridge introduce una penalización que reduce la magnitud de los coeficientes, evitando valores extremos. Aunque Ridge no realiza selección de variables, su capacidad para reducir la varianza y mejorar la capacidad predictiva lo convierte en una herramienta esencial en el análisis de datos modernos.</p>
<p>En la siguiente sección, exploraremos la <strong>regresión Lasso</strong>, que extiende este concepto permitiendo la eliminación de variables irrelevantes del modelo.</p>
</section>
<section id="regresión-lasso" class="level3" data-number="3.6.2">
<h3 data-number="3.6.2" class="anchored" data-anchor-id="regresión-lasso"><span class="header-section-number">3.6.2</span> Regresión Lasso</h3>
<p>Cuando se tiene un conjunto de predictores con posibles redundancias o ruido, Lasso permite identificar cuáles son las variables más relevantes para el modelo, lo que facilita la interpretación y reduce la complejidad del análisis.</p>
<p>Al igual que ocurría en Ridge Regression, el modelo de regresión Lasso se basa en la minimización de la siguiente función de error <span class="citation" data-cites="ranstam2018lasso">(<a href="references.html#ref-ranstam2018lasso" role="doc-biblioref">Ranstam y Cook 2018</a>)</span>: <span class="math display">\[
SSE_{lasso} = \| \mathbf{Y}- \mathbf{X} \beta \|^2 + \lambda \sum_{j=1}^{p} |\beta_j|
\]</span></p>
<p>donde el <strong>término de penalización</strong>, (<span class="math inline">\(L_1=\sum |\beta_j|\)</span>) hace que algunos coeficientes se reduzcan exactamente a <strong>cero</strong>, lo que significa que esas variables son eliminadas del modelo.</p>
<p>La diferencia clave con <strong>Ridge Regressión</strong>, visto anteriormente, es que Ridge reduce la magnitud de los coeficientes pero no los anula, mientras que <strong>Lasso puede eliminar variables por completo</strong>.</p>
<p><strong>Interpretación del parámetro</strong> <span class="math inline">\(\lambda\)</span></p>
<ul>
<li>Si <span class="math inline">\(\lambda = 0\)</span>, el modelo es equivalente a la regresión lineal tradicional (OLS).</li>
<li>A medida que <span class="math inline">\(\lambda\)</span> aumenta, más coeficientes se reducen a cero, lo que equivale a realizar <strong>selección de variables</strong>.</li>
<li>Si <span class="math inline">\(\lambda\)</span> es demasiado grande, se eliminan demasiadas variables, lo que puede resultar en un modelo subóptimo.</li>
</ul>
<p>Al igual que en el método <em>Risge</em>, la selección óptima de <span class="math inline">\(\lambda\)</span> se realiza generalmente mediante <strong>validación cruzada</strong>.</p>
<hr>
<div class="callout callout-style-default callout-note callout-titled" title="Propiedades Clave">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Propiedades Clave
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><strong>Selección de variables automática:</strong> Lasso no solo regulariza, sino que también selecciona las variables más importantes eliminando aquellas menos relevantes.</li>
<li><strong>Manejo de la multicolinealidad:</strong> Puede mejorar la interpretación del modelo cuando hay muchas variables correlacionadas.</li>
<li><strong>Simplicidad y interpretabilidad:</strong> Un modelo con menos variables es más fácil de interpretar y aplicar en la práctica.</li>
<li><strong>Reduce el sobreajuste:</strong> La penalización <span class="math inline">\(L_1\)</span> evita que el modelo se ajuste demasiado a los datos de entrenamiento, mejorando su capacidad predictiva en datos nuevos.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="Ejemplo">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Ejemplo
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajustar modelo Lasso</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>modelo_lasso <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, Y, <span class="at">alpha =</span> <span class="dv">1</span>)  <span class="co"># alpha = 1 indica regresión Lasso</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Seleccionar lambda óptimo con validación cruzada</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>cv_lasso <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X, Y, <span class="at">alpha =</span> <span class="dv">1</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>lambda_optimo <span class="ot">&lt;-</span> cv_lasso<span class="sc">$</span>lambda.min  <span class="co"># Mejor valor de lambda</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(lambda_optimo)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.03260326</code></pre>
</div>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajustar modelo final con lambda óptimo</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>modelo_lasso_final <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, Y, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">lambda =</span> lambda_optimo)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Mostrar coeficientes</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>output<span class="ot">=</span><span class="fu">cbind</span>(<span class="fu">round</span>(<span class="fu">coef</span>(modelo_lasso_final),<span class="dv">3</span>),output)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(output)<span class="ot">=</span><span class="fu">c</span>(<span class="st">"LASSO"</span>,<span class="st">"RIDGE"</span>,<span class="st">"OLS"</span>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>output</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>11 x 3 sparse Matrix of class "dgCMatrix"
             LASSO  RIDGE    OLS
(Intercept)  0.131  0.118  0.132
V1          -0.950 -0.874 -0.995
V2          -1.078 -1.019 -1.131
V3           0.006  0.040  0.039
V4           .      0.002  0.001
V5          -2.652 -2.500 -2.703
V6           1.058  1.001  1.104
V7           0.235  0.247  0.274
V8           2.213  2.125  2.244
V9           0.629  0.635  0.658
V10         -0.392 -0.390 -0.427</code></pre>
</div>
</div>
</div>
</div>
</div>
<p><strong>Consideraciones Importantes</strong></p>
<p>La regresión Lasso es una poderosa técnica de regularización que no solo mejora la estabilidad del modelo en presencia de muchas variables predictoras, sino que también realiza una selección automática de las más relevantes. Su capacidad para reducir coeficientes a cero la convierte en una herramienta esencial en el análisis de datos de alta dimensión.</p>
<ul>
<li><strong>Lasso puede eliminar demasiadas variables si</strong> <span class="math inline">\(\lambda\)</span> es demasiado grande, lo que puede llevar a la pérdida de información importante.</li>
<li><strong>No maneja bien grupos de predictores altamente correlacionados</strong>, ya que selecciona solo uno de ellos y elimina los demás.</li>
<li><strong>Elastic Net</strong>, que combina Ridge y Lasso, puede ser una mejor opción cuando hay <strong>multicolinealidad fuerte</strong> en los datos.</li>
</ul>
<p>En la siguiente sección, exploraremos <strong>Elastic Net</strong>, una técnica híbrida que combina las ventajas de Ridge y Lasso para mejorar la selección de variables en presencia de predictores altamente correlacionados.</p>
</section>
<section id="elastic-net" class="level3" data-number="3.6.3">
<h3 data-number="3.6.3" class="anchored" data-anchor-id="elastic-net"><span class="header-section-number">3.6.3</span> Elastic Net</h3>
<p>La regresión <strong>Elastic Net</strong> es una técnica de regularización que combina las propiedades de <strong>Ridge</strong> y <strong>Lasso</strong>, abordando algunas de sus limitaciones individuales <span class="citation" data-cites="zou2005regularization">(<a href="references.html#ref-zou2005regularization" role="doc-biblioref">Zou y Hastie 2005</a>)</span>. Mientras que Ridge es útil para manejar la multicolinealidad sin eliminar variables y Lasso selecciona un subconjunto de predictores, Elastic Net equilibra ambos enfoques permitiendo la selección de variables en presencia de alta correlación entre los predictores.</p>
<p>Este método es particularmente efectivo cuando el número de predictores es grande y existe <strong>multicolinealidad</strong>, ya que permite controlar simultáneamente la <strong>reducción de la magnitud de los coeficientes</strong> y la <strong>eliminación de variables irrelevantes</strong>.</p>
<p>Elastic Net introduce una penalización que combina los términos de Ridge (<span class="math inline">\(L_2\)</span>) y Lasso (<span class="math inline">\(L_1\)</span>):</p>
<p><span class="math display">\[
SSE_{\text{Elastic Net}} = \| Y - X \beta \|^2 + \lambda_1 \sum_{j=1}^{p} |\beta_j| + \lambda_2 \sum_{j=1}^{p} \beta_j^2
\]</span></p>
<p>donde:</p>
<ul>
<li><span class="math inline">\(\lambda_1\)</span> (asociado a Lasso) controla la cantidad de coeficientes que se reducen a <strong>cero</strong>.</li>
<li><span class="math inline">\(\lambda_2\)</span> (asociado a Ridge) controla la <strong>reducción de magnitud</strong> de los coeficientes sin anularlos.</li>
<li><span class="math inline">\(\alpha\)</span> es un parámetro adicional que pondera la combinación entre Lasso y Ridge, con:
<ul>
<li><span class="math inline">\(\alpha = 1\)</span> → Elastic Net se comporta como Lasso.</li>
<li><span class="math inline">\(\alpha = 0\)</span> → Elastic Net se comporta como Ridge.</li>
<li><span class="math inline">\(0 &lt; \alpha &lt; 1\)</span> → Elastic Net combina ambos métodos.</li>
</ul></li>
</ul>
<p>La estimación de los coeficientes en Elastic Net se obtiene resolviendo:</p>
<p><span class="math display">\[
\hat{\beta}_{\text{Elastic Net}} = \arg \min_{\beta} \left( \| Y - X \beta \|^2 + \lambda \left( \alpha \sum |\beta_j| + (1 - \alpha) \sum \beta_j^2 \right) \right)
\]</span></p>
<hr>
<div class="callout callout-style-default callout-note callout-titled" title="Propiedades Clave">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Propiedades Clave
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>Manejo de la Multicolinealidad:</strong> A diferencia de Lasso, que selecciona solo una de las variables correlacionadas y elimina las demás, Elastic Net distribuye la penalización entre todas las variables correlacionadas, evitando una selección arbitraria.</p></li>
<li><p><strong>Selección de variables más estable:</strong> La combinación de Lasso y Ridge permite una selección más robusta, manteniendo información relevante del modelo sin eliminar predictores clave.</p></li>
<li><p><strong>Mejora del rendimiento predictivo:</strong> Al utilizar validación cruzada para seleccionar los hiperparámetros <span class="math inline">\(\lambda_1\)</span>, <span class="math inline">\(\lambda_2\)</span> y <span class="math inline">\(\alpha\)</span>, se optimiza la capacidad del modelo para generalizar a nuevos datos.</p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="Ejemplo">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Ejemplo
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajustar modelo Elastic Net</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>modelo_elastic_net <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, Y, <span class="at">alpha =</span> <span class="fl">0.5</span>)  <span class="co"># Alpha = 0.5 (50% Ridge, 50% Lasso)</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Seleccionar lambda óptimo con validación cruzada</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>cv_elastic_net <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X, Y, <span class="at">alpha =</span> <span class="fl">0.5</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>lambda_optimo <span class="ot">&lt;-</span> cv_elastic_net<span class="sc">$</span>lambda.min  <span class="co"># Mejor valor de lambda</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(lambda_optimo)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.0213522</code></pre>
</div>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajustar modelo final con lambda óptimo</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>modelo_elastic_final <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, Y, <span class="at">alpha =</span> <span class="fl">0.5</span>, <span class="at">lambda =</span> lambda_optimo)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Mostrar coeficientes</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>output<span class="ot">=</span><span class="fu">cbind</span>(<span class="fu">round</span>(<span class="fu">coef</span>(modelo_elastic_final),<span class="dv">3</span>),output)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(output)<span class="ot">=</span><span class="fu">c</span>(<span class="st">"ELASTIC"</span>,<span class="st">"LASSO"</span>,<span class="st">"RIDGE"</span>,<span class="st">"OLS"</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>output</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>11 x 4 sparse Matrix of class "dgCMatrix"
            ELASTIC  LASSO  RIDGE    OLS
(Intercept)   0.131  0.131  0.118  0.132
V1           -0.975 -0.950 -0.874 -0.995
V2           -1.108 -1.078 -1.019 -1.131
V3            0.028  0.006  0.040  0.039
V4            .      .      0.002  0.001
V5           -2.677 -2.652 -2.500 -2.703
V6            1.084  1.058  1.001  1.104
V7            0.260  0.235  0.247  0.274
V8            2.229  2.213  2.125  2.244
V9            0.647  0.629  0.635  0.658
V10          -0.414 -0.392 -0.390 -0.427</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>Para determinar el mejor valor de <span class="math inline">\(\alpha\)</span>, se usa <strong>validación cruzada</strong> probando distintos valores entre <span class="math inline">\(0\)</span> y 1. Algunas estrategias comunes incluyen:</p>
<ul>
<li><strong>Si hay muchas variables irrelevantes</strong>, se recomienda <span class="math inline">\(\alpha\)</span> cercano a 1 (Lasso).</li>
<li><strong>Si hay fuerte multicolinealidad</strong>, se recomienda <span class="math inline">\(\alpha\)</span> cercano a 0 (Ridge).</li>
<li><strong>Si se desea un balance entre selección y estabilidad</strong>, se suele usar <span class="math inline">\(\alpha = 0.5\)</span>.</li>
</ul>
<p>La regresión Elastic Net combina lo mejor de Ridge y Lasso, ofreciendo un método de regularización robusto para modelos con muchas variables predictoras y posible multicolinealidad. Su capacidad para seleccionar variables sin eliminar información clave lo convierte en una opción ideal para modelos complejos y de alta dimensionalidad.</p>
</section>
<section id="comparación-de-los-métodos-de-regularización" class="level3" data-number="3.6.4">
<h3 data-number="3.6.4" class="anchored" data-anchor-id="comparación-de-los-métodos-de-regularización"><span class="header-section-number">3.6.4</span> Comparación de los métodos de Regularización</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Método</th>
<th>Penalización</th>
<th>Efecto sobre los coeficientes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>OLS</strong></td>
<td>Ninguna</td>
<td>Sin restricción, puede haber multicolinealidad</td>
</tr>
<tr class="even">
<td><strong>Ridge</strong></td>
<td><span class="math inline">\(L_2\)</span></td>
<td>Reduce la magnitud de los coeficientes, pero no los anula</td>
</tr>
<tr class="odd">
<td><strong>Lasso</strong></td>
<td><span class="math inline">\(L_1\)</span></td>
<td>Puede anular coeficientes, permitiendo selección de variables</td>
</tr>
<tr class="even">
<td><strong>Elastic Net</strong></td>
<td><span class="math inline">\(L_1 + L_2\)</span></td>
<td>Combinación de Ridge y Lasso</td>
</tr>
</tbody>
</table>
<hr>
<p>Lasso es especialmente útil cuando se sospecha que muchas variables son irrelevantes, mientras que Ridge es preferido cuando se espera que todas las variables aporten información al modelo.</p>
<p>Elastic Net es ideal cuando hay <strong>muchas variables correlacionadas</strong> y se desea un modelo <strong>estable y parsimonioso</strong>.</p>
<ul>
<li><p>Elastic Net mejora la estabilidad del modelo en comparación con Lasso, especialmente cuando hay variables predictoras altamente correlacionadas.</p></li>
<li><p>Es más flexible que Ridge y Lasso individualmente, permitiendo un ajuste más fino a distintos tipos de problemas.</p></li>
<li><p>Requiere la selección de hiperparámetros (<span class="math inline">\(\lambda\)</span> y <span class="math inline">\(\alpha\)</span>), por lo que debe usarse validación cruzada para encontrar la combinación óptima.</p></li>
</ul>
<hr>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-james2013introduction" class="csl-entry" role="listitem">
James, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et&nbsp;al. 2013. <em>An introduction to statistical learning</em>. Vol. 112. Springer.
</div>
<div id="ref-kutner2005applied" class="csl-entry" role="listitem">
Kutner, Michael H, Christopher J Nachtsheim, John Neter, y William Li. 2005. <em>Applied linear statistical models</em>. McGraw-hill.
</div>
<div id="ref-marquardt1975ridge" class="csl-entry" role="listitem">
Marquardt, Donald W, y Ronald D Snee. 1975. <span>«Ridge regression in practice»</span>. <em>The American Statistician</em> 29 (1): 3-20.
</div>
<div id="ref-ranstam2018lasso" class="csl-entry" role="listitem">
Ranstam, Jonas, y Jonathan A Cook. 2018. <span>«LASSO regression»</span>. <em>Journal of British Surgery</em> 105 (10): 1348-48.
</div>
<div id="ref-zou2005regularization" class="csl-entry" role="listitem">
Zou, Hui, y Trevor Hastie. 2005. <span>«Regularization and variable selection via the elastic net»</span>. <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> 67 (2): 301-20.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copiado");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copiado");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./tema1.html" class="pagination-link" aria-label="El modelo de regresión lineal simple">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">El modelo de regresión lineal simple</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./tema3.html" class="pagination-link" aria-label="Modelos no lineales. Transformación de variables. Ingeniería de características.">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Modelos no lineales. Transformación de variables. Ingeniería de características.</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>